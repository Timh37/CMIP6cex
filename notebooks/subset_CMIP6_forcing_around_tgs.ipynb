{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357/2340292891.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import dask\n",
    "import intake\n",
    "import fsspec\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0790c4-8c30-4d6d-a968-d6c1c2832687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cat_to_max_num_realizations(cmip6_cat):\n",
    "    '''Reduce grid labels in pangeo cmip6 catalogue by \n",
    "    keeping grid_label and 'ipf' identifier combination with most datasets (=most realizations if using require_all_on)'''\n",
    "    df = cmip6_cat.df\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    df['ipf'] = [s[s.find('i'):] for s in df.member_id] #extract 'ipf' from 'ripf'\n",
    "\n",
    "    #generate list of tuples of (source_id,ipf,grid_label) that provide most realizations (note this will omit realizations not available at this grid but possibly at others)\n",
    "    max_num_ds_tuples = df.groupby(['source_id','ipf'])['grid_label'].value_counts().groupby(level=0).head(1).index.to_list() #head(1) gives (first) max. value since value_counts sorts max to min\n",
    "    df_filter = pd.DataFrame(max_num_ds_tuples,columns=['source_id','ipf','grid_label']) #generate df to merge catalogue df on\n",
    "    \n",
    "    df = df_filter.merge(right=df, on = ['source_id','ipf','grid_label'], how='left') #do the subsetting\n",
    "    df = df.drop(columns=['ipf']) #clean up\n",
    "    df= df[cols]\n",
    "\n",
    "    cmip6_cat.esmcat._df = df #(columns now ordered differently, probably not an issue?)\n",
    "    return cmip6_cat\n",
    "\n",
    "def drop_vars_from_cat(cmip6_cat,vars_to_drop):\n",
    "    cmip6_cat.esmcat._df = cmip6_cat.df.drop(cmip6_cat.df[cmip6_cat.df.variable_id.isin(vars_to_drop)].index).reset_index(drop=True)\n",
    "    return cmip6_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preselect_years(ddict_in,start_year,end_year):\n",
    "    '''select range of years of datasets'''\n",
    "    ddict_out = defaultdict(dict)\n",
    "    \n",
    "    assert start_year<end_year\n",
    "        \n",
    "    if start_year>2014: #only using SSP\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif end_year<=2014: #only using historical\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif ((start_year<=2014) & (end_year>2014)): #using both\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(None, str(end_year)))\n",
    "            elif 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), None))\n",
    "    return ddict_out #NB: may result in no timesteps being selected at all\n",
    "\n",
    "def drop_duplicate_timesteps(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():  \n",
    "        unique_time, idx = np.unique(v.time,return_index=True)\n",
    "        \n",
    "        if len(v.time) != len(unique_time):\n",
    "            ddict_out[k] = v.isel(time=idx)\n",
    "            print('Dropping duplicate timesteps for:' + k)   \n",
    "    return ddict_out\n",
    "\n",
    "def drop_coords(ddict_in,coords_to_drop):\n",
    "    for k, v in ddict_in.items():\n",
    "        ddict_in[k] = v.drop_dims(coords_to_drop,errors=\"ignore\")\n",
    "    return ddict_in\n",
    "\n",
    "def align_lonlat(ds_list):\n",
    "    aligned_ds_list = []\n",
    "    for ds in ds_list: #list of ds can't seem to be passed to xr.align instead\n",
    "        a,b = xr.align(ds_list[0],ds,join='override',exclude=['time','member_id'])\n",
    "        aligned_ds_list.append(b)\n",
    "    return aligned_ds_list\n",
    "\n",
    "def concat_members_aligning_lonlat(ds_list):\n",
    "    aligned_ds_list = align_lonlat(ds_list) #override same-dimension lon/lat prior to concatenating (ensures lon/lats are not padded)\n",
    "    return xr.concat(aligned_ds_list, dim='member_id', join='outer', coords='minimal',compat='override') #return xr.concat(ds_pick, dim='member_id')\n",
    "\n",
    "def merge_variables_aligning_lonlat(ds_list):\n",
    "    aligned_ds_list = align_lonlat(ds_list) #override same-dimension lon/lat prior to concatenating (ensures lon/lats are not padded)\n",
    "    return xr.merge(aligned_ds_list, join='outer',compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "my_models = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5','EC-Earth3',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "'''\n",
    "my_models = ['MPI-ESM1-2-HR']\n",
    "\n",
    "col = google_cmip_col()\n",
    "\n",
    "cat_data_ssp245 = col.search( #find instances providing all required variables for both historical & ssp245\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=['sfcWind','psl','pr'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "\n",
    "\n",
    "cat_data_ssp585 = col.search( #find instances providing all required variables for both historical & ssp585\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=['sfcWind','psl','pr'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "cat_data = cat_data_ssp585\n",
    "cat_data.esmcat._df = pd.concat([cat_data_ssp245.df,cat_data_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True) #all instances we want\n",
    "\n",
    "cat_data = reduce_cat_to_max_num_realizations(cat_data) #per model, select grid and 'ipf' combination providing most realizations\n",
    "cat_data = drop_vars_from_cat(cat_data,['pr']) #we query only instances that also provide 'pr', but don't process 'pr' it in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d7d0f5-b789-4ad3-ad23-1e0a233e89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_357/772692342.py:12: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat_data.to_dataset_dict(**kwargs) #open datasets into dictionary\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='12' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [12/12 00:05&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_data.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':True #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "    #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "}\n",
    "\n",
    "ddict = cat_data.to_dataset_dict(**kwargs) #open datasets into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9769a6a5-dbb9-42d8-8119-81d044abce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = drop_duplicate_timesteps(ddict) #CESM2-WACCM has duplicate timeseries\n",
    "ddict = preselect_years(ddict,1850,2100) #for now, limit analysis to up to 2100\n",
    "ddict = drop_coords(ddict,['bnds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063720f2-391d-4dcb-bb21-c2c5654aa940",
   "metadata": {},
   "source": [
    "Member concatenation runs into memory issues for more than 26 members for EC-Earth3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d613d7-8240-489d-b908-0b85f16b8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}): #join=outer pads NaNs which result in large chunks for timeseries that differ in length\n",
    "    ddict_merged = combine_datasets(ddict,merge_variables_aligning_lonlat,match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id','variant_label'])\n",
    "    #ddict_concat = combine_datasets(ddict_merged,concat_members_aligning_lonlat,match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6115e-9d0a-430a-9337-f848f6b9791b",
   "metadata": {},
   "source": [
    "Sanity-check number of timesteps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f47220-ce35-4846-a7e9-7b588829a791",
   "metadata": {},
   "source": [
    "```python\n",
    "for k,v in ddict_concat.items():\n",
    "    num_days = (v.time[-1]-v.time[0]).dt.days\n",
    "    assert (len(v.time) > .9*num_days) & (len(v.time) < 1.1*num_days)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92478f-f7ec-4177-878f-efe18bd294f0",
   "metadata": {},
   "source": [
    "Do the regridding & subsetting. First, determine the grid coordinates around each tide gauge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs\n",
    "\n",
    "era5_grid = xr.Dataset( #the ERA5 grid used to derive the MLR coefficients\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-40,30,1)+1/2, {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,10,-1)-1/2, {\"units\": \"degrees_north\"}),})\n",
    "\n",
    "#get coordinates of n x n degree grids around each tide gauge\n",
    "num_degr = 2\n",
    "lat_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "lon_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "\n",
    "for t,tg in enumerate(mlrcoefs.tg.values):\n",
    "    lat_ranges[t,:] = era5_grid.latitude[((era5_grid.latitude>=(mlrcoefs.sel(tg=tg).lat-num_degr/2)) & (era5_grid.latitude<=(mlrcoefs.sel(tg=tg).lat+num_degr/2)))][0:2]\n",
    "    lon_ranges[t,:] = era5_grid.longitude[((era5_grid.longitude>=(mlrcoefs.sel(tg=tg).lon-num_degr/2)) & (era5_grid.longitude<=(mlrcoefs.sel(tg=tg).lon+num_degr/2)))][0:2]\n",
    "\n",
    "#create da's to index the CMIP6 files with:\n",
    "lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlrcoefs.tg,'lon_around_tg':[0,1]})\n",
    "lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlrcoefs.tg,'lat_around_tg':[0,1]})\n",
    "\n",
    "target_grid = era5_grid.sel(latitude=slice(np.max(lats_da)+2,np.min(lats_da)-2),longitude=slice(np.min(lons_da)-2,np.max(lons_da)+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974653c-8138-4826-a9a2-6e6a05ebd70b",
   "metadata": {},
   "source": [
    "Now, loop over each model/grid (should be only 1 grid per model), and apply the model/grid-specific interpolation weights to each dataset belonging to that model/grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef1795-e316-4a3c-863c-0a8b90215a71",
   "metadata": {},
   "source": [
    "**Note:** It is probably more memory-efficient to loop over the tide gauges and regrid to each tide-gauge specific 2 by 2 degree grid, especially if considering larger domains. For now, we regrid to a subset of the ERA5 grid big enough to contain all tide-gauge specific grids (~20x20 grid points), and index the regridded dataset at the tide-gauge specific grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5ad8d0-456f-451b-90cd-d9a839350d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,ds in ddict_merged.items():\n",
    "    lon_coord = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    ds.coords[lon_coord] = ((ds.coords[lon_coord] + 180) % 360) - 180 #wrap around 0\n",
    "    ds = ds.reindex({ lon_coord : np.sort(ds[lon_coord])})\n",
    "    ddict_merged[key] = ds\n",
    "\n",
    "ds_dict = {k: v for k, v in ddict_merged.items()}\n",
    "ddict_subsetted = {k: v for k, v in ddict_merged.items()}\n",
    "\n",
    "while len(ds_dict) > 0: #<- from xmip's combine_datasets\n",
    "    k = list(ds_dict.keys())[0]\n",
    "    ds = ds_dict.pop(k)\n",
    "    ds.attrs[\"original_key\"] = k\n",
    "    \n",
    "    matched_datasets = _match_datasets(ds, ds_dict, ['source_id', 'grid_label'], pop=True) #find datasets belonging to same model/grid\n",
    "  \n",
    "    regridder = xe.Regridder(matched_datasets[0],target_grid,'bilinear',ignore_degenerate=True) #interpolation weights for this grid\n",
    "    \n",
    "    for matched_ds in matched_datasets:\n",
    "        first_ds,aligned_ds = xr.align(matched_datasets[0],matched_ds,join='override',exclude=['time','member_id','dcpp_init_year']) #make sure lon/lat coordinates are exactly the same\n",
    "        \n",
    "        aligned_ds = aligned_ds.isel(dcpp_init_year=0,drop=True) #get rid of dcpp_init_year dimension\n",
    "        ddict_subsetted[matched_ds.original_key] = regridder(aligned_ds,keep_attrs=True).sel(latitude=lats_da,longitude=lons_da) #regrid to target grid and add to ddict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ab0c4-fe02-4c8e-b0c3-f351a17cb085",
   "metadata": {},
   "source": [
    "Store the datasets (directories structured per model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b8061f-26d9-4f51-b6ce-02bf34c77e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595a01668a9f406a909aa476271ec8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key,ds in tqdm(ddict_subsetted.items()):\n",
    "    model_path = os.path.join('/home/jovyan/CMIP6cf/output/subsetted_forcing/',ds.source_id)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    ds.to_netcdf(os.path.join(model_path,key.replace('.','_')+'.nc'),mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527442-521e-4730-95e7-09e3dcc9b58b",
   "metadata": {},
   "source": [
    "Next, derive surges from the atmospheric forcing:\n",
    "- combine the historical and ssp data (**need custom combination if quering multiple SSPs at once?**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410a3f9-881f-43c4-9996-8ebf4bbdc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_forcing = combine_datasets(ddict_subsetted,\n",
    "                        _concat_sorted_time,\n",
    "                       match_attrs =['source_id', 'grid_label','table_id'] ) #appends SSP to historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47632cb4-545a-46a8-91d7-fd6f3bf4e14f",
   "metadata": {},
   "source": [
    "- generate the normalized forcing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f0c6f-ac79-43cf-9b74-df3723ae5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate forcing to compute surges with\n",
    "for k,v in ddict_forcing.items():\n",
    "    attrs = v.attrs\n",
    "    \n",
    "    v['sfcWind_sqd'] = v['sfcWind']**2 #add wind squared\n",
    "    v['sfcWind_cbd'] = v['sfcWind']**3 #add wind cubed\n",
    "    \n",
    "    v = (v-v.mean(dim='time'))/v.std(dim='time',ddof=0) #normalize\n",
    "    v.attrs = attrs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * num_degr * num_degr))\n",
    "    v['forcing'] = v[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"forcing_var\") \n",
    "    v['forcing'] = v['forcing'].transpose(\"time\",\"forcing_var\",\"lon_around_tg\",...).stack(f=['forcing_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    ddict_forcing[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3b439-c794-4826-ba59-1c8afa26122c",
   "metadata": {},
   "source": [
    "- derive the principal components and multiply with regression coefficients derived from ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadaaeb-72d8-4fba-aacc-4d4a08693733",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_surges = defaultdict(dict)\n",
    "for k,ds in ddict_forcing.items(): #loop over datasets\n",
    "     \n",
    "    ds['surges'] = ( ('member_id','time','tg'), np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(ds.tg)) )) #initialize output\n",
    "            \n",
    "    for i_member,member in tqdm(enumerate(ds.member_id)):\n",
    "        forcing_mem = ds.forcing.sel(member_id=member).load() #load forcing data array into memory (for all tg for current dataset and member)\n",
    "        \n",
    "        #^^^ is it necessary to remove forcing_mem from memory at the end of the loop or is this taken care of by overwriting it with the next member?\n",
    "        \n",
    "        for i_tg,tg in enumerate(ds.tg):\n",
    "            #get model forcing at TG\n",
    "            forcing_tg = forcing_mem.sel(tg=tg) \n",
    "            \n",
    "            #get MLR coefficients at TG\n",
    "            tg_coefs = mlrcoefs.mlrcoefs.sel(tg=tg)\n",
    "            num_pcs = int(np.sum(np.isfinite(tg_coefs)))-1 #number of coefs = number of PCs to derive, intercept doesn't count\n",
    "            \n",
    "            i_timesteps_w_data = np.argwhere(np.isfinite(forcing_tg.data).all(axis=1)).flatten()\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(forcing_tg.isel(time=i_timesteps_w_data).data) #remove missing values for PCA\n",
    "            pcs = pca.transform(forcing_tg.isel(time=i_timesteps_w_data).data)\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            ds['surges'][i_member,i_timesteps_w_data,i_tg] = np.sum(tg_coefs[np.isfinite(tg_coefs)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "            \n",
    "    ddict_surges[k] = ds['surges'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    #probably want to store the surges and close the dataset here to avoid overflowing the memory?        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47888364-f390-4197-8658-d760e12ef0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_surges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfde47-44a7-4a29-8b60-81db175f2e89",
   "metadata": {},
   "source": [
    "**NB: not sure if it is possible to do this without a loop? The alternative is 'eofs.xarray', but I don't think that can be applied along axes either, and that results in different signs for the principal components because of sklearn.svd_flip()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214c28f-702b-4ab0-968a-3614eb3bf287",
   "metadata": {},
   "source": [
    "**How to make sure this loop doesn't overflow the memory for large queries?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a176ea-88cf-4f02-a2e8-8821c3da7ac3",
   "metadata": {},
   "source": [
    "=> Takes about 6 minutes per model per member for historical+ssp (1850-2100), or 4 minutes if I preselect (1950-2100). I think we will analyse approximately 150 members in total times two scenarios, so that would take 20-30h? Takes longer for higher resolution models with larger file sizes (e.g., EC-Earth3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f681d-a3c7-40ea-852a-b721cb3b98b6",
   "metadata": {},
   "source": [
    "Code for listing the instance_id of runs with missing timesteps:\n",
    "```python\n",
    "for k,v in ddict.items():\n",
    "    if (len(v.time) < len(np.arange(1850,2015))*12*30):\n",
    "    #if len(v.time) < len(np.arange(2015,2101))*12*30:\n",
    "        print(v.attrs['intake_esm_attrs:zstore'][0:-1].replace('gs://cmip6/','').replace('/','.'))\n",
    "```\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5e005-f756-4e82-a851-736fcdba3b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
