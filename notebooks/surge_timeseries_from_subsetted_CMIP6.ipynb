{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204334ce-ab38-445f-aad0-ae91d07b400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Open the subsetted CMIP6 `psl` & `sfcWind` data (**To-do: test if concatenating results in issues for many realizations (e.g., EC-Earth3)**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '/home/jovyan/CMIP6cf/output/subsetted_forcing/'\n",
    "\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for source_id in [s for s in os.listdir(in_dir) if s.startswith('.')==False]:\n",
    "    \n",
    "    experiment_ids = [s.split('_')[2] for s in os.listdir(os.path.join(in_dir,source_id)) if s.startswith('.')==False]\n",
    "    for experiment_id in set(experiment_ids): #for each experiment_id, open the datasets, concatenating all realizations:\n",
    "        \n",
    "        source_ds = xr.open_mfdataset(os.path.join(in_dir,source_id,'*'+experiment_id+'*.nc'),join='outer',combine='nested',\n",
    "                                      compat='override',coords='minimal',concat_dim='member_id') #need to test this for large np. of realizations, like EC-Earth3\n",
    "        ddict[source_ds.original_key.rsplit('.',1)[0]] = source_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa5f5a-beb7-4e81-b6c7-e913faeed978",
   "metadata": {},
   "source": [
    "Concatenate matching realizations of historical & SSP run in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b18b5-86aa-4075-b10a-986e817754f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssps = set([k.split('.')[2] for k in ddict.keys() if 'ssp' in k])\n",
    "\n",
    "ddict_concat = defaultdict(dict)\n",
    "\n",
    "for ssp in ssps:\n",
    "    ddict_ssp = defaultdict(dict)\n",
    "    \n",
    "    for k in ddict.keys():\n",
    "        if ((ssp in k) or ('historical' in k)):\n",
    "            ddict_ssp[k] = ddict[k]\n",
    "            \n",
    "    #append SSP to historical, only for realizations for which both experiments are provided (join=inner)\n",
    "    hist_ssp = combine_datasets(ddict_ssp,\n",
    "                                _concat_sorted_time,\n",
    "                                match_attrs =['source_id', 'grid_label','table_id'],combine_func_kwargs={'join':'inner'})\n",
    "    \n",
    "    for key,ds in hist_ssp.items(): #put back together in dictionary\n",
    "        ddict_concat[key+'.'+ssp] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba084-7e91-4236-8038-0f102af6a5fb",
   "metadata": {},
   "source": [
    "Sanity-check timeseries length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e669831-6ae9-4d7c-acbf-337047c2e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ddict_concat.items():\n",
    "    num_days = (v.time[-1]-v.time[0]).dt.days\n",
    "    assert (len(v.time) > .9*num_days) & (len(v.time) < 1.1*num_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1e7bc-b42d-4831-88f6-f466c61472bc",
   "metadata": {},
   "source": [
    "Generate forcing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b52404-5a12-416d-b07c-93167e7926f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate forcing to compute surges with\n",
    "for k,v in ddict_concat.items():\n",
    "    attrs = v.attrs\n",
    "    \n",
    "    v['sfcWind_sqd'] = v['sfcWind']**2 #add wind squared\n",
    "    v['sfcWind_cbd'] = v['sfcWind']**3 #add wind cubed\n",
    "    \n",
    "    v = (v-v.mean(dim='time'))/v.std(dim='time',ddof=0) #normalize (ignores nan by default?)\n",
    "    v.attrs = attrs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * num_degr * num_degr))\n",
    "    v['forcing'] = v[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"forcing_var\") \n",
    "    v['forcing'] = v['forcing'].transpose(\"time\",\"forcing_var\",\"lon_around_tg\",...).stack(f=['forcing_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    ddict_concat[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58fb60-3813-4edc-92b5-32b93741a8e9",
   "metadata": {},
   "source": [
    "Derive the principal components and multiply with regression coefficients derived from ERA5 (**to-do**: check lat/lon coords of result, must be of TGs to match pr timeseries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2fbb9-aa37-4b5d-a27b-6e84b946ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d50cfe-172c-4687-88cf-04c9111d0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_surges = defaultdict(dict)\n",
    "for k,ds in ddict_concat.items(): #loop over datasets\n",
    "     \n",
    "    ds['surge'] = ( ('member_id','time','tg'), np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(ds.tg)) )) #initialize output\n",
    "            \n",
    "    for i_member,member in tqdm(enumerate(ds.member_id)):\n",
    "        forcing_mem = ds.forcing.sel(member_id=member).load() #load forcing data array into memory (for all tg for current dataset and member)\n",
    "        \n",
    "        for i_tg,tg in enumerate(ds.tg):\n",
    "            #get model forcing at TG\n",
    "            forcing_tg = forcing_mem.sel(tg=tg) \n",
    "            \n",
    "            #get MLR coefficients at TG\n",
    "            tg_coefs = mlrcoefs.mlrcoefs.sel(tg=tg)\n",
    "            num_pcs = int(np.sum(np.isfinite(tg_coefs)))-1 #number of coefs = number of PCs to derive, intercept doesn't count\n",
    "            \n",
    "            i_timesteps_w_data = np.argwhere(np.isfinite(forcing_tg.data).all(axis=1)).flatten()\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(forcing_tg.isel(time=i_timesteps_w_data).data) #remove missing values for PCA\n",
    "            pcs = pca.transform(forcing_tg.isel(time=i_timesteps_w_data).data)\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            ds['surge'][i_member,i_timesteps_w_data,i_tg] = np.sum(tg_coefs[np.isfinite(tg_coefs)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "     \n",
    "    ds['surge'] = ds['surge'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    \n",
    "    #store:\n",
    "    model_path = os.path.join('/home/jovyan/CMIP6cf/output/surge_timeseries/',ds.source_id)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    \n",
    "    ds['surge'].to_dataset(name='surge').to_netcdf(os.path.join(model_path,k.replace('.','_')+'.nc'),mode='w')\n",
    "    ds.close()\n",
    "        \n",
    "        #ddict_surges[k] = ds['surges'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    \n",
    "    #save the data instead of storing it in dictionary? probably, don't need it afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951c09d-2ed8-4fdc-9489-10dc6b1d5086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
