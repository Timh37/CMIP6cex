{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_864/1987646690.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import dask\n",
    "import intake\n",
    "import fsspec\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.preprocessing import combined_preprocessing\n",
    "from xmip.postprocessing import merge_variables, combine_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_to_era5(ds,era5_grid):\n",
    "    \"\"\"wrapper around xesmf regridding\"\"\"\n",
    "    regridder = xe.Regridder(ds,era5_grid,'bilinear',ignore_degenerate=True)\n",
    "    \n",
    "    return regridder(ds,keep_attrs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_864/2890987920.py:26: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat_data.to_dataset_dict(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='11' class='' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [11/11 00:02&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "my_models = ['BCC-CSM2-MR','CESM2''CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5','EC-Earth3',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "\n",
    "'''\n",
    "my_models = ['HadGEM3-GC31-MM','MPI-ESM1-2-HR']\n",
    "\n",
    "col = google_cmip_col()\n",
    "experiment_id='ssp585'\n",
    "source_id = my_models\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':False\n",
    "}\n",
    "\n",
    "cat_data = col.search(\n",
    "    source_id=source_id,\n",
    "    experiment_id=['ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=['psl','sfcWind']\n",
    ")\n",
    "ddict = cat_data.to_dataset_dict(**kwargs)\n",
    "#ddict = cat_data.to_dataset_dict(**kwargs,preprocess=combined_preprocessing) # a lot of 'renaming failed' warnings here\n",
    "\n",
    "#NB: I'm not applying any preprocessing/renaming here at the moment because the regridding seems to work fine. The atmospheric fields are typically a bit more straightforward to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03d613d7-8240-489d-b908-0b85f16b8579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/xmip/postprocessing.py:157: UserWarning: ScenarioMIP.MOHC.HadGEM3-GC31-MM.ssp585.r2i1p1f3.day.gn.none.sfcWind failed to combine with :cannot align objects with join='exact' where index/labels/sizes are not equal along these coordinates (dimensions): 'time' ('time',)\n",
      "  warnings.warn(f\"{cmip6_dataset_id(ds)} failed to combine with :{e}\")\n"
     ]
    }
   ],
   "source": [
    "ddict_merged = merge_variables(ddict) #skips model realizations for which the different variables have a different time coverage (see warnings), need to fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56eefb1e-9ab3-49e7-b14a-a51be3693d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HadGEM3-GC31-MM.gn.ssp585.day.r1i1p1f3',\n",
       " 'MPI-ESM1-2-HR.gn.ssp585.day.r1i1p1f1',\n",
       " 'HadGEM3-GC31-MM.gn.ssp585.day.r4i1p1f3',\n",
       " 'MPI-ESM1-2-HR.gn.ssp585.day.r2i1p1f1']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reqVars = ['sfcWind','psl'] #need both these variables\n",
    "ddict_has_reqVars = {k: v for k, v in ddict_merged.items() if set(reqVars).issubset(list(ddict_merged[k].variables))} #drop datasets which don't have both\n",
    "list(ddict_has_reqVars.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5dcce72-cf81-4c1d-857f-560c48c2267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmip.postprocessing import _match_datasets\n",
    "from collections import Counter\n",
    "\n",
    "def filter_ddict_most_common_ipf(ddict): #like combine_datasets but without actually combining the datasets\n",
    "    filtered_ddict = {k: v for k, v in ddict.items()}\n",
    "    ds_dict = {k: v for k, v in ddict.items()}\n",
    "    \n",
    "    while len(ds_dict) > 0:\n",
    "        # The order does not matter here, so just choose the first key\n",
    "        k = list(ds_dict.keys())[0]\n",
    "        ds = ds_dict.pop(k)\n",
    "        matched_datasets = _match_datasets(ds, ds_dict, match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id'], pop=True)\n",
    "        \n",
    "        member_ids = [ds.member_id.data[0] for ds in matched_datasets]\n",
    "        member_ids.sort()\n",
    "        \n",
    "        ipf_ids = [s[s.find('i'):] for s in member_ids] #separate 'ipf' from 'r'\n",
    "        \n",
    "        most_common_ipf = Counter(ipf_ids).most_common()[0][0]\n",
    "        \n",
    "        key_prefix = \".\".join([ds.attrs[i] for i in ['source_id', 'grid_label', 'experiment_id', 'table_id']])\n",
    "  \n",
    "        keys_to_drop = [key_prefix+\".\"+member_id for member_id in member_ids if most_common_ipf not in member_id]\n",
    "        [filtered_ddict.pop(k) for k in keys_to_drop]\n",
    "        \n",
    "    return filtered_ddict\n",
    "\n",
    "ddict_filtered = filter_ddict_most_common_ipf(ddict_has_reqVars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cda5ef23-aedd-40c2-a9fa-e5e841894898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nddict_concat = combine_datasets( #this becomes pretty slow for large queries\\n    ddict_filtered,\\n    concat_realizations_most_common_ipf,\\n    match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id']\\n)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concat_realizations_most_common_ipf(ds_list):\n",
    "    '''custom function that concatenates only the realizations of the most common 'ipf' combination,\n",
    "    takes the first sorted 'ipf' if multiple 'ipf' are equally common'''\n",
    "    member_ids = [ds.member_id.data[0] for ds in ds_list]\n",
    "    \n",
    "    member_ids.sort() #often i1 is the baseline?\n",
    "\n",
    "    ipf_ids = [s[s.find('i'):] for s in member_ids] #separate 'ipf' from 'r'\n",
    "    from collections import Counter\n",
    "\n",
    "    most_common_ipf = Counter(ipf_ids).most_common()[0][0]\n",
    "\n",
    "    # find unique members and decide which values of 'ipf' give the most members/variants?\n",
    "    # pick only the matching datasets from the list\n",
    "    ds_pick = [ds for ds in ds_list if (most_common_ipf in ds.member_id.data[0])]\n",
    "\n",
    "    return xr.concat(ds_pick, dim='member_id')\n",
    "\n",
    "'''\n",
    "ddict_concat = combine_datasets( #this becomes pretty slow for large queries\n",
    "    ddict_filtered,\n",
    "    concat_realizations_most_common_ipf,\n",
    "    match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id']\n",
    ")'''\n",
    "#NB: This leaves multiple datasets for the same model with different grid labels. \n",
    "# That is not straightforward to filter out if querying multiple experiments. \n",
    "# Since this occurs only for a few models it is probably OK to just save these and remove them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients around TGs\n",
    "\n",
    "era5_grid = xr.Dataset( #the ERA5 grid used to derive the MLR coefficients\n",
    "        {\n",
    "            \"longitude\": ([\"longitude\"], np.arange(-40,30,1)+1/2, {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,10,-1)-1/2, {\"units\": \"degrees_north\"}),\n",
    "        }\n",
    "    )\n",
    "\n",
    "#get coordinates of n x n degree grids around each tide gauge\n",
    "num_degr = 2\n",
    "lat_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "lon_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "\n",
    "for t,tg in enumerate(mlrcoefs.tg.values):\n",
    "    lat_ranges[t,:] = era5_grid.latitude[((era5_grid.latitude>=(mlrcoefs.sel(tg=tg).lat-num_degr/2)) & (era5_grid.latitude<=(mlrcoefs.sel(tg=tg).lat+num_degr/2)))][0:2]\n",
    "    lon_ranges[t,:] = era5_grid.longitude[((era5_grid.longitude>=(mlrcoefs.sel(tg=tg).lon-num_degr/2)) & (era5_grid.longitude<=(mlrcoefs.sel(tg=tg).lon+num_degr/2)))][0:2]\n",
    "\n",
    "#create da's to index the CMIP6 files with:\n",
    "lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlrcoefs.tg,'lon_around_tg':[0,1]})\n",
    "lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlrcoefs.tg,'lat_around_tg':[0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89be42e1-5a0a-41af-912c-4e9e3b8ab78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3153df03e20f42ce95c95818be158b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ddict_subsetted = ddict_filtered#copy dictionary with concatenated realizations\n",
    "for key in tqdm(ddict_filtered):\n",
    "    ds = ddict_filtered[key]\n",
    "\n",
    "    #change longitude coordinates to -180 -> 180 (avoids getting NaNs at the 0-meridian)\n",
    "    lon_coord = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    ds.coords[lon_coord] = ((ds.coords[lon_coord] + 180) % 360) - 180 #wrap around 0\n",
    "    ds = ds.reindex({ lon_coord : np.sort(ds[lon_coord])})\n",
    "    \n",
    "    regridded_ds = regrid_to_era5(ds,era5_grid) #regrid to the ERA5 grid bilinearly\n",
    "    ds_around_tgs = regridded_ds.sel(latitude=lats_da,longitude=lons_da) #subset at n x n degree grids around TGs\n",
    "    ddict_subsetted[key] = ds_around_tgs\n",
    "    ds_around_tgs.to_netcdf(key+'.nc',mode='w') #test storing the subsetted output, takes ~3-4 min per file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
