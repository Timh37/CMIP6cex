{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import dask\n",
    "import intake\n",
    "import fsspec\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.preprocessing import combined_preprocessing,_drop_coords\n",
    "from xmip.postprocessing import merge_variables, combine_datasets, concat_experiments,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_to_era5(ds,era5_grid):\n",
    "    \"\"\"wrapper around xesmf regridding\"\"\"\n",
    "    regridder = xe.Regridder(ds,era5_grid,'bilinear',ignore_degenerate=True)\n",
    "    \n",
    "    return regridder(ds,keep_attrs=True)\n",
    "\n",
    "def shorten_ssp_runs(ddict_in,end_year):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        if 'ssp' in k:\n",
    "            ddict_out[k] = v.sel(time=slice(None, str(end_year)))\n",
    "        else:\n",
    "            ddict_out[k] = v\n",
    "    return ddict_out\n",
    "\n",
    "def shorten_historical_runs(ddict_in,start_year):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        if 'historical' in k:\n",
    "            ddict_out[k] = v.sel(time=slice(str(start_year), None))\n",
    "        else:\n",
    "            ddict_out[k] = v\n",
    "    return ddict_out\n",
    "\n",
    "def drop_duplicate_timesteps(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        \n",
    "        unique_time, idx = np.unique(v.time,return_index=True)\n",
    "        \n",
    "        if len(v.time) != len(unique_time):\n",
    "            ddict_out[k] = v.isel(time=idx)\n",
    "            print('Dropping duplicate timesteps for:' + k)\n",
    "            \n",
    "    return ddict_out\n",
    "\n",
    "def drop_coords(ddict_in,coords_to_drop):\n",
    "    \n",
    "    for k, v in ddict_in.items():\n",
    "        \n",
    "        ddict_in[k] = v.drop_dims(coords_to_drop,errors=\"ignore\")\n",
    "          \n",
    "    return ddict_in\n",
    "\n",
    "def concat_realizations_most_common_ipf(ds_list):\n",
    "    '''custom function that concatenates only the realizations of the most common 'ipf' combination,\n",
    "    takes the first sorted 'ipf' if multiple 'ipf' are equally common'''\n",
    "    member_ids = [ds.member_id.data[0] for ds in ds_list]\n",
    "    \n",
    "    member_ids.sort() #often i1 is the baseline?\n",
    "\n",
    "    ipf_ids = [s[s.find('i'):] for s in member_ids] #separate 'ipf' from 'r'\n",
    "    from collections import Counter\n",
    "\n",
    "    most_common_ipf = Counter(ipf_ids).most_common()[0][0]\n",
    "\n",
    "    # find unique members and decide which values of 'ipf' give the most members/variants?\n",
    "    # pick only the matching datasets from the list\n",
    "    ds_pick = [ds for ds in ds_list if (most_common_ipf in ds.member_id.data[0])]\n",
    "    \n",
    "    #not ideal way to ensure coordinates are the same, otherwise differences in coordinates are padded with nans if using {join='outer'}\n",
    "    for idx,ds in enumerate(ds_pick):\n",
    "        if idx==0:\n",
    "            lat = ds['lat']\n",
    "            lon = ds['lon']\n",
    "        if ((ds['lat'].shape==lat.shape) & (ds['lon'].shape==lon.shape)):\n",
    "            ds['lat'] = lat\n",
    "            ds['lon'] = lon\n",
    "        ds_pick[idx] = ds\n",
    "        \n",
    "    return xr.concat(ds_pick, dim='member_id', join='outer', coords='minimal',compat='override') #return xr.concat(ds_pick, dim='member_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712/434113886.py:27: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat_data.to_dataset_dict(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8' class='' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8/8 00:03&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "my_models = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5','EC-Earth3',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "\n",
    "'''\n",
    "my_models = ['MPI-ESM1-2-HR']\n",
    "col = google_cmip_col()\n",
    "experiment_id='ssp585'\n",
    "source_id = my_models\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':False\n",
    "}\n",
    "\n",
    "cat_data = col.search(\n",
    "    source_id=source_id,\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=['psl','sfcWind'],\n",
    "    member_id=['r1i1p1f1','r2i1p1f1'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "ddict = cat_data.to_dataset_dict(**kwargs)\n",
    "#ddict = cat_data.to_dataset_dict(**kwargs,preprocess=combined_preprocessing) # a lot of 'renaming failed' warnings here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9f6f1-8b0a-4e6a-922b-cc9461aba1c7",
   "metadata": {},
   "source": [
    "**NB: I don't seem to need any preprocessing. If I turn it on I get a lot of renaming failed warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9769a6a5-dbb9-42d8-8119-81d044abce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = shorten_ssp_runs(ddict,2100)\n",
    "ddict = shorten_historical_runs(ddict,1950)#may also want to shorten historical, may not need 1850-1950?\n",
    "ddict = drop_duplicate_timesteps(ddict) #CESM2-WACCM has duplicate timeseries\\\n",
    "ddict = drop_coords(ddict,['bnds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f681d-a3c7-40ea-852a-b721cb3b98b6",
   "metadata": {},
   "source": [
    "Code for listing the instance_id of runs with missing timesteps:\n",
    "```python\n",
    "for k,v in ddict.items():\n",
    "    if (len(v.time) < len(np.arange(1850,2015))*12*30):\n",
    "    #if len(v.time) < len(np.arange(2015,2101))*12*30:\n",
    "        print(v.attrs['intake_esm_attrs:zstore'][0:-1].replace('gs://cmip6/','').replace('/','.'))\n",
    "```\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d613d7-8240-489d-b908-0b85f16b8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ddict_merged = merge_variables(ddict,merge_kwargs={'join':'outer'}) #produces large chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cda5ef23-aedd-40c2-a9fa-e5e841894898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ddict_concat = combine_datasets(\n",
    "        ddict_merged,\n",
    "        concat_realizations_most_common_ipf,\n",
    "        match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id']\n",
    "    )\n",
    "#NB: This leaves multiple datasets for the same model with different grid labels. Probably need function to keep grid label with most variants?\n",
    "# Since this occurs only for a few models it is probably OK to just save these and remove them later?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c494a1-14ed-4d4a-817e-935bd155cc95",
   "metadata": {},
   "source": [
    "**NB: Padding with NaNs above creates large time chunks for runs with missing timesteps. Adding the missing timesteps to Google Cloud would solve this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed91f71-417a-416a-8e6c-f8e0957bbae9",
   "metadata": {},
   "source": [
    "**NB: `concat_realizations_most_common_ipf()` keeps multiple datasets for the same model with different grid labels. May be useful to filter these out too later, but it only occurs for a few models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92478f-f7ec-4177-878f-efe18bd294f0",
   "metadata": {},
   "source": [
    "Do the regridding & subsetting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs\n",
    "\n",
    "era5_grid = xr.Dataset( #the ERA5 grid used to derive the MLR coefficients\n",
    "        {\n",
    "            \"longitude\": ([\"longitude\"], np.arange(-40,30,1)+1/2, {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,10,-1)-1/2, {\"units\": \"degrees_north\"}),\n",
    "        }\n",
    "    )\n",
    "\n",
    "#get coordinates of n x n degree grids around each tide gauge\n",
    "num_degr = 2\n",
    "lat_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "lon_ranges = np.zeros((len(mlrcoefs.tg),2))\n",
    "\n",
    "for t,tg in enumerate(mlrcoefs.tg.values):\n",
    "    lat_ranges[t,:] = era5_grid.latitude[((era5_grid.latitude>=(mlrcoefs.sel(tg=tg).lat-num_degr/2)) & (era5_grid.latitude<=(mlrcoefs.sel(tg=tg).lat+num_degr/2)))][0:2]\n",
    "    lon_ranges[t,:] = era5_grid.longitude[((era5_grid.longitude>=(mlrcoefs.sel(tg=tg).lon-num_degr/2)) & (era5_grid.longitude<=(mlrcoefs.sel(tg=tg).lon+num_degr/2)))][0:2]\n",
    "\n",
    "#create da's to index the CMIP6 files with:\n",
    "lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlrcoefs.tg,'lon_around_tg':[0,1]})\n",
    "lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlrcoefs.tg,'lat_around_tg':[0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89be42e1-5a0a-41af-912c-4e9e3b8ab78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a566d64d59e942cfaf07cae51fee68df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict_subsetted = {k: v for k, v in ddict_concat.items()} #copy dictionary with concatenated realizations\n",
    "for key,ds in tqdm(ddict_subsetted.items()):\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True) #get rid of dcpp_init_year dimension\n",
    "    \n",
    "    #change longitude coordinates to -180 -> 180 (avoids getting NaNs at the 0-meridian)\n",
    "    lon_coord = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    ds.coords[lon_coord] = ((ds.coords[lon_coord] + 180) % 360) - 180 #wrap around 0\n",
    "    ds = ds.reindex({ lon_coord : np.sort(ds[lon_coord])})\n",
    "    \n",
    "    ds = regrid_to_era5(ds,era5_grid).sel(latitude=lats_da,longitude=lons_da) #regrid to ERA5 grid and index at n x n degree grids around each TG\n",
    "    ddict_subsetted[key] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1620b3-a350-4e3f-b0e8-24bb0b1ff592",
   "metadata": {},
   "source": [
    "**NB: If desired, we could store the subsetted data at this point?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527442-521e-4730-95e7-09e3dcc9b58b",
   "metadata": {},
   "source": [
    "Next, derive surges from the atmospheric forcing:\n",
    "- combine the historical and ssp data (**need custom combination if quering multiple SSPs at once?**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e410a3f9-881f-43c4-9996-8ebf4bbdc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_forcing = combine_datasets(ddict_subsetted,\n",
    "                        _concat_sorted_time,\n",
    "                       match_attrs =['source_id', 'grid_label','table_id'] ) #appends SSP to historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47632cb4-545a-46a8-91d7-fd6f3bf4e14f",
   "metadata": {},
   "source": [
    "- generate the normalized forcing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b52f0c6f-ac79-43cf-9b74-df3723ae5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate forcing to compute surges with\n",
    "for k,v in ddict_forcing.items():\n",
    "    attrs = v.attrs\n",
    "    \n",
    "    v['sfcWind_sqd'] = v['sfcWind']**2 #add wind squared\n",
    "    v['sfcWind_cbd'] = v['sfcWind']**3 #add wind cubed\n",
    "    \n",
    "    v = (v-v.mean(dim='time'))/v.std(dim='time',ddof=0) #normalize\n",
    "    v.attrs = attrs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * num_degr * num_degr))\n",
    "    v['forcing'] = v[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"forcing_var\") \n",
    "    v['forcing'] = v['forcing'].transpose(\"time\",\"forcing_var\",\"lon_around_tg\",...).stack(f=['forcing_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    ddict_forcing[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3b439-c794-4826-ba59-1c8afa26122c",
   "metadata": {},
   "source": [
    "- derive the principal components and multiply with regression coefficients derived from ERA5 (**need to work on a way to handle NaNs in timeseries**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edadaaeb-72d8-4fba-aacc-4d4a08693733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cb3a06f46d4ef380dbbbef8813dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict_surges = defaultdict(dict)\n",
    "for k,ds in tqdm(ddict_forcing.items()): #loop over datasets\n",
    "    forcing = ds.forcing.load() #load forcing data array into memory (for all member_id & tg for current dataset)\n",
    "    \n",
    "    ds['surges'] = ( ('member_id','time','tg'), np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(ds.tg)) )) #initialize output\n",
    "            \n",
    "    for imember,member in enumerate(ds.member_id):\n",
    "        forcing_mem = forcing.sel(member_id=member)\n",
    "\n",
    "        for itg,tg in enumerate(ds.tg):\n",
    "            #get model forcing at TG\n",
    "            forcing_tg = forcing_mem.sel(tg=tg) \n",
    "            \n",
    "            #get MLR coefficients at TG\n",
    "            tg_coefs = mlrcoefs.mlrcoefs.sel(tg=tg)\n",
    "            num_pcs = int(np.sum(np.isfinite(tg_coefs)))-1 #number of coefs = number of PCs to derive, intercept doesn't count\n",
    "\n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(forcing_tg.data)\n",
    "            pcs = pca.transform(forcing_tg.data)\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            ds['surges'][imember,:,itg] = np.sum(tg_coefs[np.isfinite(tg_coefs)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "            \n",
    "    ddict_surges[k] = ds['surges'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    #+store?        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfde47-44a7-4a29-8b60-81db175f2e89",
   "metadata": {},
   "source": [
    "**NB: not sure if it is possible to do this without a loop? The alternative is 'eofs.xarray', but I don't think that can be applied along axes either, and that results in different signs for the principal components because of sklearn.svd_flip()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a176ea-88cf-4f02-a2e8-8821c3da7ac3",
   "metadata": {},
   "source": [
    "=> Takes about 12 minutes per model per member for historical+ssp (1850-2100), or 8 minutes if I preselect (1950-2100). I think we will analyse approximately 150 members in total times two scenarios, so that would take 40-60h?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b25a90-7f23-490f-8b83-18304b080a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
