{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_331/3371931909.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import fsspec\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.preprocessing import combined_preprocessing,_drop_coords\n",
    "from xmip.postprocessing import merge_variables, combine_datasets, concat_experiments,_concat_sorted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_units_to_m(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        \n",
    "        assert v.pr.units == 'kg m-2 s-1'\n",
    "        \n",
    "        #convert 'kg m-2 s-1' to daily accumulated 'm'\n",
    "        with xr.set_options(keep_attrs=True): \n",
    "            v['pr'] = 24*3600*v['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "        v.pr.attrs['units'] = 'm'\n",
    "        \n",
    "        ddict_out[k] = v\n",
    "        \n",
    "    return ddict_out\n",
    "\n",
    "def preselect_years(ddict_in,start_year,end_year):\n",
    "    ddict_out = defaultdict(dict)\n",
    "    \n",
    "    if start_year>=end_year:\n",
    "        raise Exception(\"Start year must come before end year.\")\n",
    "    \n",
    "    if start_year>2014: #only using SSP\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif end_year<=2014: #only using historical\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif ((start_year<=2014) & (end_year>2014)): #using both\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(None, str(end_year)))\n",
    "            elif 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), None))\n",
    "    return ddict_out #NB: may result in no timesteps being selected at all\n",
    "\n",
    "def drop_duplicate_timesteps(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        \n",
    "        unique_time, idx = np.unique(v.time,return_index=True)\n",
    "        \n",
    "        if len(v.time) != len(unique_time):\n",
    "            ddict_out[k] = v.isel(time=idx)\n",
    "            print('Dropping duplicate timesteps for:' + k)\n",
    "            \n",
    "    return ddict_out\n",
    "\n",
    "def drop_coords(ddict_in,coords_to_drop):\n",
    "    \n",
    "    for k, v in ddict_in.items():\n",
    "        \n",
    "        ddict_in[k] = v.drop_dims(coords_to_drop,errors=\"ignore\")\n",
    "          \n",
    "    return ddict_in\n",
    "\n",
    "def concat_realizations_most_common_ipf(ds_list):\n",
    "    '''custom function that concatenates only the realizations of the most common 'ipf' combination,\n",
    "    takes the first sorted 'ipf' if multiple 'ipf' are equally common'''\n",
    "    member_ids = [ds.member_id.data[0] for ds in ds_list]\n",
    "    \n",
    "    member_ids.sort() #often i1 is the baseline?\n",
    "\n",
    "    ipf_ids = [s[s.find('i'):] for s in member_ids] #separate 'ipf' from 'r'\n",
    "    from collections import Counter\n",
    "\n",
    "    most_common_ipf = Counter(ipf_ids).most_common()[0][0]\n",
    "\n",
    "    # find unique members and decide which values of 'ipf' give the most members/variants?\n",
    "    # pick only the matching datasets from the list\n",
    "    ds_pick = [ds for ds in ds_list if (most_common_ipf in ds.member_id.data[0])]\n",
    "    \n",
    "    #not ideal way to ensure coordinates are the same, otherwise differences in coordinates are padded with nans if using {join='outer'}\n",
    "    for idx,ds in enumerate(ds_pick):\n",
    "        if idx==0:\n",
    "            lat = ds['lat']\n",
    "            lon = ds['lon']\n",
    "        if ((ds['lat'].shape==lat.shape) & (ds['lon'].shape==lon.shape)):\n",
    "            ds['lat'] = lat\n",
    "            ds['lon'] = lon\n",
    "        ds_pick[idx] = ds\n",
    "        \n",
    "    return xr.concat(ds_pick, dim='member_id', join='outer', coords='minimal',compat='override') #return xr.concat(ds_pick, dim='member_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_331/3484263743.py:24: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat_data.to_dataset_dict(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='262' class='' max='262' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [262/262 01:09&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_models = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5','EC-Earth3',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "\n",
    "#my_models = ['MPI-ESM1-2-HR']\n",
    "col = google_cmip_col()\n",
    "experiment_id='ssp585'\n",
    "source_id = my_models\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':False\n",
    "}\n",
    "\n",
    "cat_data = col.search(\n",
    "    source_id=source_id,\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=['pr'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "ddict = cat_data.to_dataset_dict(**kwargs)\n",
    "#ddict = cat_data.to_dataset_dict(**kwargs,preprocess=combined_preprocessing) # a lot of 'renaming failed' warnings here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9f6f1-8b0a-4e6a-922b-cc9461aba1c7",
   "metadata": {},
   "source": [
    "**NB: I don't seem to need any preprocessing. If I turn it on I get a lot of renaming failed warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9769a6a5-dbb9-42d8-8119-81d044abce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicate timesteps for:ScenarioMIP.NCAR.CESM2-WACCM.ssp585.r1i1p1f1.day.pr.gn.gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2-WACCM/ssp585/r1i1p1f1/day/pr/gn/v20200702/.nan.20200702\n"
     ]
    }
   ],
   "source": [
    "ddict = pr_units_to_m(ddict)\n",
    "ddict = drop_duplicate_timesteps(ddict) #CESM2-WACCM has duplicate timeseries\n",
    "ddict = preselect_years(ddict,1850,2100)\n",
    "ddict = drop_coords(ddict,['bnds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cda5ef23-aedd-40c2-a9fa-e5e841894898",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ddict_concat_mem = combine_datasets(\n",
    "        ddict,\n",
    "        concat_realizations_most_common_ipf,\n",
    "        match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id']\n",
    "    )\n",
    "#NB: This leaves multiple datasets for the same model with different grid labels. Probably need function to keep grid label with most variants?\n",
    "# Since this occurs only for a few models it is probably OK to just save these and remove them later?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c494a1-14ed-4d4a-817e-935bd155cc95",
   "metadata": {},
   "source": [
    "**NB: Padding with NaNs above creates large time chunks for runs with missing timesteps. When loading this data into memory the kernel seems to crash. How to solve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed91f71-417a-416a-8e6c-f8e0957bbae9",
   "metadata": {},
   "source": [
    "**NB: `concat_realizations_most_common_ipf()` keeps multiple datasets for the same model with different grid labels. May be useful to filter these out too later, but it only occurs for a few models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76527442-521e-4730-95e7-09e3dcc9b58b",
   "metadata": {},
   "source": [
    "Combine the historical and ssp data if desired (**need custom combination if quering multiple SSPs at once?**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e410a3f9-881f-43c4-9996-8ebf4bbdc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_concat = combine_datasets(ddict_concat_mem,\n",
    "                        _concat_sorted_time,\n",
    "                       match_attrs =['source_id', 'grid_label','table_id'] ) #appends SSP to historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92478f-f7ec-4177-878f-efe18bd294f0",
   "metadata": {},
   "source": [
    "Do the subsetting at grid cells nearest to the tide gauges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e159ab552b4c8694607456b91f7bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs\n",
    "\n",
    "ddict_near_tgs = defaultdict(dict)\n",
    "\n",
    "for key,ds in tqdm(ddict_concat.items()):\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True)\n",
    "    \n",
    "    #change longitude coordinates to -180 -> 180 (avoids getting NaNs at the 0-meridian)\n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    '''\n",
    "    #I think this is probably not necessary, Haversine formula should work regardlessly?\n",
    "    ds.coords[lon_name] = ((ds.coords[lon_name] + 180) % 360) - 180 #wrap around 0\n",
    "    ds = ds.reindex({ lon_name : np.sort(ds[lon_name])})\n",
    "    '''\n",
    "    \n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-mlrcoefs.tg.lat) )**2 +\n",
    "        np.cos((np.pi/180)*mlrcoefs.tg.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-mlrcoefs.tg.lon))**2) )\n",
    "\n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name])\n",
    "    ds_nearest = ds[idx_nearest]\n",
    "    \n",
    "    ds_nearest = ds_nearest.rename_vars({'lon':'gridcell_lon','lat':'gridcell_lat'}) #store nearest grid cell coordinates for later\n",
    "    ds_nearest = ds_nearest.assign_coords(lon=mlrcoefs.lon,lat=mlrcoefs.lat) #replace with TG coordinates\n",
    "    \n",
    "    ddict_near_tgs[key] = ds_nearest\n",
    "    #ds_nearest.load()\n",
    "    #store at this point?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
