{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_549/2963243200.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffe4773-8e14-4bb0-96bb-41a4c5a89caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cat_to_max_num_realizations(cmip6_cat):\n",
    "    '''Reduce grid labels in pangeo cmip6 catalogue by \n",
    "    keeping grid_label and 'ipf' identifier combination with most datasets (=most realizations if using require_all_on)'''\n",
    "    df = cmip6_cat.df\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    df['ipf'] = [s[s.find('i'):] for s in df.member_id] #extract 'ipf' from 'ripf'\n",
    "\n",
    "    #generate list of tuples of (source_id,ipf,grid_label) that provide most realizations (note this will omit realizations not available at this grid but possibly at others)\n",
    "    max_num_ds_tuples = df.groupby(['source_id','ipf'])['grid_label'].value_counts().groupby(level=0).head(1).index.to_list() #head(1) gives (first) max. value since value_counts sorts max to min\n",
    "    df_filter = pd.DataFrame(max_num_ds_tuples,columns=['source_id','ipf','grid_label']) #generate df to merge catalogue df on\n",
    "    \n",
    "    df = df_filter.merge(right=df, on = ['source_id','ipf','grid_label'], how='left') #do the subsetting\n",
    "    df = df.drop(columns=['ipf']) #clean up\n",
    "    df= df[cols]\n",
    "\n",
    "    cmip6_cat.esmcat._df = df #(columns now ordered differently, probably not an issue?)\n",
    "    return cmip6_cat\n",
    "\n",
    "def drop_vars_from_cat(cmip6_cat,vars_to_drop):\n",
    "    cmip6_cat.esmcat._df = cmip6_cat.df.drop(cmip6_cat.df[cmip6_cat.df.variable_id.isin(vars_to_drop)].index).reset_index(drop=True)\n",
    "    return cmip6_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preselect_years(ddict_in,start_year,end_year):\n",
    "    '''select range of years of datasets'''\n",
    "    ddict_out = defaultdict(dict)\n",
    "    \n",
    "    assert start_year<end_year\n",
    "        \n",
    "    if start_year>2014: #only using SSP\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif end_year<=2014: #only using historical\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif ((start_year<=2014) & (end_year>2014)): #using both\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice('2015', str(end_year)))\n",
    "            elif 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), '2014'))\n",
    "    return ddict_out #NB: may result in no timesteps being selected at all\n",
    "\n",
    "def pr_units_to_m(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        assert v.pr.units == 'kg m-2 s-1'\n",
    "        \n",
    "        with xr.set_options(keep_attrs=True): #convert 'kg m-2 s-1' to daily accumulated 'm'\n",
    "            v['pr'] = 24*3600*v['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "        v.pr.attrs['units'] = 'm'\n",
    "        \n",
    "        ddict_out[k] = v\n",
    "    return ddict_out\n",
    "\n",
    "def drop_duplicate_timesteps(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():  \n",
    "        unique_time, idx = np.unique(v.time,return_index=True)\n",
    "        \n",
    "        if len(v.time) != len(unique_time):\n",
    "            ddict_out[k] = v.isel(time=idx)\n",
    "            print('Dropping duplicate timesteps for:' + k)   \n",
    "    return ddict_out\n",
    "\n",
    "def drop_coords(ddict_in,coords_to_drop):\n",
    "    for k, v in ddict_in.items():\n",
    "        ddict_in[k] = v.drop_dims(coords_to_drop,errors=\"ignore\")\n",
    "    return ddict_in\n",
    "\n",
    "def align_lonlat(ds_list):\n",
    "    aligned_ds_list = []\n",
    "    for ds in ds_list: #list of ds can't seem to be passed to xr.align instead\n",
    "        a,b = xr.align(ds_list[0],ds,join='override',exclude=['time','member_id'])\n",
    "        aligned_ds_list.append(b)\n",
    "    return aligned_ds_list\n",
    "\n",
    "def merge_variables_aligning_lonlat(ds_list):\n",
    "    aligned_ds_list = align_lonlat(ds_list) #override same-dimension lon/lat prior to concatenating (ensures lon/lats are not padded)\n",
    "    return xr.merge(aligned_ds_list, join='outer',compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bed2f12-9b92-4110-aa12-7a045937e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gridcells_nearest_to_tgs(tg_ds,ds):\n",
    "    '''\n",
    "    tg_ds = xr.DataSet containing 'lon' and 'lat' coordinates of tide gauges\n",
    "    ds    = xr.DataSet containing CMIP6 data to subset\n",
    "    '''\n",
    "    \n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #compute distances between TG coordinates and grid cell centers\n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-tg_ds.lat) )**2 +\n",
    "        np.cos((np.pi/180)*tg_ds.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-tg_ds.lon))**2) )\n",
    "    \n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name]) #find indices of nearest grid cells\n",
    "    ds_subsetted = ds[idx_nearest] #subset ds at nearest grid cells\n",
    "    \n",
    "    ds_subsetted = ds_subsetted.rename_vars({'lon':'gridcell_lon','lat':'gridcell_lat'}) #keep coordinates of nearest grid cells\n",
    "    ds_subsetted = ds_subsetted.assign_coords(lon=tg_ds.lon,lat=tg_ds.lat) #replace coordinates with TG coordinates\n",
    "    \n",
    "    return ds_subsetted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0a5ed7-c683-4c1e-818f-946d4d7bcb43",
   "metadata": {},
   "source": [
    "Query runs & manipulate catalogue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'pr'\n",
    "query_vars = ['sfcWind','pr','psl']\n",
    "\n",
    "my_models = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5','EC-Earth3',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "'''\n",
    "my_models = ['MPI-ESM1-2-HR']\n",
    "'''\n",
    "col = google_cmip_col()\n",
    "\n",
    "cat_data_ssp245 = col.search( #find instances providing all required variables for both historical & ssp245\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "\n",
    "\n",
    "cat_data_ssp585 = col.search( #find instances providing all required variables for both historical & ssp585\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "cat_data = cat_data_ssp585\n",
    "cat_data.esmcat._df = pd.concat([cat_data_ssp245.df,cat_data_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True) #all instances we want\n",
    "\n",
    "cat_data = reduce_cat_to_max_num_realizations(cat_data) #per model, select grid and 'ipf' combination providing most realizations\n",
    "query_vars.remove(variable)\n",
    "\n",
    "cat_data = drop_vars_from_cat(cat_data,query_vars) #we query only instances that provide all 'query_vars', but only process 'variable' in this script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cecae-2b3d-4a4d-a29a-752868fa9645",
   "metadata": {},
   "source": [
    "Open datasets into dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190fdaf1-4226-4f53-a1c6-ef0fe8a22b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_549/772692342.py:12: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat_data.to_dataset_dict(**kwargs) #open datasets into dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='199' class='' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [199/199 02:24&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_data.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':True #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "    #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "}\n",
    "\n",
    "ddict = cat_data.to_dataset_dict(**kwargs) #open datasets into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9f6f1-8b0a-4e6a-922b-cc9461aba1c7",
   "metadata": {},
   "source": [
    "**NB: I don't seem to need any preprocessing. If I turn it on I get a lot of renaming failed warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9769a6a5-dbb9-42d8-8119-81d044abce1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1575766974.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    if variable=='pr'\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "if variable=='pr':\n",
    "    ddict = pr_units_to_m(ddict)\n",
    "ddict = drop_duplicate_timesteps(ddict) #CESM2-WACCM has duplicate timeseries\n",
    "ddict = preselect_years(ddict,1850,2100)\n",
    "ddict = drop_coords(ddict,['bnds','nbnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915aabb7-d78d-4d84-ad0f-0e9e96f8d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_grid = xr.Dataset( #the ERA5 grid used to derive the MLR coefficients\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-30,21,1), {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(35,69,1), {\"units\": \"degrees_north\"}),})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c0b5a-3f22-4e77-ba19-f27f7b9fe58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}): #join=outer pads NaNs which result in large chunks for timeseries that differ in length\n",
    "    ddict_merged = combine_datasets(ddict,merge_variables_aligning_lonlat,match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id','variant_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92478f-f7ec-4177-878f-efe18bd294f0",
   "metadata": {},
   "source": [
    "Do the subsetting at grid cells nearest to the tide gauges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,ds in ddict_merged.items():\n",
    "    lon_coord = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    \n",
    "    ds.coords[lon_coord] = ((ds.coords[lon_coord] + 180) % 360) - 180 #wrap around 0\n",
    "    ds = ds.reindex({ lon_coord : np.sort(ds[lon_coord])})\n",
    "    ds.attrs[\"original_key\"] = key\n",
    "    ddict_merged[key] = ds\n",
    "\n",
    "\n",
    "ddict_eu = defaultdict(dict)\n",
    "ds_dict = {k: v for k, v in ddict_merged.items()}\n",
    "\n",
    "while len(ds_dict) > 0: #<- from xmip's combine_datasets\n",
    "    k = list(ds_dict.keys())[0]\n",
    "    ds = ds_dict.pop(k)\n",
    "\n",
    "    matched_datasets = _match_datasets(ds, ds_dict, ['source_id', 'grid_label'], pop=True) #find datasets belonging to same model/grid\n",
    "\n",
    "    regridder = xe.Regridder(matched_datasets[0],target_grid,'bilinear',ignore_degenerate=True)\n",
    "    \n",
    "    for matched_ds in matched_datasets:\n",
    "        first_ds,aligned_ds = xr.align(matched_datasets[0],matched_ds,join='override',exclude=['time','member_id','dcpp_init_year']) #make sure lon/lat coordinates are exactly the same\n",
    "\n",
    "        aligned_ds = aligned_ds.isel(dcpp_init_year=0,drop=True) #get rid of dcpp_init_year dimension\n",
    "        ddict_eu[matched_ds.original_key] = regridder(aligned_ds,keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33350914-b9d3-43c8-875d-38bfa83ac491",
   "metadata": {},
   "source": [
    "Store the datasets (directories structured per model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "013e49c6-208a-4a23-840c-144f08f2dadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b798ad01f4c64a1cb4fd6db346a75e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key,ds in tqdm(ddict_eu.items()):\n",
    "    model_path = os.path.join('/home/jovyan/CMIP6cf/output/subsetted_data/'+variable+'_europe/',ds.source_id)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    ds.to_netcdf(os.path.join(model_path,key.replace('.','_')+'.nc'),mode='w')\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07418a26-5aa7-4d10-99d9-79cf3d3b147e",
   "metadata": {},
   "source": [
    "^takes about an hour for hist+ssp245+ssp585 excluding EC-Earth3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
