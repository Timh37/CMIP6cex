{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204334ce-ab38-445f-aad0-ae91d07b400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_579/2340292891.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Open the subsetted CMIP6 `psl` & `sfcWind` data (**To-do: test if concatenating results in issues for many realizations (e.g., EC-Earth3)**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '/home/jovyan/CMIP6cf/output/subsetted_forcing/'\n",
    "\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for source_id in [s for s in os.listdir(in_dir) if s.startswith('.')==False]:\n",
    "    \n",
    "    experiment_ids = [s.split('_')[2] for s in os.listdir(os.path.join(in_dir,source_id)) if s.startswith('.')==False]\n",
    "    for experiment_id in set(experiment_ids): #for each experiment_id, open the datasets, concatenating all realizations:\n",
    "        \n",
    "        source_ds = xr.open_mfdataset(os.path.join(in_dir,source_id,'*'+experiment_id+'*.nc'),join='outer',combine='nested',\n",
    "                                      compat='override',coords='minimal',concat_dim='member_id') #need to test this for large np. of realizations, like EC-Earth3\n",
    "        ddict[source_ds.original_key.rsplit('.',1)[0]] = source_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa5f5a-beb7-4e81-b6c7-e913faeed978",
   "metadata": {},
   "source": [
    "Concatenate matching realizations of historical & SSP run in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6b18b5-86aa-4075-b10a-986e817754f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssps = set([k.split('.')[2] for k in ddict.keys() if 'ssp' in k])\n",
    "\n",
    "ddict_concat = defaultdict(dict)\n",
    "\n",
    "for ssp in ssps:\n",
    "    ddict_ssp = defaultdict(dict)\n",
    "    \n",
    "    for k in ddict.keys():\n",
    "        if ((ssp in k) or ('historical' in k)):\n",
    "            ddict_ssp[k] = ddict[k]\n",
    "            \n",
    "    #append SSP to historical, only for realizations for which both experiments are provided (join=inner)\n",
    "    hist_ssp = combine_datasets(ddict_ssp,\n",
    "                                _concat_sorted_time,\n",
    "                                match_attrs =['source_id', 'grid_label','table_id'],combine_func_kwargs={'join':'inner'})\n",
    "    \n",
    "    for key,ds in hist_ssp.items(): #put back together in dictionary\n",
    "        ddict_concat[key+'.'+ssp] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba084-7e91-4236-8038-0f102af6a5fb",
   "metadata": {},
   "source": [
    "Sanity-check timeseries length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e669831-6ae9-4d7c-acbf-337047c2e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ddict_concat.items():\n",
    "    num_days = (v.time[-1]-v.time[0]).dt.days\n",
    "    assert (len(v.time) > .9*num_days) & (len(v.time) < 1.1*num_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1e7bc-b42d-4831-88f6-f466c61472bc",
   "metadata": {},
   "source": [
    "Generate forcing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b52404-5a12-416d-b07c-93167e7926f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate forcing to compute surges with\n",
    "for k,v in ddict_concat.items():\n",
    "    attrs = v.attrs\n",
    "    \n",
    "    v['sfcWind_sqd'] = v['sfcWind']**2 #add wind squared\n",
    "    v['sfcWind_cbd'] = v['sfcWind']**3 #add wind cubed\n",
    "    \n",
    "    v = (v-v.mean(dim='time'))/v.std(dim='time',ddof=0) #normalize (ignores nan by default?)\n",
    "    v.attrs = attrs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * num_degr * num_degr))\n",
    "    v['forcing'] = v[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"forcing_var\") \n",
    "    v['forcing'] = v['forcing'].transpose(\"time\",\"forcing_var\",\"lon_around_tg\",...).stack(f=['forcing_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    ddict_concat[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58fb60-3813-4edc-92b5-32b93741a8e9",
   "metadata": {},
   "source": [
    "Derive the principal components and multiply with regression coefficients derived from ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29e2fbb9-aa37-4b5d-a27b-6e84b946ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d50cfe-172c-4687-88cf-04c9111d0fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10ff3fe72654c0d86d7139076646c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedbd398dc2c4fe88329fe9b413caae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/dask/core.py:119: RuntimeWarning: invalid value encountered in divide\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by PCA.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#get principal components (using sklearn to keep deterministic signs consistent)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(num_pcs)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforcing_tg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_timesteps_w_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#remove missing values for PCA\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pcs \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(forcing_tg\u001b[38;5;241m.\u001b[39misel(time\u001b[38;5;241m=\u001b[39mi_timesteps_w_data)\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#multiply with ERA5 regression coefficients to compute surges\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:435\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA does not support sparse input. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncatedSVD for a possible alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 485\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 535\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/sklearn/utils/validation.py:929\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    927\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 929\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    930\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    932\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    933\u001b[0m         )\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    936\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 16)) while a minimum of 1 is required by PCA."
     ]
    }
   ],
   "source": [
    "ddict_surges = defaultdict(dict)\n",
    "for k,ds in ddict_concat.items(): #loop over datasets\n",
    "     \n",
    "    ds['surge'] = ( ('member_id','time','tg'), np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(ds.tg)) )) #initialize output\n",
    "            \n",
    "    for i_member,member in tqdm(enumerate(ds.member_id)):\n",
    "        forcing_mem = ds.forcing.sel(member_id=member).load() #load forcing data array into memory (for all tg for current dataset and member)\n",
    "        \n",
    "        for i_tg,tg in enumerate(ds.tg):\n",
    "            #get model forcing at TG\n",
    "            forcing_tg = forcing_mem.sel(tg=tg) \n",
    "            \n",
    "            #get MLR coefficients at TG\n",
    "            tg_coefs = mlrcoefs.mlrcoefs.sel(tg=tg)\n",
    "            num_pcs = int(np.sum(np.isfinite(tg_coefs)))-1 #number of coefs = number of PCs to derive, intercept doesn't count\n",
    "            \n",
    "            i_timesteps_w_data = np.argwhere(np.isfinite(forcing_tg.data).all(axis=1)).flatten()\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(forcing_tg.isel(time=i_timesteps_w_data).data) #remove missing values for PCA\n",
    "            pcs = pca.transform(forcing_tg.isel(time=i_timesteps_w_data).data)\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            ds['surge'][i_member,i_timesteps_w_data,i_tg] = np.sum(tg_coefs[np.isfinite(tg_coefs)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "     \n",
    "    ds['surge'] = ds['surge'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    \n",
    "    #store:\n",
    "    model_path = os.path.join('/home/jovyan/CMIP6cf/output/surge_timeseries/',ds.source_id)\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    \n",
    "    ds['surges'].to_dataset(name='surge').to_netcdf(os.path.join(model_path,k.replace('.','_')+'.nc'),mode='w')\n",
    "    \n",
    "        \n",
    "        #ddict_surges[k] = ds['surges'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    \n",
    "    #save the data instead of storing it in dictionary? probably, don't need it afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebac7a3-8ad5-464c-a61f-c0dd64bc7959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
