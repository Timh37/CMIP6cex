{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe4773-8e14-4bb0-96bb-41a4c5a89caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cat_to_max_num_realizations(cmip6_cat):\n",
    "    '''Reduce grid labels in pangeo cmip6 catalogue by \n",
    "    keeping grid_label and 'ipf' identifier combination with most datasets (=most realizations if using require_all_on)'''\n",
    "    df = cmip6_cat.df\n",
    "    cols = df.columns.tolist()\n",
    "    \n",
    "    df['ipf'] = [s[s.find('i'):] for s in df.member_id] #extract 'ipf' from 'ripf'\n",
    "\n",
    "    #generate list of tuples of (source_id,ipf,grid_label) that provide most realizations (note this will omit realizations not available at this grid but possibly at others)\n",
    "    max_num_ds_tuples = df.groupby(['source_id','ipf'])['grid_label'].value_counts().groupby(level=0).head(1).index.to_list() #head(1) gives (first) max. value since value_counts sorts max to min\n",
    "    df_filter = pd.DataFrame(max_num_ds_tuples,columns=['source_id','ipf','grid_label']) #generate df to merge catalogue df on\n",
    "    \n",
    "    df = df_filter.merge(right=df, on = ['source_id','ipf','grid_label'], how='left') #do the subsetting\n",
    "    df = df.drop(columns=['ipf']) #clean up\n",
    "    df= df[cols]\n",
    "\n",
    "    cmip6_cat.esmcat._df = df #(columns now ordered differently, probably not an issue?)\n",
    "    return cmip6_cat\n",
    "\n",
    "def drop_vars_from_cat(cmip6_cat,vars_to_drop):\n",
    "    cmip6_cat.esmcat._df = cmip6_cat.df.drop(cmip6_cat.df[cmip6_cat.df.variable_id.isin(vars_to_drop)].index).reset_index(drop=True)\n",
    "    return cmip6_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preselect_years(ddict_in,start_year,end_year):\n",
    "    '''select range of years of datasets'''\n",
    "    ddict_out = defaultdict(dict)\n",
    "    \n",
    "    assert start_year<end_year\n",
    "        \n",
    "    if start_year>2014: #only using SSP\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif end_year<=2014: #only using historical\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), str(end_year)))\n",
    "                \n",
    "    elif ((start_year<=2014) & (end_year>2014)): #using both\n",
    "        for k, v in ddict_in.items():\n",
    "            if 'ssp' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(None, str(end_year)))\n",
    "            elif 'historical' in k:\n",
    "                ddict_out[k] = v.sel(time=slice(str(start_year), None))\n",
    "    return ddict_out #NB: may result in no timesteps being selected at all\n",
    "\n",
    "def pr_units_to_m(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():\n",
    "        assert v.pr.units == 'kg m-2 s-1'\n",
    "        \n",
    "        with xr.set_options(keep_attrs=True): #convert 'kg m-2 s-1' to daily accumulated 'm'\n",
    "            v['pr'] = 24*3600*v['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "        v.pr.attrs['units'] = 'm'\n",
    "        \n",
    "        ddict_out[k] = v\n",
    "    return ddict_out\n",
    "\n",
    "def drop_duplicate_timesteps(ddict_in):\n",
    "    ddict_out = ddict_in\n",
    "    for k, v in ddict_in.items():  \n",
    "        unique_time, idx = np.unique(v.time,return_index=True)\n",
    "        \n",
    "        if len(v.time) != len(unique_time):\n",
    "            ddict_out[k] = v.isel(time=idx)\n",
    "            print('Dropping duplicate timesteps for:' + k)   \n",
    "    return ddict_out\n",
    "\n",
    "def drop_coords(ddict_in,coords_to_drop):\n",
    "    for k, v in ddict_in.items():\n",
    "        ddict_in[k] = v.drop_dims(coords_to_drop,errors=\"ignore\")\n",
    "    return ddict_in\n",
    "\n",
    "def align_lonlat(ds_list):\n",
    "    aligned_ds_list = []\n",
    "    for ds in ds_list: #list of ds can't seem to be passed to xr.align instead\n",
    "        a,b = xr.align(ds_list[0],ds,join='override',exclude=['time','member_id'])\n",
    "        aligned_ds_list.append(b)\n",
    "    return aligned_ds_list\n",
    "\n",
    "def merge_variables_aligning_lonlat(ds_list):\n",
    "    aligned_ds_list = align_lonlat(ds_list) #override same-dimension lon/lat prior to concatenating (ensures lon/lats are not padded)\n",
    "    return xr.merge(aligned_ds_list, join='outer',compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed2f12-9b92-4110-aa12-7a045937e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gridcells_nearest_to_tgs(tg_ds,ds):\n",
    "    '''\n",
    "    tg_ds = xr.DataSet containing 'lon' and 'lat' coordinates of tide gauges\n",
    "    ds    = xr.DataSet containing CMIP6 data to subset\n",
    "    '''\n",
    "    \n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #compute distances between TG coordinates and grid cell centers\n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-tg_ds.lat) )**2 +\n",
    "        np.cos((np.pi/180)*tg_ds.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-tg_ds.lon))**2) )\n",
    "    \n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name]) #find indices of nearest grid cells\n",
    "    ds_subsetted = ds[idx_nearest] #subset ds at nearest grid cells\n",
    "    \n",
    "    ds_subsetted = ds_subsetted.rename_vars({'lon':'gridcell_lon','lat':'gridcell_lat'}) #keep coordinates of nearest grid cells\n",
    "    ds_subsetted = ds_subsetted.assign_coords(lon=tg_ds.lon,lat=tg_ds.lat) #replace coordinates with TG coordinates\n",
    "    \n",
    "    return ds_subsetted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "my_models = ['BCC-CSM2-MR','CESM2','CESM2-WACCM','CMCC-ESM2','CMCC-CM2-SR5',\n",
    "                'GFDL-CM4','GFDL-ESM4','HadGEM3-GC31-MM','MIROC6','MPI-ESM1-2-HR','MRI-ESM2-0',\n",
    "                'NorESM2-MM','TaiESM1']\n",
    "'''\n",
    "my_models = ['EC-Earth3']\n",
    "\n",
    "col = google_cmip_col()\n",
    "\n",
    "cat_data_ssp245 = col.search( #find instances providing all required variables for both historical & ssp245\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=['sfcWind','psl','pr'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "\n",
    "\n",
    "cat_data_ssp585 = col.search( #find instances providing all required variables for both historical & ssp585\n",
    "    source_id=my_models,\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=['sfcWind','psl','pr'],\n",
    "    require_all_on=['source_id', 'member_id','grid_label']\n",
    ")\n",
    "cat_data = cat_data_ssp585\n",
    "cat_data.esmcat._df = pd.concat([cat_data_ssp245.df,cat_data_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True) #all instances we want\n",
    "\n",
    "cat_data = reduce_cat_to_max_num_realizations(cat_data) #per model, select grid and 'ipf' combination providing most realizations\n",
    "cat_data = drop_vars_from_cat(cat_data,['psl','sfcWind']) #we query only instances that also provide 'pr', but don't process 'pr' it in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fdaf1-4226-4f53-a1c6-ef0fe8a22b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "kwargs = {\n",
    "    'zarr_kwargs':{\n",
    "        'consolidated':True,\n",
    "        'use_cftime':True\n",
    "    },\n",
    "    'aggregate':True #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "    #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "}\n",
    "\n",
    "ddict = cat_data.to_dataset_dict(**kwargs) #open datasets into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9f6f1-8b0a-4e6a-922b-cc9461aba1c7",
   "metadata": {},
   "source": [
    "**NB: I don't seem to need any preprocessing. If I turn it on I get a lot of renaming failed warnings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a6a5-dbb9-42d8-8119-81d044abce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = pr_units_to_m(ddict)\n",
    "ddict = drop_duplicate_timesteps(ddict) #CESM2-WACCM has duplicate timeseries\n",
    "ddict = preselect_years(ddict,1850,2100)\n",
    "ddict = drop_coords(ddict,['bnds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c0b5a-3f22-4e77-ba19-f27f7b9fe58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}): #join=outer pads NaNs which result in large chunks for timeseries that differ in length\n",
    "    ddict_merged = combine_datasets(ddict,merge_variables_aligning_lonlat,match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id','variant_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92478f-f7ec-4177-878f-efe18bd294f0",
   "metadata": {},
   "source": [
    "Do the subsetting at grid cells nearest to the tide gauges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba17baf-805b-4e10-a073-7c5217cca98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs\n",
    "\n",
    "ddict_at_tgs = defaultdict(dict)\n",
    "\n",
    "for key,ds in tqdm(ddict_merged.items()):\n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True)\n",
    "    ds.attrs[\"original_key\"] = key\n",
    "    ddict_at_tgs[key] = select_gridcells_nearest_to_tgs(mlrcoefs,ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e49c6-208a-4a23-840c-144f08f2dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,ds in tqdm(ddict_at_tgs.items()):\n",
    "    model_path = os.path.join('/home/jovyan/CMIP6cf/output/subsetted_pr/',ds.source_id)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.mkdir(model_path)\n",
    "    ds.to_netcdf(os.path.join(model_path,key.replace('.','_')+'.nc'),mode='w')\n",
    "    ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07418a26-5aa7-4d10-99d9-79cf3d3b147e",
   "metadata": {},
   "source": [
    "^takes about an hour for hist+ssp245+ssp585 excluding EC-Earth3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758d034-e064-48a3-9a13-555e0b015c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
