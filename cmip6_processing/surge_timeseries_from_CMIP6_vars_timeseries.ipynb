{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204334ce-ab38-445f-aad0-ae91d07b400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3964/2781798931.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb6074-9387-42a2-9560-3fe9d3b1334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 9 #n by n degree grid around each TG\n",
    "cmip6_resolution = 1.5\n",
    "\n",
    "num_grid_cells = int(grid_size/cmip6_resolution)\n",
    "\n",
    "mlr_coefs = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/gssr_mlr_coefs_1p5_9deg_codec.nc') #load MLR coefficients at TGs\n",
    "era5_pcs = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/era5_pca_components_1p5_9deg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1d6b8-c443-4332-a354-6d0137fa8ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs.ls('gs://leap-persistent/timh37/CMIP6/timeseries'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Loop over timeseries of `psl` & `sfcWind` at common 1.5 by 1.5 degree grid and open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_dir = '/home/jovyan/CMIP6cf/output/subsetted_forcing/'\n",
    "var1 = 'psl'\n",
    "var2 = 'sfcWind'\n",
    "\n",
    "var1_dir = 'leap-persistent/timh37/CMIP6/timeseries/'+var1+'_europe'\n",
    "var2_dir = 'leap-persistent/timh37/CMIP6/timeseries/'+var2+'_europe'\n",
    "\n",
    "output_dir = 'leap-persistent/timh37/CMIP6/timeseries/surge_tgs'\n",
    "\n",
    "var1_models = [k.split('/')[-1] for k in fs.ls(var1_dir) if k.startswith('.')==False]\n",
    "var2_models = [k.split('/')[-1] for k in fs.ls(var2_dir) if k.startswith('.')==False]\n",
    "\n",
    "models = [k for k in var1_models if k in var2_models]\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for source_id in models:\n",
    "    var1_model_path = os.path.join(var1_dir,source_id)\n",
    "    var2_model_path = os.path.join(var2_dir,source_id)\n",
    "    \n",
    "    var1_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var1_model_path) if s.startswith('.')==False] \n",
    "    var2_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var2_model_path) if s.startswith('.')==False]\n",
    "    experiment_ids = list(set(var1_exps) & set(var2_exps))\n",
    "    \n",
    "    for experiment_id in set(experiment_ids): #for each experiment_id, open the datasets, concatenating all realizations:\n",
    "        #load data:\n",
    "        fn = fnmatch.filter(fs.ls(var1_model_path),'*'+experiment_id+'*')[0]\n",
    "        fn = fn.split('/')[-1]\n",
    "        \n",
    "        var1_var2_data = xr.open_mfdataset((os.path.join('gs://',var1_model_path,fn),os.path.join('gs://',var2_model_path,fn)),engine='zarr',chunks={'member_id':1,'time':100000,'longitude':5})\n",
    "        \n",
    "        ddict[fn.replace('.zarr','')] = var1_var2_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2a215-c34a-4a4e-96f3-3838baf7485d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate predictor data and multiply with regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5857398e-34fc-48b2-86f7-2293bf829ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dd495aee52436cac788d2e6fb60e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddict_predictors = defaultdict(dict)\n",
    "\n",
    "for key,ds in tqdm(ddict.items()):\n",
    "   \n",
    "    for m,member in enumerate(ds.member_id.values):\n",
    "        ds_mem = ds.sel(member_id=member)\n",
    "        \n",
    "        if key == list(ddict.keys())[0]: #common grid, so only need to generate these grids for the first ds\n",
    "            lat_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution)))\n",
    "            lon_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution)))\n",
    "\n",
    "            for t,tg in enumerate(mlr_coefs.tg.values):\n",
    "                lat_ranges[t,:] = ds_mem.latitude[((ds_mem.latitude>=(mlr_coefs.sel(tg=tg).lat-grid_size/2)) & (ds_mem.latitude<=(mlr_coefs.sel(tg=tg).lat+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "                lon_ranges[t,:] = ds_mem.longitude[((ds_mem.longitude>=(mlr_coefs.sel(tg=tg).lon-grid_size/2)) & (ds_mem.longitude<=(mlr_coefs.sel(tg=tg).lon+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "        \n",
    "            lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlr_coefs.tg,'lon_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "            lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlr_coefs.tg,'lat_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "        \n",
    "        \n",
    "        #sanity check timeseries length\n",
    "        num_days = (ds_mem.time[-1]-ds_mem.time[0]).dt.days\n",
    "        assert (len(ds_mem.time) > .9*num_days) & (len(ds_mem.time) < 1.1*num_days)\n",
    "        \n",
    "        predictors = ds_mem\n",
    "        \n",
    "        #generate predictors\n",
    "        predictors['sfcWind_sqd'] = predictors['sfcWind']**2 #add wind squared\n",
    "        predictors['sfcWind_cbd'] = predictors['sfcWind']**3 #add wind cubed\n",
    "        \n",
    "        predictors = (predictors-predictors.mean(dim='time'))/predictors.std(dim='time',ddof=0) #normalize predictor variables (ignores nan by default?)\n",
    "        predictors = predictors.sel(latitude=lats_da,longitude=lons_da).load() #takes a lot of memory, but is much more efficient than loading per TG?\n",
    "        \n",
    "        #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * grid_size * grid_size))\n",
    "        predictors['predictors'] = predictors[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"predictor_var\") \n",
    "        predictors['predictors'] = predictors['predictors'].transpose(\"time\",\"predictor_var\",\"lon_around_tg\",...).stack(f=['predictor_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "            \n",
    "        #compute surges from predictors\n",
    "        if m==0: #initialize\n",
    "            surge_ds = xr.Dataset(data_vars=dict(surge=(['member_id','time','tg'], np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(mlr_coefs.tg))) )),\n",
    "                            coords=dict(member_id=ds.member_id,time=ds.time,tg=mlr_coefs.tg)) #initialize output dataset per model\n",
    "        \n",
    "        for t,tg in enumerate(mlr_coefs.tg):\n",
    "            predictors_at_tg = predictors.sel(tg=tg)\n",
    "            mlr_coefs_at_tg = mlr_coefs.mlr_coefs.sel(tg=tg)\n",
    "            \n",
    "            num_pcs = int(np.sum(np.isfinite(mlr_coefs_at_tg)))-1 #number of mlr coefs = number of PCs to derive, intercept doesn't count\n",
    "            idx_timesteps_w_data = np.argwhere((np.isfinite(predictors_at_tg.predictors).all(axis=1)).values).flatten() #omit timesteps with NaN if any\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data)) #remove missing values for PCA\n",
    "            pcs = pca.transform(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data))\n",
    "            \n",
    "            components = xr.DataArray(data=pca.components_,dims=['pc','f'],coords=dict(pc=np.arange(num_pcs),f=predictors_at_tg.f))\n",
    "            \n",
    "            #compute RMSEs with ERA5 principal components, only considering the pressure part of the forcing (first num_grid_cells**2)\n",
    "            rmses = np.sqrt(((components.isel(f=np.arange(num_grid_cells**2))-era5_pcs.sel(tg=tg).isel(f=np.arange(num_grid_cells**2)).isel(pc=np.arange(num_pcs)).component)**2).mean(dim='f')) #original sign\n",
    "            rmses_flipped = np.sqrt(((components.isel(f=np.arange(num_grid_cells**2))--era5_pcs.isel(f=np.arange(num_grid_cells**2)).sel(tg=tg).isel(pc=np.arange(num_pcs)).component)**2).mean(dim='f')) #opposite sign\n",
    "\n",
    "            s = (rmses<rmses_flipped).astype('int') #flip pcs if rmse of flipped pc is lower\n",
    "            s[s==0]=-1\n",
    "            pcs = pcs * s.values\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            surge_ds['surge'][m,idx_timesteps_w_data,t] = np.sum(mlr_coefs_at_tg[np.isfinite(mlr_coefs_at_tg)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "            \n",
    "    surge_ds.to_zarr(os.path.join('gs://',output_dir,ds.source_id,key+'.zarr'),mode='w')\n",
    "    surge_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
