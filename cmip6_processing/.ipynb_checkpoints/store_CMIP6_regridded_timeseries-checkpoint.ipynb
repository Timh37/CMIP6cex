{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559/2346015076.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, pr_flux_to_m, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda571f0-f7d0-4d5b-8697-78b61c731032",
   "metadata": {
    "tags": []
   },
   "source": [
    "fs.ls('leap-persistent/timh37/CMIP6/subsetted_data/pr_europe/ACCESS-CM2')\n",
    "\n",
    "test=xr.open_dataset('gs://leap-persistent/timh37/CMIP6/subsetted_data/pr_europe/ACCESS-CM2/ACCESS-CM2_gn_historical_day_r1i1p1f1.zarr',engine='zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "overwrite_existing = True #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "target_grid = xr.Dataset( #grid to interpolate CMIP6 simulations to\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-30,22.5,1.5), {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,30,-1.5), {\"units\": \"degrees_north\"}),})\n",
    "\n",
    "query_vars = ['sfcWind','pr','psl'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalogue\n",
    "lcol = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "col.esmcat._df = pd.concat([col.df,lcol.df],ignore_index=True) #merge these catalogues\n",
    "\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalogue per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogues provide duplicate datasets, keep the newest version, and if the versions are identical, keep the leap-pangeo dataset\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)\n",
    "ssp_cats_merged = drop_vars_from_cat(ssp_cats_merged,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0215002d74e4ac3862a24fbb04d3e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1308' class='' max='1308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1308/1308 03:04&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicate timesteps for:ScenarioMIP.NCAR.CESM2-WACCM.ssp585.r1i1p1f1.day.psl.gn.gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2-WACCM/ssp585/r1i1p1f1/day/psl/gn/v20200702/.20200702\n",
      "Dropping duplicate timesteps for:ScenarioMIP.NCAR.CESM2-WACCM.ssp585.r1i1p1f1.day.pr.gn.gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2-WACCM/ssp585/r1i1p1f1/day/pr/gn/v20200702/.20200702\n",
      "Dropping duplicate timesteps for:ScenarioMIP.NCAR.CESM2-WACCM.ssp585.r1i1p1f1.day.sfcWind.gn.gs://cmip6/CMIP6/ScenarioMIP/NCAR/CESM2-WACCM/ssp585/r1i1p1f1/day/sfcWind/gn/v20200702/.20200702\n",
      "Dropping duplicate timesteps for:CMIP.NCAR.CESM2-WACCM.historical.r3i1p1f1.day.psl.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r3i1p1f1/day/psl/gn/v20190227/.20190227\n",
      "Dropping duplicate timesteps for:CMIP.NCAR.CESM2-WACCM.historical.r2i1p1f1.day.psl.gn.gs://cmip6/CMIP6/CMIP/NCAR/CESM2-WACCM/historical/r2i1p1f1/day/psl/gn/v20190227/.20190227\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r11i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r2i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r3i1p1f1.psl\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r3i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r1i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r1i1p1f1.psl\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r11i1p1f1.psl\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r1i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r4i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r11i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r2i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r4i1p1f1.psl\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r3i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r4i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r2i1p1f1.psl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebf8e377cc64c40bc4b16135ba2da12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(ssps)): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    #open datasets into dictionary\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "\n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps if datasets have them\n",
    "    #ddict = select_period(ddict,1850,2100) #preselect time periods, do this at later stage in the chain?\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    #concatenate historical and ssp datasets in time\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        hist_ssp = combine_datasets(ddict,_concat_sorted_time,match_attrs =['source_id', 'grid_label','table_id','variant_label','variable_id'],combine_func_kwargs={'join':'inner','coords':'minimal'})    \n",
    "\n",
    "    hist_ssp_ = defaultdict(dict) #probably a better way to do this, but there are approx. 1 files for which the time units are inconsistent between historical and ssp\n",
    "    for k,v in hist_ssp.items():\n",
    "        if v.time[-1].values.dtype != v.time[0].values.dtype:\n",
    "            print('dropping ' + k +' due to inconsistent timestamps in historical and ssp runs')\n",
    "            continue\n",
    "        else:\n",
    "            hist_ssp_[k] = v\n",
    "            \n",
    "    hist_ssp_ = drop_duplicate_timesteps(hist_ssp_) #remove overlap between historical and ssp experiments which sometimes exists\n",
    "    hist_ssp_complete = drop_incomplete(hist_ssp_) #remove historical+ssp timeseries which are not montonically increasing or have large timegaps (based on Julius Buseckes rudimentary testing in CMIP6-LEAP-feadstock)\n",
    "    \n",
    "    #regrid these datasets to the target grid\n",
    "    hist_ssp_eu = defaultdict(dict)\n",
    "    for key,ds in tqdm(hist_ssp_complete.items()):\n",
    "        ds.attrs[\"time_concat_key\"] = key #add current key information to attributes\n",
    "        ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "        regridder = xe.Regridder(ds,target_grid,'bilinear',ignore_degenerate=True,periodic=True) #create regridder for this dataset\n",
    "        try:\n",
    "            hist_ssp_eu[key] = regridder(ds,keep_attrs=True) #apply regridder\n",
    "        except: #issue with 1 dataset that is chunked along two dimensions, rechunk that\n",
    "            hist_ssp_eu[key] = regridder(ds.chunk({'time':100,'lat':1000,'lon':1000}),keep_attrs=True)\n",
    "        \n",
    "    #storage (to-do..)\n",
    "    #for key,ds in tqdm(ddict_eu.items()):\n",
    "    #    model_path = os.path.join('leap-persistent/timh37/CMIP6/subsetted_data/'+variable+'_europe/',ds.source_id) #store to leap-persistent\n",
    "    #    ds.chunk({'member_id':1,'longitude':5,'time':100000}).to_zarr(os.path.join('gs://',model_path,key.replace('.','_')+'.zarr'),mode='w') #store to leap-persistent as .zarr\n",
    "    #    ds.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b003760-0a1c-49b5-a481-717f5e0ae45c",
   "metadata": {},
   "source": [
    "Calculate total size of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76a33c-3898-4d87-909c-ff2fee9d8498",
   "metadata": {
    "tags": []
   },
   "source": [
    "x=0\n",
    "for k,v in hist_ssp.items():\n",
    " \n",
    "    x += v.nbytes/1000000000\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ef08d-6b8a-4269-9a16-f3cc0e182a95",
   "metadata": {},
   "source": [
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}): #join=outer pads NaNs which result in large chunks for timeseries that differ in length\n",
    "    ddict_merged = combine_datasets(ddict,merge_variables_aligning_lonlat,match_attrs=['source_id', 'grid_label', 'experiment_id', 'table_id','variant_label']) #group datasets of same model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120773a6-1bcc-4526-ac75-8b27b8af12e5",
   "metadata": {},
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalogue\n",
    "lcol = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "col.esmcat._df = pd.concat([col.df,lcol.df],ignore_index=True) #merge these catalogues\n",
    "\n",
    "\n",
    "#search catalogue (need to do this for each SSP separately as availability may differ between them)\n",
    "cat_data_ssp245 = col.search( #find instances providing all required query_vars for both historical & ssp245 experiments\n",
    "    experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "cat_data_ssp585 = col.search( #find instances providing all required query_vars for both historical & ssp585 experiments\n",
    "    experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "#merge SSPs and remove duplicate historical simulations\n",
    "cat_data=cat_data_ssp585\n",
    "cat_data.esmcat._df = pd.concat([cat_data_ssp245.df,cat_data_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True) \n",
    "\n",
    "cat_data = drop_older_versions(cat_data) #if google cloud and leap-pangeo catalogues provide duplicate datasets, keep the newest version, and if the versions are identical, keep the leap-pangeo dataset\n",
    "cat_data = reduce_cat_to_max_num_realizations(cat_data) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)\n",
    "cat_data = drop_vars_from_cat(cat_data,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33350914-b9d3-43c8-875d-38bfa83ac491",
   "metadata": {},
   "source": [
    "Store the dataset to leap-persistent share (directories structured per model):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144f760-276b-43e9-89d2-076ea060521e",
   "metadata": {},
   "source": [
    "for key,ds in tqdm(ddict_eu.items()):\n",
    "    model_path = os.path.join('leap-persistent/timh37/CMIP6/subsetted_data/'+variable+'_europe/',ds.source_id) #store to leap-persistent\n",
    "    ds.chunk({'member_id':1,'longitude':5,'time':100000}).to_zarr(os.path.join('gs://',model_path,key.replace('.','_')+'.zarr'),mode='w') #store to leap-persistent as .zarr\n",
    "    ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
