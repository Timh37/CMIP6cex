{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3592/1025683497.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, pr_flux_to_m, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'\n",
    "overwrite_existing = False #whether or not to process files for which output already exists in the output path\n",
    "\n",
    "target_grid = xr.Dataset( #grid to interpolate CMIP6 simulations to\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-30,22.5,1.5), {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,30,-1.5), {\"units\": \"degrees_north\"}),})\n",
    "\n",
    "query_vars = ['sfcWind','pr','psl'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalog:\n",
    "col = google_cmip_col() #google cloud catalog\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "#assign priority for keeping duplicate datasets in these catalogs\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogs\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalog per hist+ssp combination (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "#merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged = ssp_cats[ssp] \n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogs provide duplicate datasets, keep the newest version, and if the versions are identical, keep the highest priority catalog\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe55f5239c9e491ea5a6836cb91d7e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='30' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [30/30 00:03&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fadec27f4f5401a9d0e2f30cf3743ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='42' class='' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [42/42 00:06&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c1d643811a4926bf7f87bdcc39aacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(ssps)): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    cat_to_open = drop_vars_from_cat(cat_to_open,[k for k in required_vars if k not in query_vars]) #only process desired variables\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #open datasets into dictionary\n",
    "\n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "\n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr if pr in ds\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps from ds if present\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    #concatenate historical and ssp datasets in time\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        hist_ssp = combine_datasets(ddict,_concat_sorted_time,match_attrs =['source_id', 'grid_label','table_id','variant_label','variable_id'],combine_func_kwargs={'join':'inner','coords':'minimal'})    \n",
    "\n",
    "    hist_ssp_ = defaultdict(dict) #probably a better way to do this, but there are approx. 1 files for which the time units are inconsistent between historical and ssp\n",
    "    for k,v in hist_ssp.items():\n",
    "        if v.time[-1].values.dtype != v.time[0].values.dtype:\n",
    "            print('dropping ' + k +' due to inconsistent timestamps in historical and ssp runs')\n",
    "            continue\n",
    "        else:\n",
    "            hist_ssp_[k] = v\n",
    "            \n",
    "    hist_ssp_ = drop_duplicate_timesteps(hist_ssp_) #remove overlap between historical and ssp experiments which sometimes exists\n",
    "    hist_ssp_complete = drop_incomplete(hist_ssp_) #remove historical+ssp timeseries which are not montonically increasing or have large timegaps (based on Julius Buseckes rudimentary testing in CMIP6-LEAP-feadstock)\n",
    "    \n",
    "    #regrid these datasets to the target grid\n",
    "    regridded_datasets = defaultdict(dict)\n",
    "    for key,ds in tqdm(hist_ssp_complete.items()):\n",
    "        variable = key.split('.')[-1]\n",
    "\n",
    "        output_fn = os.path.join(output_path,variable,ds.source_id,key+'.hist_'+ssp)\n",
    "        \n",
    "        try:\n",
    "            if overwrite_existing or not fs.exists(output_fn):\n",
    "                ds.attrs[\"time_concat_key\"] = key+'.hist_'+ssp #add current key information to attributes\n",
    "                ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "                regridder = xe.Regridder(ds,target_grid,'bilinear',ignore_degenerate=True,periodic=True) #create regridder for this dataset\n",
    "                try:\n",
    "                    regridded_ds = regridder(ds,keep_attrs=True) #apply regridder\n",
    "                except: #issue with 1 dataset that is chunked along two dimensions, rechunk that\n",
    "                    regridded_ds = regridder(ds.chunk({'time':100,'lat':1000,'lon':1000}),keep_attrs=True)\n",
    "\n",
    "                regridded_datasets[key] = regridded_ds\n",
    "\n",
    "                #store dataset as .zarr to leap-persistent storage\n",
    "                try:\n",
    "                    regridded_ds.to_zarr(output_fn,mode='w') #fails if chunks are not uniform due to time concatenation\n",
    "                except:\n",
    "                    regridded_ds[variable] = regridded_ds[variable].chunk({'time':'auto'})\n",
    "                    regridded_ds.to_zarr(output_fn,mode='w')\n",
    "\n",
    "                regridded_ds.close()\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b003760-0a1c-49b5-a481-717f5e0ae45c",
   "metadata": {},
   "source": [
    "Calculate total size of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5e757-04a4-4c05-aeef-83a4acdb7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=0\n",
    "for k,v in ddict.items():\n",
    "    if 'ssp245' in k:\n",
    "        x += v.nbytes/1000000000\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7406f-a16b-4562-888c-46d2b89503b7",
   "metadata": {},
   "source": [
    "List available members per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f54472d-35c7-4926-a103-31f253880064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = ssp_cats_merged.df.source_id.unique()\n",
    "count_members = np.zeros(len(models))\n",
    "\n",
    "for k,ds in hist_ssp_complete.items():\n",
    "    count_members[np.where(models==ds.source_id)[0][0]] = count_members[np.where(models==ds.source_id)[0][0]] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b21653b7-fff1-4cd9-80dd-8d61bf74771a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCESS-CM2' 'ACCESS-ESM1-5' 'CESM2' 'CESM2-WACCM' 'CMCC-CM2-SR5'\n",
      " 'CMCC-ESM2' 'CanESM5' 'EC-Earth3' 'EC-Earth3-Veg' 'FGOALS-g3' 'GFDL-CM4'\n",
      " 'GFDL-ESM4' 'HadGEM3-GC31-LL' 'HadGEM3-GC31-MM' 'IITM-ESM' 'INM-CM4-8'\n",
      " 'INM-CM5-0' 'IPSL-CM6A-LR' 'KACE-1-0-G' 'MIROC-ES2L' 'MIROC6'\n",
      " 'MPI-ESM1-2-HR' 'MPI-ESM1-2-LR' 'MRI-ESM2-0' 'NorESM2-LM' 'NorESM2-MM'\n",
      " 'TaiESM1' 'UKESM1-0-LL']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5., 39.,  2.,  3.,  1.,  1., 25., 61.,  1.,  1.,  1.,  1.,  5.,\n",
       "        0.,  1.,  1.,  1., 11.,  3., 10., 45.,  2., 24.,  3.,  3.,  2.,\n",
       "        1.,  5.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(models)\n",
    "np.floor_divide(count_members,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
