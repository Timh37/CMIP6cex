{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_926/933853136.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time,concat_members\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, pr_flux_to_m, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a10f51e-3913-4e72-ad64-61e67de65953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lonlat_idx_nearest_to_tgs(tg_ds,ds):\n",
    "    '''tg_ds = xr.DataSet containing 'lon' and 'lat' coordinates of tide gauges\n",
    "    ds    = xr.DataSet containing CMIP6 data to subset\n",
    "    '''\n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #compute distances between TG coordinates and grid cell centers\n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-tg_ds.lat) )**2 +\n",
    "        np.cos((np.pi/180)*tg_ds.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-tg_ds.lon))**2) )\n",
    "    \n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name]) #find indices of nearest grid cells\n",
    "    return idx_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_gesla2_tgs/'\n",
    "overwrite_existing = False #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "tg_coords = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/gssr_mlr_coefs_1p5_9deg_gesla2.nc')\n",
    "\n",
    "query_vars = ['sfcWind','pr'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalogue\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogues\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalogue per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogues provide duplicate datasets, keep the newest version, and if the versions are identical, keep the leap-pangeo dataset\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77dfd80f2d3845beb8539fee8cf6ef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s,ssp \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m([ssps])): \u001b[38;5;66;03m#for each ssp:  \u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#select historical and ssp data in merged catalogue for this particular ssp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     cat_to_open \u001b[38;5;241m=\u001b[39m \u001b[43mssp_cats_merged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mssp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequire_all_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmember_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrid_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     cat_to_open \u001b[38;5;241m=\u001b[39m drop_vars_from_cat(cat_to_open,[k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m required_vars \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m query_vars]) \u001b[38;5;66;03m#out of required variables only process query variables\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#open datasets into dictionary\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/pydantic/deprecated/decorator.py:55\u001b[0m, in \u001b[0;36mvalidate_arguments.<locals>.validate.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(_func)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/pydantic/deprecated/decorator.py:150\u001b[0m, in \u001b[0;36mValidatedFunction.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    149\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_model_instance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/pydantic/deprecated/decorator.py:222\u001b[0m, in \u001b[0;36mValidatedFunction.execute\u001b[0;34m(self, m)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_function(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvar_kwargs)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvar_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/intake_esm/core.py:393\u001b[0m, in \u001b[0;36mesm_datastore.search\u001b[0;34m(self, require_all_on, **query)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search for entries in the catalog.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m4    landCoverFrac\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# step 1: Search in the base/main catalog\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m esmcat_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesmcat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequire_all_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_all_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# step 2: Search for entries required to derive variables in the derived catalogs\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# This requires a bit of a hack i.e. the user has to specify the variable in the query\u001b[39;00m\n\u001b[1;32m    397\u001b[0m derivedcat_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/intake_esm/cat.py:389\u001b[0m, in \u001b[0;36mESMCatalogModel.search\u001b[0;34m(self, query, require_all_on)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03mSearch for entries in the catalog.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    382\u001b[0m     query\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(query, QueryModel)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 389\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_with_iterables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns_with_iterables\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _query\u001b[38;5;241m.\u001b[39mrequire_all_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    393\u001b[0m     results \u001b[38;5;241m=\u001b[39m search_apply_require_all_on(\n\u001b[1;32m    394\u001b[0m         df\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m    395\u001b[0m         query\u001b[38;5;241m=\u001b[39m_query\u001b[38;5;241m.\u001b[39mquery,\n\u001b[1;32m    396\u001b[0m         require_all_on\u001b[38;5;241m=\u001b[39m_query\u001b[38;5;241m.\u001b[39mrequire_all_on,\n\u001b[1;32m    397\u001b[0m         columns_with_iterables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns_with_iterables,\n\u001b[1;32m    398\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/intake_esm/_search.py:51\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(df, query, columns_with_iterables)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m column_is_stringtype \u001b[38;5;129;01mand\u001b[39;00m is_pattern(value):\n\u001b[1;32m     50\u001b[0m     mask \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(value, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(value):\n\u001b[1;32m     52\u001b[0m     mask \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39misnull()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(ssps)): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    cat_to_open = drop_vars_from_cat(cat_to_open,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables\n",
    "    #open datasets into dictionary\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "    \n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "    \n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps if datasets have them\n",
    "    #ddict = select_period(ddict,1850,2100) #preselect time periods, do this at later stage in the chain?\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    #concatenate historical and ssp datasets in time\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        hist_ssp = combine_datasets(ddict,_concat_sorted_time,match_attrs =['source_id', 'grid_label','table_id','variant_label','variable_id'],combine_func_kwargs={'join':'inner','coords':'minimal'})    \n",
    "\n",
    "    hist_ssp_ = defaultdict(dict) #probably a better way to do this, but there are approx. 1 files for which the time units are inconsistent between historical and ssp\n",
    "    for k,v in hist_ssp.items():\n",
    "        if v.time[-1].values.dtype != v.time[0].values.dtype:\n",
    "            print('dropping ' + k +' due to inconsistent timestamps in historical and ssp runs')\n",
    "            continue\n",
    "        else:\n",
    "            hist_ssp_[k] = v\n",
    "            \n",
    "    hist_ssp_ = drop_duplicate_timesteps(hist_ssp_) #remove overlap between historical and ssp experiments which sometimes exists\n",
    "    hist_ssp_complete = drop_incomplete(hist_ssp_) #remove historical+ssp timeseries which are not montonically increasing or have large timegaps (based on Julius Buseckes rudimentary testing in CMIP6-LEAP-feadstock)\n",
    "    \n",
    "    #regrid these datasets to the target tide gauges\n",
    "    for key,ds in tqdm(hist_ssp_complete.items()):\n",
    "        print(key)\n",
    "        variable = key.split('.')[-1]\n",
    "        #check if dataset was already stored\n",
    "        output_fn = os.path.join(output_path,variable,ds.source_id,key+'.hist_'+ssp) #output filename\n",
    "        try:\n",
    "            if (~overwrite_existing) & (output_fn.replace('gs://','') in fs.ls(os.path.join(output_path,variable,ds.source_id))):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        ds.attrs[\"time_concat_key\"] = key #add current key information to attributes\n",
    "        ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "        \n",
    "        if ('missing_value' in ds[variable].encoding) & ('_FillValue' in ds[variable].encoding):\n",
    "            if ds[variable].encoding['missing_value'] != ds[variable].encoding['_FillValue']:\n",
    "                del ds[variable].encoding['missing_value']\n",
    "                \n",
    "        datasets=[]\n",
    "        for tg in range(len(tg_coords.tg)):\n",
    "            single_tg = tg_coords.isel(tg=[tg])\n",
    "            regridder = xe.Regridder(ds,single_tg, method='nearest_s2d')\n",
    "            try:\n",
    "                datasets.append(regridder(ds,keep_attrs=True).squeeze())\n",
    "            except:\n",
    "                datasets.append(regridder(ds.chunk({'lon':1000,'lat':1000}),keep_attrs=True).squeeze())\n",
    "        ds_at_tgs = xr.concat(datasets, dim='tg')\n",
    "        ds_at_tgs['tg']=tg_coords.tg.values\n",
    "        ds_at_tgs = ds_at_tgs.unify_chunks().chunk({'time':100000,'tg':200})\n",
    "        ds_at_tgs['tg'] = ds_at_tgs.tg.astype('str') #something wrong with encoding object types in zarr, this is the work-around\n",
    "    \n",
    "        ds_at_tgs.to_zarr(output_fn,mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
