{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8899c2-c522-42e0-aa61-ce53a758f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4289/4005817676.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import fnmatch\n",
    "from tqdm.autonotebook import tqdm\n",
    "import dask\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853c8e9b-73b2-47c3-9881-09abe4ce60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def po_t_of_refyear(da,threshold,refyear,dim):#peak over threshold analysis relative to threshold in reference period\n",
    "    return da.where(da>da.sel(window=refyear).quantile(threshold,dim=dim))\n",
    "            \n",
    "def rolling_max(da,window_len,dim):\n",
    "    return da.rolling({dim:window_len},center=True,min_periods=1).max()\n",
    "\n",
    "def count_num_extremes_pmonth(extremes): #count number of (joint) extremes on a monthly basis\n",
    "    extremes_ = extremes.copy(deep=True) #input must be boolean array (True or False (joint) extreme occurs on that day)!\n",
    "    if len(extremes.time.shape)>1:\n",
    "        extremes_['time_in_window_idx'] = extremes_.time.dt.month.isel(window=0).values\n",
    "    else:\n",
    "        extremes_['time_in_window_idx'] = extremes_.time.dt.month.values\n",
    "    num_extremes_pmonth = extremes_.rename({'time_in_window_idx':'month'}).groupby('month').sum()\n",
    "    return num_extremes_pmonth        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be0e9a-7d00-4d95-b211-57e51b0467c6",
   "metadata": {},
   "source": [
    "Configure the bivariate sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfd07a-2eea-4f4b-a14a-4efa58c0cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = 0 #maximum time between co-occurring extremes (0 = no lag)\n",
    "declus_window_len = 1 #rolling window length for declustering (1 = no declustering)\n",
    "threshold = .98 #quantile above which events are defined extreme\n",
    "\n",
    "#time windows to select for the analysis\n",
    "output_yrs = np.arange(2000,2100,20)\n",
    "window_len = 40 #period length around output_yrs\n",
    "ref_year = 2000 #historical period to to compute thresholds from\n",
    "\n",
    "#configure the input directories and grid_type\n",
    "var1 = 'surge'\n",
    "var2 = 'pr'\n",
    "\n",
    "var1_dir = 'leap-persistent/timh37/CMIP6/timeseries_eu_gesla2_tgs/'+var1\n",
    "var2_dir = 'leap-persistent/timh37/CMIP6/timeseries_eu_gesla2_tgs/'+var2\n",
    "\n",
    "out_dir = '/home/jovyan/CMIP6cex/output/num_extremes/'\n",
    "\n",
    "overwrite_existing=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c60a7-b2b9-423f-bb2a-5e8263f9c71b",
   "metadata": {},
   "source": [
    "Open datasets into dictionary of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57ca67d-4dd6-4eb7-bd54-d3f2d63df818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd416dce890b473694ea05d023bf9162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make output folder\n",
    "out_path = os.path.join(out_dir,var1_dir.split('/')[-2],var1+'_'+var2,\n",
    "                        str(window_len)+'yr_'+str(threshold).replace('0.','p')+'_lag'+str(max_lag)+'d_declus'+str(declus_window_len)+'d_ref'+str(ref_year))\n",
    "\n",
    "#open datasets\n",
    "var1_models = [k.split('/')[-1] for k in fs.ls(var1_dir) if k.startswith('.')==False]\n",
    "var2_models = [k.split('/')[-1] for k in fs.ls(var2_dir) if k.startswith('.')==False]\n",
    "models = [k for k in var1_models if k in var2_models]\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for s,source_id in tqdm(enumerate(models)): #loop over models\n",
    "    for f,file in enumerate(fs.ls(os.path.join(var1_dir,source_id))): #loop over variants & SSPs\n",
    "        var1_path = file\n",
    "        var2_path = os.path.join(os.path.join(var2_dir,source_id),file.split('/')[-1].replace(var1,var2))\n",
    "        try:\n",
    "            var1_var2_ds = xr.open_mfdataset(('gs://'+var1_path,'gs://'+var2_path),engine='zarr',use_cftime=True)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        k = file.split('/')[-1]\n",
    "        ddict[k] = var1_var2_ds.sel(time=slice(str(int(output_yrs[0]-window_len/2)), str(int(output_yrs[-1]+window_len/2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95b265-b5ae-4eba-a704-6919d396dc50",
   "metadata": {},
   "source": [
    "Do the bivariate extremes analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e78fb4-9992-4ae1-ad1f-19a4dab57942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2a19e8516449ecb62ac907938b5f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,ds in tqdm(ddict.items()): #loop over datasets\n",
    "    if 'member_id' not in ds.dims:\n",
    "        ds = ds.expand_dims('member_id')\n",
    "    ds = ds.load() #load dataset into memory\n",
    "    ds = ds.transpose('time',...)\n",
    "   \n",
    "    source_id = k.split('.')[0]\n",
    "    output_fn = os.path.join(out_path,source_id,k.replace(var1,'num_extremes'))+'.nc'\n",
    "    \n",
    "    if (~overwrite_existing) & (os.path.exists(output_fn)): #check if dataset results have already been computed\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(os.path.join(out_path,source_id))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    attrs = ds.attrs #store dataset attributes to save later\n",
    "    ds = ds.isel(member_id=0) \n",
    "    \n",
    "    #remove leap days\n",
    "    if len(np.unique(ds.time.resample(time='1Y').count()))>1: #remove leap days so that each computation window has the same length\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            ds = ds.sel(time=~((ds.time.dt.month == 2) & (ds.time.dt.day == 29))) #^probably only has a small effect on the results\n",
    "\n",
    "    days_in_year = int(ds.time.resample(time='1Y').count()[0]) #days per year for current dataset calendar\n",
    "    \n",
    "    #construct the indices of time periods of 'window_len' years centered at 'output_yrs'\n",
    "    if window_len%2 !=0: #odd\n",
    "        window_start_idx = days_in_year*(output_yrs-int(output_yrs[0]-window_len/2)-int(np.floor(window_len/2)))\n",
    "        first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "    else: #even\n",
    "        window_start_idx = days_in_year*(output_yrs-int(output_yrs[0]-window_len/2)-int(window_len/2)+1)\n",
    "        first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "\n",
    "    if np.max(first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:])>=len(ds.time): #if window exceeds simulation length\n",
    "        if (int(output_yrs[-1]+window_len/2)==2100) & (ddict[k].time.dt.year[-1]==2099): #if end year is 2099 instead of 2100, shift windows to 1 year earlier\n",
    "            window_start_idx = window_start_idx - days_in_year #shift windows back by 1 year\n",
    "        else:\n",
    "            print('Requested time periods exceed simulation length for '+k)\n",
    "        continue #skip dataset\n",
    "\n",
    "    window_idx = xr.DataArray( #put indices of time periods into data array for fancy indexing\n",
    "        data=first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:],\n",
    "        dims=[\"time_in_window_idx\",\"window\"],\n",
    "        coords=dict(time_in_window_idx=first_window_idx,window=output_yrs),)\n",
    "    \n",
    "    #select data in user-defined time windows\n",
    "    try:\n",
    "        ds_wdws = ds.isel(time=window_idx) \n",
    "    except:\n",
    "        print('could not select requested time windows from '+k)\n",
    "        continue\n",
    "        \n",
    "    #find extremes using POT with historical threshold values\n",
    "    var1_extremes = po_t_of_refyear(ds_wdws[var1],threshold,ref_year,dim='time_in_window_idx')\n",
    "    var2_extremes = po_t_of_refyear(ds_wdws[var2],threshold,ref_year,dim='time_in_window_idx')\n",
    "\n",
    "    #decluster the peaks using a rolling maximum\n",
    "    var1_extremes_declustered = var1_extremes.where(var1_extremes==var1_extremes.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "    var2_extremes_declustered = var2_extremes.where(var2_extremes==var2_extremes.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "\n",
    "    #boolean array of when days experience joint extremes within 'max_lag' lag from eachother\n",
    "    joint_extremes = np.isfinite((rolling_max(var2_extremes_declustered,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_declustered)) \n",
    "    \n",
    "    #store number of extremes per month in dataset\n",
    "    num_extremes = count_num_extremes_pmonth(joint_extremes).to_dataset(name='num_joint_extremes') #joint extremes\n",
    "    num_extremes['num_'+var1+'_extremes'] = count_num_extremes_pmonth(np.isfinite(var1_extremes_declustered)) #univariate extremes var1\n",
    "    num_extremes['num_'+var2+'_extremes'] = count_num_extremes_pmonth(np.isfinite(var2_extremes_declustered)) #univariate extremes var2\n",
    "\n",
    "    #decompose future changes in number of joint extremes (only if not declustering and 0-day lag), this could probably be done more efficiently?:\n",
    "    #initialize output\n",
    "    num_extremes['num_joint_extremes_'+var1+'_driven']          = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var1\n",
    "    num_extremes['num_joint_extremes_'+var2+'_driven']          = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var2\n",
    "    num_extremes['num_joint_extremes_'+var1+'_'+var2+'_driven'] = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var1 and var2\n",
    "    \n",
    "    num_extremes['num_'+var1+'_extremes_refWindow_futT']        = np.nan*num_extremes['num_'+var1+'_extremes'].copy(deep=True) #number of peaks in future windows exceeding the reference threshold values\n",
    "    num_extremes['num_'+var2+'_extremes_refWindow_futT']        = np.nan*num_extremes['num_'+var2+'_extremes'].copy(deep=True) #number of peaks in future windows exceeding the reference threshold values\n",
    "    \n",
    "    if (max_lag == 0) & (declus_window_len==1): #decomposition for now only works if not declustering and only considering joint extremes on the same day\n",
    "    \n",
    "        #1) sort (in magnitude) the values in the reference period of each variable\n",
    "        if ('latitude' in ds.dims) & ('longitude' in ds.dims):\n",
    "            sorted_var1_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var1],axis=0),dims=ds_wdws.sel(window=ref_year)[var1].dims,\n",
    "                                               coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,latitude=ds_wdws.latitude,longitude=ds_wdws.longitude))\n",
    "            sorted_var2_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var2],axis=0),dims=ds_wdws.sel(window=ref_year)[var2].dims,\n",
    "                                          coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,latitude=ds_wdws.latitude,longitude=ds_wdws.longitude))\n",
    "        elif ('tg' in ds.dims):\n",
    "            sorted_var1_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var1],axis=0),dims=ds_wdws.sel(window=ref_year)[var1].dims,\n",
    "                                               coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,tg=ds_wdws.tg))\n",
    "            sorted_var2_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var2],axis=0),dims=ds_wdws.sel(window=ref_year)[var2].dims,\n",
    "                                          coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,tg=ds_wdws.tg))\n",
    "        \n",
    "        #2) find the number of extremes that exceed the reference period threshold value in other windows    \n",
    "        for w,win in enumerate(ds_wdws.window): #loop over each window\n",
    "            num_var1_extremes_in_wdw = np.isfinite(var1_extremes_declustered).sum(dim='time_in_window_idx').sel(window=win)#.load()\n",
    "            num_var2_extremes_in_wdw = np.isfinite(var2_extremes_declustered).sum(dim='time_in_window_idx').sel(window=win)#.load()\n",
    "        \n",
    "            #use that number to define the equivalent threshold in the historical period (var_{U_{var}}^{hist} in the paper)\n",
    "            var1_eqv_thresholds = sorted_var1_ref.isel(time_in_window_idx=-1*(num_var1_extremes_in_wdw))\n",
    "            var2_eqv_thresholds = sorted_var2_ref.isel(time_in_window_idx=-1*(num_var2_extremes_in_wdw))\n",
    "\n",
    "            #3) determine the extremes above those threshold values in the reference window\n",
    "            var1_extremes_fut_threshold = ds_wdws[var1].sel(window=ref_year).where(ds_wdws[var1].sel(window=ref_year)>=var1_eqv_thresholds)\n",
    "            var2_extremes_fut_threshold = ds_wdws[var2].sel(window=ref_year).where(ds_wdws[var2].sel(window=ref_year)>=var2_eqv_thresholds)\n",
    "\n",
    "            #4) determine the joint extremes using these extremes:\n",
    "            joint_extremes_var1_driven = np.isfinite((rolling_max(var2_extremes_declustered.sel(window=ref_year),max_lag*2+1,dim='time_in_window_idx')*var1_extremes_fut_threshold))\n",
    "            joint_extremes_var2_driven = np.isfinite((rolling_max(var2_extremes_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_declustered.sel(window=ref_year)))\n",
    "            joint_extremes_var1_var2_driven = np.isfinite((rolling_max(var2_extremes_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_fut_threshold))\n",
    "\n",
    "            #5) count per month & write to output dataset\n",
    "            num_extremes['num_joint_extremes_'+var1+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var1_driven)\n",
    "            num_extremes['num_joint_extremes_'+var2+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var2_driven)\n",
    "            num_extremes['num_joint_extremes_'+var1+'_'+var2+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var1_var2_driven)\n",
    "            \n",
    "            num_extremes['num_'+var1+'_extremes_refWindow_futT'].loc[dict(window=win)] = count_num_extremes_pmonth(np.isfinite(var1_extremes_fut_threshold))\n",
    "            num_extremes['num_'+var2+'_extremes_refWindow_futT'].loc[dict(window=win)] = count_num_extremes_pmonth(np.isfinite(var2_extremes_fut_threshold))\n",
    "            \n",
    "    #store the results\n",
    "    num_extremes = num_extremes.expand_dims('member_id') #add back member_id as dimension\n",
    "    num_extremes.attrs = attrs #add original attributes\n",
    "    \n",
    "    #add information about the joint extremes analysis\n",
    "    num_extremes.attrs['window_length'] = str(window_len)\n",
    "    num_extremes.attrs['declustering_length'] = str(declus_window_len)\n",
    "    num_extremes.attrs['allowed_lag'] = str(max_lag)\n",
    "    num_extremes.attrs['ref_window'] = str(ref_year)\n",
    "    num_extremes.attrs['source_id'] = source_id\n",
    "    num_extremes.to_netcdf(output_fn,mode='w')\n",
    "    num_extremes.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
