{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204334ce-ab38-445f-aad0-ae91d07b400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_958/2781798931.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb6074-9387-42a2-9560-3fe9d3b1334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_coefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_mlr_coefs_1p5_9deg_codec.nc') #load MLR coefficients at TGs\n",
    "grid_size = 9 #n by n degree grid around each TG\n",
    "cmip6_resolution = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Loop over timeseries of `psl` & `sfcWind` at common 1.5 by 1.5 degree grid and open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_dir = '/home/jovyan/CMIP6cf/output/subsetted_forcing/'\n",
    "var1 = 'psl'\n",
    "var2 = 'sfcWind'\n",
    "\n",
    "var1_dir = 'leap-persistent/timh37/CMIP6/timeseries/'+var1+'_europe'\n",
    "var2_dir = 'leap-persistent/timh37/CMIP6/timeseries/'+var2+'_europe'\n",
    "\n",
    "output_dir = 'leap-persistent/timh37/CMIP6/timeseries/surge_tgs'\n",
    "\n",
    "var1_models = [k.split('/')[-1] for k in fs.ls(var1_dir) if k.startswith('.')==False]\n",
    "var2_models = [k.split('/')[-1] for k in fs.ls(var2_dir) if k.startswith('.')==False]\n",
    "\n",
    "models = [k for k in var1_models if k in var2_models]\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for source_id in ['EC-Earth3']:#models:\n",
    "    var1_model_path = os.path.join(var1_dir,source_id)\n",
    "    var2_model_path = os.path.join(var2_dir,source_id)\n",
    "    \n",
    "    var1_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var1_model_path) if s.startswith('.')==False] \n",
    "    var2_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var2_model_path) if s.startswith('.')==False]\n",
    "    experiment_ids = list(set(var1_exps) & set(var2_exps))\n",
    "    \n",
    "    for experiment_id in set(experiment_ids): #for each experiment_id, open the datasets, concatenating all realizations:\n",
    "        #load data:\n",
    "        fn = fnmatch.filter(fs.ls(var1_model_path),'*'+experiment_id+'*')[0]\n",
    "        fn = fn.split('/')[-1]\n",
    "        \n",
    "        var1_var2_data = xr.open_mfdataset((os.path.join('gs://',var1_model_path,fn),os.path.join('gs://',var2_model_path,fn)),engine='zarr',chunks={'member_id':1,'time':100000,'longitude':5})\n",
    "        \n",
    "        ddict[fn.replace('.zarr','')] = var1_var2_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb20383-2258-4baa-8b0b-fbb35f7979e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCESS-CM2_gn_day_ssp585',\n",
       " 'ACCESS-CM2_gn_day_ssp245',\n",
       " 'CESM2_gn_day_ssp585',\n",
       " 'CESM2_gn_day_ssp245',\n",
       " 'CESM2-WACCM_gn_day_ssp585',\n",
       " 'CESM2-WACCM_gn_day_ssp245',\n",
       " 'CMCC-CM2-SR5_gn_day_ssp585',\n",
       " 'CMCC-CM2-SR5_gn_day_ssp245',\n",
       " 'CMCC-ESM2_gn_day_ssp585',\n",
       " 'CMCC-ESM2_gn_day_ssp245',\n",
       " 'CanESM5_gn_day_ssp585',\n",
       " 'CanESM5_gn_day_ssp245',\n",
       " 'EC-Earth3_gr_day_ssp585',\n",
       " 'EC-Earth3_gr_day_ssp245',\n",
       " 'EC-Earth3-Veg_gr_day_ssp245']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ddict.keys())[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2a215-c34a-4a4e-96f3-3838baf7485d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generate predictor data and multiply with regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5857398e-34fc-48b2-86f7-2293bf829ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250e2c05d53f4517957b5715a68ba363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'lats_da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfcWind_cbd\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfcWind\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m#add wind cubed\u001b[39;00m\n\u001b[1;32m     31\u001b[0m predictors \u001b[38;5;241m=\u001b[39m (predictors\u001b[38;5;241m-\u001b[39mpredictors\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m/\u001b[39mpredictors\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#normalize predictor variables (ignores nan by default?)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m predictors \u001b[38;5;241m=\u001b[39m predictors\u001b[38;5;241m.\u001b[39msel(latitude\u001b[38;5;241m=\u001b[39m\u001b[43mlats_da\u001b[49m,longitude\u001b[38;5;241m=\u001b[39mlons_da)\u001b[38;5;241m.\u001b[39mload() \u001b[38;5;66;03m#takes a lot of memory, but is much more efficient than loading per TG?\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * grid_size * grid_size))\u001b[39;00m\n\u001b[1;32m     35\u001b[0m predictors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictors[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfcWind\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfcWind_sqd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfcWind_cbd\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_array(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor_var\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'lats_da' is not defined"
     ]
    }
   ],
   "source": [
    "ddict_predictors = defaultdict(dict)\n",
    "\n",
    "for key,ds in tqdm(ddict.items()):\n",
    "    for m,member in enumerate(ds.member_id.values):\n",
    "        ds_mem = ds.sel(member_id=member)\n",
    "        \n",
    "        if key == list(ddict.keys())[0]: #common grid, so only need to generate these grids for the first ds\n",
    "            lat_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution)))\n",
    "            lon_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution)))\n",
    "\n",
    "            for t,tg in enumerate(mlr_coefs.tg.values):\n",
    "                lat_ranges[t,:] = ds_mem.latitude[((ds_mem.latitude>=(mlr_coefs.sel(tg=tg).lat-grid_size/2)) & (ds_mem.latitude<=(mlr_coefs.sel(tg=tg).lat+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "                lon_ranges[t,:] = ds_mem.longitude[((ds_mem.longitude>=(mlr_coefs.sel(tg=tg).lon-grid_size/2)) & (ds_mem.longitude<=(mlr_coefs.sel(tg=tg).lon+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "        \n",
    "            lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlr_coefs.tg,'lon_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "            lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlr_coefs.tg,'lat_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "        \n",
    "        \n",
    "        #sanity check timeseries length\n",
    "        num_days = (ds_mem.time[-1]-ds_mem.time[0]).dt.days\n",
    "        assert (len(ds_mem.time) > .9*num_days) & (len(ds_mem.time) < 1.1*num_days)\n",
    "        \n",
    "        predictors = ds_mem\n",
    "        \n",
    "        #generate predictors\n",
    "        predictors['sfcWind_sqd'] = predictors['sfcWind']**2 #add wind squared\n",
    "        predictors['sfcWind_cbd'] = predictors['sfcWind']**3 #add wind cubed\n",
    "        \n",
    "        predictors = (predictors-predictors.mean(dim='time'))/predictors.std(dim='time',ddof=0) #normalize predictor variables (ignores nan by default?)\n",
    "        predictors = predictors.sel(latitude=lats_da,longitude=lons_da).load() #takes a lot of memory, but is much more efficient than loading per TG?\n",
    "        \n",
    "        #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * grid_size * grid_size))\n",
    "        predictors['predictors'] = predictors[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"predictor_var\") \n",
    "        predictors['predictors'] = predictors['predictors'].transpose(\"time\",\"predictor_var\",\"lon_around_tg\",...).stack(p=['predictor_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "        \n",
    "        #compute surges from predictors\n",
    "        if m==0: #initialize\n",
    "            surge_ds = xr.Dataset(data_vars=dict(surge=(['member_id','time','tg'], np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(mlr_coefs.tg))) )),\n",
    "                            coords=dict(member_id=ds.member_id,time=ds.time,tg=mlr_coefs.tg)) #initialize output dataset per model\n",
    "        \n",
    "        for t,tg in enumerate(mlr_coefs.tg):\n",
    "            predictors_at_tg = predictors.sel(tg=tg)\n",
    "            mlr_coefs_at_tg = mlr_coefs.mlr_coefs.sel(tg=tg)\n",
    "            \n",
    "            num_pcs = int(np.sum(np.isfinite(mlr_coefs_at_tg)))-1 #number of mlr coefs = number of PCs to derive, intercept doesn't count\n",
    "            idx_timesteps_w_data = np.argwhere((np.isfinite(predictors_at_tg.predictors).all(axis=1)).values).flatten() #omit timesteps with NaN if any\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data)) #remove missing values for PCA\n",
    "            pcs = pca.transform(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data))\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            surge_ds['surge'][m,idx_timesteps_w_data,t] = np.sum(mlr_coefs_at_tg[np.isfinite(mlr_coefs_at_tg)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "        \n",
    "    surge_ds.to_zarr(os.path.join('gs://',output_dir,ds.source_id,key+'.zarr'),mode='w')\n",
    "    surge_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2417d9-a751-493d-b578-1f2e721a8248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
