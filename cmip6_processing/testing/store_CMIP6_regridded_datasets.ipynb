{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6272/3747057756.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "from distributed import worker_client, as_completed\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "from typing import Dict\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8731ac78-8165-4157-affd-4c48215cb4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create regridders per source_id (datasets of source_id should be on the same grid after passing through reduce_cat_to_max_num_realizations\n",
    "def create_regridder_dict(dataset_dict: Dict[str, xr.Dataset], target_grid_ds: xr.Dataset) -> Dict[str, xe.Regridder]:\n",
    "    regridders = {}\n",
    "    source_ids = np.unique([ds.attrs['source_id'] for ds in dataset_dict.values()])\n",
    "    for si in tqdm(source_ids):\n",
    "        matching_keys = [k for k in dataset_dict.keys() if si in k]\n",
    "        # take the first one (we don't really care here which one we use)\n",
    "        ds = dataset_dict[matching_keys[0]]\n",
    "        regridder = xe.Regridder(ds,target_grid_ds,'bilinear',ignore_degenerate=True,periodic=True) #create regridder for this source_id\n",
    "        regridders[si] = regridder\n",
    "    return regridders\n",
    "\n",
    "def make_filepath(output_path, ds):\n",
    "    key = ds.attrs[\"ddict_key\"]\n",
    "    variable = ds.attrs['variable_id']\n",
    "    return os.path.join(output_path,variable,ds.source_id,key+'.hist_'+ssp)\n",
    "\n",
    "def pr_flux_to_m(ds):\n",
    "    if 'pr' in ds.variables: #if dataset contains precipitation\n",
    "        if ds.pr.units == 'kg m-2 s-1': #if precipitation units are flux 'kg m-2 s-1', convert to daily accumulated 'm'\n",
    "            with xr.set_options(keep_attrs=True):\n",
    "                ds['pr'] = 24*3600*ds['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "                ds.pr.attrs['units'] = 'm'#convert pr units to m if\n",
    "        elif ds.pr.units == 'm':\n",
    "            ds = ds\n",
    "        else:\n",
    "            raise('Variable pr has unrecognized unit.')\n",
    "    return ds \n",
    "\n",
    "def drop_duplicate_timesteps_from_ds(ds):\n",
    "    #select only unique timesteps\n",
    "    unique_time, idx = np.unique(ds.time,return_index=True)\n",
    "\n",
    "    if len(ds.time) != len(unique_time):\n",
    "        ds = ds.isel(time=idx)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "# output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'\n",
    "output_path = 'gs://leap-scratch/timh37/CMIP6/datasets_eu_1p5/'\n",
    "overwrite_existing = True #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "target_grid = xr.Dataset( #grid to interpolate CMIP6 simulations to\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-30,22.5,1.5), {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,30,-1.5), {\"units\": \"degrees_north\"}),})\n",
    "\n",
    "query_vars = ['sfcWind','pr','psl'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalog\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "#assign priority for keeping duplicate datasets in these catalogs\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogs\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalog per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogs provide duplicate datasets, keep the newest version, and if the versions are identical, keep the highest priority catalog\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21df06e-0a6b-47f9-b48d-b032aed6d2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2373' class='' max='2373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2373/2373 04:34&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c3a422a4b04dae9eeb5ca17d271560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#open datasets in dictionary\n",
    "ssp_cats_merged.esmcat.aggregation_control.groupby_attrs = []\n",
    "ddict_all = ssp_cats_merged.to_dataset_dict(zarr_kwargs={'use_cftime':True},aggregate=True) # single stores (Perhaps we dont need some of them, but at this point we do not really care)\n",
    "\n",
    "#create xesmf regridders for each unique CMIP6 model\n",
    "regridder_dict = create_regridder_dict(ddict_all, target_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f7dd37-45d9-4805-942d-cd10bced18fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ClusterReport<name=prod.593d3a0b876a430c83e81dfcfd87fc2c, status=RUNNING>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-15cd2178-6cd3-11ee-9880-5278ac735694</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_gateway.GatewayCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/services/dask-gateway/clusters/prod.6d206a3df0fd4a0882539666f28e9840/status\" target=\"_blank\">/services/dask-gateway/clusters/prod.6d206a3df0fd4a0882539666f28e9840/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/services/dask-gateway/clusters/prod.6d206a3df0fd4a0882539666f28e9840/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>GatewayCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Name: </b>prod.6d206a3df0fd4a0882539666f28e9840\n",
       "    <li><b>Dashboard: </b><a href='/services/dask-gateway/clusters/prod.6d206a3df0fd4a0882539666f28e9840/status' target='_blank'>/services/dask-gateway/clusters/prod.6d206a3df0fd4a0882539666f28e9840/status</a>\n",
       "  </ul>\n",
       "</div>\n",
       "\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tls://10.0.57.55:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start new dask cluster\n",
    "gateway = Gateway()\n",
    "\n",
    "# close existing clusters (be careful if you have multiple clusters/servers open!)\n",
    "open_clusters = gateway.list_clusters()\n",
    "print(list(open_clusters))\n",
    "if len(open_clusters)>0:\n",
    "    for c in open_clusters:\n",
    "        cluster = gateway.connect(c.name)\n",
    "        cluster.shutdown()  \n",
    "\n",
    "\n",
    "options = gateway.cluster_options()\n",
    "options.worker_memory = 18\n",
    "# options.worker_cores = 12\n",
    "\n",
    "# Create a cluster with those options\n",
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client()\n",
    "cluster.adapt(20, 100)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbb1af-d12b-464b-88aa-bb46283b4c7d",
   "metadata": {},
   "source": [
    "Regridding step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed0495e-813e-4181-a601-e4d548807eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29356b24f8884127a8204e8e9ea1ffed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regridded_datasets = []\n",
    "for key,ds in tqdm(ddict_all.items()):\n",
    "    ds.attrs[\"ddict_key\"] = key #add current key information to attributes\n",
    "    output_fn = make_filepath(output_path, ds)\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "    regridder = regridder_dict[ds.attrs['source_id']] #select regridder for this source_id\n",
    "    regridded_ds = regridder(ds, keep_attrs=True) #do the regridding\n",
    "    \n",
    "    regridded_datasets.append(regridded_ds.unify_chunks().chunk({'time':40000})) #append to list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9d3cd-2366-4957-973d-ffa7bb0c02aa",
   "metadata": {},
   "source": [
    "Save regridder datasets to zarr in parallel in batches using dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f713e1f-4765-4e72-aae9-1a6c980cdc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef666a1c7f0400ca68ef633c19365cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 187.69 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.98 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 186.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 168.37 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 180.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 196.40 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 191.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 219.45 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 198.96 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 177.23 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 179.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 181.61 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 203.81 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.21 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 200.72 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 199.32 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 183.00 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 182.08 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 200.73 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 187.60 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 187.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 181.51 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.05 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 191.30 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 176.82 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 193.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 193.54 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 174.76 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 176.15 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 167.65 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 180.42 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.39 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 190.80 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 179.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 194.94 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 198.67 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 206.69 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.45 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 180.27 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 204.70 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 194.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 197.15 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 213.71 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 175.39 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 190.63 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 86.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# following https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask\n",
    "def write_wrapper(ds, overwrite=overwrite_existing, fs=None): #wrapper around to_zarr to submit to dask distributed cluster\n",
    "    target = make_filepath(output_path, ds)\n",
    "    with worker_client() as client:\n",
    "        try:\n",
    "            if overwrite or not fs.exists(target): # only write if store doesnt exist or overwrite is true\n",
    "                ds = ds.drop_dims(['bnds','nbnd','height'],errors=\"ignore\") #drop some auxiliary coordinates\n",
    "                ds = pr_flux_to_m(ds) #convert pr units to m if pr in ds\n",
    "                ds = drop_duplicate_timesteps_from_ds(ds)          \n",
    "                \n",
    "                ds.to_zarr(store=target, mode='w') #store\n",
    "                return target, 'written freshly'\n",
    "            else:\n",
    "                return target, 'already written, skipped'\n",
    "        except Exception as e:\n",
    "            return target, f\"Failed with: {e}\"\n",
    "\n",
    "# There is some more advanced way of doing this with the `as_completed` iterator, to achieve a 'steady' supply of submissions to the client. # (see answers in https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask), \n",
    "# but for our intents and purposes, we can just submit medium sized batches here:\n",
    "# This seems to scale ok (there is still downtime between the batch submissions). For comparison, just using a big cluster and looping over the datasets achieved ~3x speed up (not bad), \n",
    "# but here we are looking at 10+x\n",
    "\n",
    "interval = 50 # this seems to work fine, except a few warnings about a large graph... You could play with this, but higher numbers seemed to \n",
    "# make the scheduler quite unstable...\n",
    "regridded_datasets_batches = [regridded_datasets[a:a+interval] for a in range(0,len(regridded_datasets), interval)]\n",
    "\n",
    "written_stores = []\n",
    "\n",
    "for ds_batch in tqdm(regridded_datasets_batches):\n",
    "    # futures = [client.submit(write_wrapper, ds) for ds in ds_batch]\n",
    "    futures = client.map(write_wrapper, ds_batch, overwrite=False, fs=fs)\n",
    "    for future, result in as_completed(futures, with_results=True):\n",
    "        written_stores.append(result)\n",
    "        future.release()\n",
    "    # do we need to deal with failed futures?\n",
    "    # explicitly delete futures to ease pressure on client (JB: I do not 100% understand how this works TBH).\n",
    "    del futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977e0ce5f88e41c6b76a5e39a5cf8848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1069' class='' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      67.49% [1069/1584 03:09&lt;01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(['ssp245','ssp585'])): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    cat_to_open = drop_vars_from_cat(cat_to_open,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables\n",
    "    #open datasets into dictionary\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug # @jbusecke: Which bug are you referring to? Issue?\n",
    "\n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    # @jbusecke: Curious why you are not using xMIP here?\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "    #\n",
    "\n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps if datasets have them\n",
    "    #ddict = select_period(ddict,1850,2100) #preselect time periods, do this at later stage in the chain?\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b003760-0a1c-49b5-a481-717f5e0ae45c",
   "metadata": {},
   "source": [
    "Calculate total size of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5e757-04a4-4c05-aeef-83a4acdb7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=0\n",
    "for k,v in ddict.items():\n",
    "    if 'ssp245' in k:\n",
    "        x += v.nbytes/1000000000\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
