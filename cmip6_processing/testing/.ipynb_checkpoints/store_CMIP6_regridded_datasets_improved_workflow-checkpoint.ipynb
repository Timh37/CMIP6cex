{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "from distributed import worker_client, as_completed\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "from typing import Dict\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8731ac78-8165-4157-affd-4c48215cb4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create regridders per source_id (datasets of source_id should be on the same grid after passing through reduce_cat_to_max_num_realizations\n",
    "def create_regridder_dict(dataset_dict: Dict[str, xr.Dataset], target_grid_ds: xr.Dataset) -> Dict[str, xe.Regridder]:\n",
    "    regridders = {}\n",
    "    source_ids = np.unique([ds.attrs['source_id'] for ds in dataset_dict.values()])\n",
    "    for si in tqdm(source_ids):\n",
    "        matching_keys = [k for k in dataset_dict.keys() if si in k]\n",
    "        # take the first one (we don't really care here which one we use)\n",
    "        ds = dataset_dict[matching_keys[0]]\n",
    "        regridder = xe.Regridder(ds,target_grid_ds,'bilinear',ignore_degenerate=True,periodic=True) #create regridder for this source_id\n",
    "        regridders[si] = regridder\n",
    "    return regridders\n",
    "\n",
    "def make_filepath(output_path, ds):\n",
    "    key = ds.attrs[\"ddict_key\"]\n",
    "    variable = ds.attrs['variable_id']\n",
    "    return os.path.join(output_path,variable,ds.source_id,key.split('.gs')[0])\n",
    "\n",
    "def pr_flux_to_m(ds):\n",
    "    if 'pr' in ds.variables: #if dataset contains precipitation\n",
    "        if ds.pr.units == 'kg m-2 s-1': #if precipitation units are flux 'kg m-2 s-1', convert to daily accumulated 'm'\n",
    "            with xr.set_options(keep_attrs=True):\n",
    "                ds['pr'] = 24*3600*ds['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "                ds.pr.attrs['units'] = 'm'#convert pr units to m if\n",
    "        elif ds.pr.units == 'm':\n",
    "            ds = ds\n",
    "        else:\n",
    "            raise('Variable pr has unrecognized unit.')\n",
    "    return ds \n",
    "\n",
    "def drop_duplicate_timesteps_from_ds(ds):\n",
    "    #select only unique timesteps\n",
    "    unique_time, idx = np.unique(ds.time,return_index=True)\n",
    "\n",
    "    if len(ds.time) != len(unique_time):\n",
    "        ds = ds.isel(time=idx)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "# output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'\n",
    "output_path = 'gs://leap-persistent/timh37/CMIP6/datasets_eu_1p5/'\n",
    "overwrite_existing = True #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "target_grid = xr.Dataset( #grid to interpolate CMIP6 simulations to\n",
    "        {   \"longitude\": ([\"longitude\"], np.arange(-30,22.5,1.5), {\"units\": \"degrees_east\"}),\n",
    "            \"latitude\": ([\"latitude\"], np.arange(70,30,-1.5), {\"units\": \"degrees_north\"}),})\n",
    "\n",
    "query_vars = ['sfcWind','pr','psl'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalog\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "#assign priority for keeping duplicate datasets in these catalogs\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogs\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalog per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogs provide duplicate datasets, keep the newest version, and if the versions are identical, keep the highest priority catalog\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21df06e-0a6b-47f9-b48d-b032aed6d2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2373' class='' max='2373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2373/2373 04:12&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022d3610e7ca4efe82638f47833810ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#open datasets in dictionary\n",
    "ssp_cats_merged.esmcat.aggregation_control.groupby_attrs = []\n",
    "ddict_all = ssp_cats_merged.to_dataset_dict(zarr_kwargs={'use_cftime':True},aggregate=True) # single stores (Perhaps we dont need some of them, but at this point we do not really care)\n",
    "\n",
    "#create xesmf regridders for each unique CMIP6 model\n",
    "regridder_dict = create_regridder_dict(ddict_all, target_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f7dd37-45d9-4805-942d-cd10bced18fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ClusterReport<name=prod.4adef9d9663248e3802d1793181dccdb, status=RUNNING>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "No option 'worker_memory' available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m         cluster\u001b[38;5;241m.\u001b[39mshutdown()  \n\u001b[1;32m     13\u001b[0m options \u001b[38;5;241m=\u001b[39m gateway\u001b[38;5;241m.\u001b[39mcluster_options()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker_memory\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# options.worker_cores = 12\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a cluster with those options\u001b[39;00m\n\u001b[1;32m     18\u001b[0m cluster \u001b[38;5;241m=\u001b[39m gateway\u001b[38;5;241m.\u001b[39mnew_cluster(options)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/dask_gateway/options.py:112\u001b[0m, in \u001b[0;36mOptions.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mAttributeError\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/dask_gateway/options.py:106\u001b[0m, in \u001b[0;36mOptions._set\u001b[0;34m(self, key, value, exc_cls)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields[key]\u001b[38;5;241m.\u001b[39mset(value)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_cls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo option \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: No option 'worker_memory' available"
     ]
    }
   ],
   "source": [
    "## start new dask cluster\n",
    "gateway = Gateway()\n",
    "\n",
    "# close existing clusters (be careful if you have multiple clusters/servers open!)\n",
    "open_clusters = gateway.list_clusters()\n",
    "print(list(open_clusters))\n",
    "if len(open_clusters)>0:\n",
    "    for c in open_clusters:\n",
    "        cluster = gateway.connect(c.name)\n",
    "        cluster.shutdown()  \n",
    "\n",
    "\n",
    "options = gateway.cluster_options()\n",
    "options.worker_memory = 18\n",
    "# options.worker_cores = 12\n",
    "\n",
    "# Create a cluster with those options\n",
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client()\n",
    "cluster.adapt(20, 100)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fba8e6b-82de-496c-baee-f6c783cb5f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2cc549fd414047a9d86bdfdb886bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Cluster Options</h2>'), GridBox(children=(HTML(value=\"<p style='font-weight: bo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options<instance_type='n2-highmem-16',\n",
      "        worker_resource_allocation='1CPU, 7.2Gi',\n",
      "        image='pangeo/pangeo-notebook:2023.08.29',\n",
      "        environment={'SCRATCH_BUCKET': 'gs://leap-scratch/timh37',\n",
      "         'PANGEO_SCRATCH': 'gs://leap-scratch/timh37'},\n",
      "        idle_timeout_minutes=30>\n"
     ]
    }
   ],
   "source": [
    "gateway.cluster_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbb1af-d12b-464b-88aa-bb46283b4c7d",
   "metadata": {},
   "source": [
    "Regridding step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed0495e-813e-4181-a601-e4d548807eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8c3ff5b0f741a1a26a71bb794ff41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regridded_datasets = []\n",
    "for key,ds in tqdm(ddict_all.items()):\n",
    "    ds.attrs[\"ddict_key\"] = key #add current key information to attributes\n",
    "    output_fn = make_filepath(output_path, ds)\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "    regridder = regridder_dict[ds.attrs['source_id']] #select regridder for this source_id\n",
    "    regridded_ds = regridder(ds, keep_attrs=True) #do the regridding\n",
    "    \n",
    "    regridded_datasets.append(regridded_ds.unify_chunks().chunk({'time':40000})) #append to list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9d3cd-2366-4957-973d-ffa7bb0c02aa",
   "metadata": {},
   "source": [
    "Save regridder datasets to zarr in parallel in batches using dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f713e1f-4765-4e72-aae9-1a6c980cdc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e188f0ba6525440185db305e9973a4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 185.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 197.61 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 203.33 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 185.26 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 178.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 178.41 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 192.10 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 195.85 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 187.80 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 181.68 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 197.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 190.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 200.24 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 174.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 175.21 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 190.12 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 196.04 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.20 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 183.72 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 184.15 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 182.43 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 191.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 185.03 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 199.03 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 184.13 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 200.02 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 201.94 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 199.65 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 185.27 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.50 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 181.59 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 196.37 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 196.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 188.39 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 200.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 204.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 168.28 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 178.29 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 173.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 190.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 181.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 186.09 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 177.97 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 211.00 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 206.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 184.56 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 81.34 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/ssl.py\", line 1342, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1007)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 192, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1454, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1385, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2023-10-23 18:50:11,566 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "# following https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask\n",
    "def write_wrapper(ds, overwrite=overwrite_existing, fs=None): #wrapper around to_zarr to submit to dask distributed cluster\n",
    "    target = make_filepath(output_path, ds)\n",
    "    with worker_client() as client:\n",
    "        try:\n",
    "            if overwrite or not fs.exists(target): # only write if store doesnt exist or overwrite is true\n",
    "                ds = ds.drop_dims(['bnds','nbnd','height'],errors=\"ignore\") #drop some auxiliary coordinates\n",
    "                ds = pr_flux_to_m(ds) #convert pr units to m if pr in ds\n",
    "                ds = drop_duplicate_timesteps_from_ds(ds)          \n",
    "                \n",
    "                ds.to_zarr(store=target, mode='w') #store\n",
    "                return target, 'written freshly'\n",
    "            else:\n",
    "                return target, 'already written, skipped'\n",
    "        except Exception as e:\n",
    "            return target, f\"Failed with: {e}\"\n",
    "\n",
    "# There is some more advanced way of doing this with the `as_completed` iterator, to achieve a 'steady' supply of submissions to the client. # (see answers in https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask), \n",
    "# but for our intents and purposes, we can just submit medium sized batches here:\n",
    "# This seems to scale ok (there is still downtime between the batch submissions). For comparison, just using a big cluster and looping over the datasets achieved ~3x speed up (not bad), \n",
    "# but here we are looking at 10+x\n",
    "\n",
    "interval = 50 # this seems to work fine, except a few warnings about a large graph... You could play with this, but higher numbers seemed to \n",
    "# make the scheduler quite unstable...\n",
    "regridded_datasets_batches = [regridded_datasets[a:a+interval] for a in range(0,len(regridded_datasets), interval)]\n",
    "\n",
    "written_stores = []\n",
    "\n",
    "for ds_batch in tqdm(regridded_datasets_batches):\n",
    "    # futures = [client.submit(write_wrapper, ds) for ds in ds_batch]\n",
    "    futures = client.map(write_wrapper, ds_batch, overwrite=False, fs=fs)\n",
    "    for future, result in as_completed(futures, with_results=True):\n",
    "        written_stores.append(result)\n",
    "        future.release()\n",
    "    # do we need to deal with failed futures?\n",
    "    # explicitly delete futures to ease pressure on client (JB: I do not 100% understand how this works TBH).\n",
    "    del futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977e0ce5f88e41c6b76a5e39a5cf8848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1069' class='' max='1584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      67.49% [1069/1584 03:09&lt;01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(['ssp245','ssp585'])): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    cat_to_open = drop_vars_from_cat(cat_to_open,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables\n",
    "    #open datasets into dictionary\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug # @jbusecke: Which bug are you referring to? Issue?\n",
    "\n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    # @jbusecke: Curious why you are not using xMIP here?\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "    #\n",
    "\n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps if datasets have them\n",
    "    #ddict = select_period(ddict,1850,2100) #preselect time periods, do this at later stage in the chain?\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b003760-0a1c-49b5-a481-717f5e0ae45c",
   "metadata": {},
   "source": [
    "Calculate total size of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5e757-04a4-4c05-aeef-83a4acdb7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=0\n",
    "for k,v in ddict.items():\n",
    "    if 'ssp245' in k:\n",
    "        x += v.nbytes/1000000000\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
