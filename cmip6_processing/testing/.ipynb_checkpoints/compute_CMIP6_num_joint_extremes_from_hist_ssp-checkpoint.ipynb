{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8899c2-c522-42e0-aa61-ce53a758f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1290/4005817676.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import fnmatch\n",
    "from tqdm.autonotebook import tqdm\n",
    "import dask\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853c8e9b-73b2-47c3-9881-09abe4ce60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def po_t_of_refyear(da,threshold,refyear,dim):\n",
    "    return da.where(da>da.sel(window=refyear).quantile(threshold,dim=dim))\n",
    "            \n",
    "def rolling_max(da,window_len,dim):\n",
    "    return da.rolling({dim:window_len},center=True,min_periods=1).max()\n",
    "\n",
    "def count_num_extremes_pmonth(extremes):\n",
    "    extremes_ = extremes.copy(deep=True) #must be boolean array (True or False (joint) extreme occurs on that day)!\n",
    "    if len(extremes.time.shape)>1:\n",
    "        extremes_['time_in_window_idx'] = extremes_.time.dt.month.isel(window=0).values\n",
    "    else:\n",
    "        extremes_['time_in_window_idx'] = extremes_.time.dt.month.values\n",
    "    num_extremes_pmonth = extremes_.rename({'time_in_window_idx':'month'}).groupby('month').sum()\n",
    "    return num_extremes_pmonth        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be0e9a-7d00-4d95-b211-57e51b0467c6",
   "metadata": {},
   "source": [
    "Configure the bivariate sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e2303f-56de-45d6-9f19-0e31cf0af7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure bivariate sampling settings\n",
    "max_lag = 0 #maximum time between co-occurring extremes (0 = no lag)\n",
    "declus_window_len = 3 #rolling window length for declustering (1 = no declustering)\n",
    "threshold = .98 #quantile above which events are defined extreme\n",
    "\n",
    "output_yrs = np.arange(2000,2100,20)\n",
    "window_len = 40 #period length around output_yrs\n",
    "ref_year = 2000 #historical period to to compute thresholds from\n",
    "\n",
    "#configure the input directories and grid_type\n",
    "var1_dir = 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind'\n",
    "var2_dir = 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/pr'\n",
    "\n",
    "out_dir = '/home/jovyan/CMIP6cex/output/num_extremes/'\n",
    "\n",
    "overwrite_existing=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4119c7f2-037c-420d-9c97-efd3992859a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r10i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r1i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r2i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r3i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r4i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r5i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r6i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r7i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r8i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/CMIP.CSIRO-ARCCSS.ACCESS-CM2.historical.r9i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp245.r1i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp245.r2i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp245.r3i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp245.r4i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp245.r5i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r10i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r1i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r2i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r3i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r4i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r5i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r6i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r7i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r8i1p1f1.day.psl.gn.gs:',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2/ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp585.r9i1p1f1.day.psl.gn.gs:']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls('leap-scratch/timh37/CMIP6/datasets_eu_1p5/psl/ACCESS-CM2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbe831e1-3bf8-46e7-9f29-8485e22e1c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/ACCESS-CM2',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/ACCESS-ESM1-5',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2-WACCM',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CMCC-CM2-SR5',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CMCC-ESM2',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CanESM5',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/EC-Earth3',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/EC-Earth3-Veg',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/FGOALS-g3',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/GFDL-CM4',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/GFDL-ESM4',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/HadGEM3-GC31-LL',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/HadGEM3-GC31-MM',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/IITM-ESM',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/INM-CM4-8',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/INM-CM5-0',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/IPSL-CM6A-LR',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/KACE-1-0-G',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/MIROC-ES2L',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/MIROC6',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/MPI-ESM1-2-HR',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/MPI-ESM1-2-LR',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/MRI-ESM2-0',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/NorESM2-LM',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/NorESM2-MM',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/TaiESM1',\n",
       " 'leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/UKESM1-0-LL']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls(var1_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e352be93-62ff-4254-9910-be60a5239fda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2/CMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'gs://leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2/CMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/mapping.py:143\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_exceptions:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:121\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:106\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:61\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:433\u001b[0m, in \u001b[0;36mAsyncFileSystem._cat\u001b[0;34m(self, path, recursive, on_error, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex:\n\u001b[0;32m--> 433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m paths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    438\u001b[0m ):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:878\u001b[0m, in \u001b[0;36mGCSFileSystem._cat_file\u001b[0;34m(self, path, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m     head \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 878\u001b[0m headers, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, u2, headers\u001b[38;5;241m=\u001b[39mhead)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:430\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    431\u001b[0m     method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_out:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/retry.py:114\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    116\u001b[0m     HttpError,\n\u001b[1;32m    117\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    121\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:423\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 423\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/retry.py:83\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     85\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: https://storage.googleapis.com/download/storage/v1/b/leap-scratch/o/timh37%2FCMIP6%2Fdatasets_eu_1p5%2FsfcWind%2FCESM2%2FCMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs%3A%2F.zmetadata?alt=media",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:1428\u001b[0m, in \u001b[0;36mFSStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/mapping.py:147\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:429\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/convenience.py:1351\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:2947\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2947\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:1430\u001b[0m, in \u001b[0;36mFSStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:432\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    447\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/hierarchy.py:1532\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, meta_array)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[0;32m-> 1532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(path)\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCESM2\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_path:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(fn)\n\u001b[0;32m----> 6\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     ddict[ds\u001b[38;5;241m.\u001b[39mkey] \u001b[38;5;241m=\u001b[39m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:965\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    946\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     zarr_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    963\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    964\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 965\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:449\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    433\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    447\u001b[0m             )\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m zarr\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mGroupNotFoundError:\n\u001b[0;32m--> 449\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_consolidated(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'gs://leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2/CMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs:'"
     ]
    }
   ],
   "source": [
    "ddict = defaultdict(dict)\n",
    "for model_path in fs.ls(var1_dir):\n",
    "    for fn in fs.ls(model_path):\n",
    "        if 'CESM2' in model_path:\n",
    "            print(fn)\n",
    "            ds = xr.open_dataset('gs://'+fn,engine='zarr',chunks={})\n",
    "            ddict[ds.key] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0091cf54-7b62-4b5a-9371-3dbbeb6ec6df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'gs://leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2/CMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/mapping.py:143\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_exceptions:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:121\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:106\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:61\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/asyn.py:433\u001b[0m, in \u001b[0;36mAsyncFileSystem._cat\u001b[0;34m(self, path, recursive, on_error, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex:\n\u001b[0;32m--> 433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m paths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    438\u001b[0m ):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:878\u001b[0m, in \u001b[0;36mGCSFileSystem._cat_file\u001b[0;34m(self, path, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m     head \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 878\u001b[0m headers, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, u2, headers\u001b[38;5;241m=\u001b[39mhead)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:430\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    431\u001b[0m     method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_out:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/retry.py:114\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    116\u001b[0m     HttpError,\n\u001b[1;32m    117\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    121\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/core.py:423\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 423\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/gcsfs/retry.py:83\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     85\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: https://storage.googleapis.com/download/storage/v1/b/leap-scratch/o/timh37%2FCMIP6%2Fdatasets_eu_1p5%2FsfcWind%2FCESM2%2FCMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs%3A%2F.zmetadata?alt=media",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:1428\u001b[0m, in \u001b[0;36mFSStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fsspec/mapping.py:147\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:429\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/convenience.py:1351\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:2947\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2947\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2949\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/storage.py:1430\u001b[0m, in \u001b[0;36mFSStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:432\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    447\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/zarr/hierarchy.py:1532\u001b[0m, in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options, zarr_version, meta_array)\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ContainsArrayError(path)\n\u001b[0;32m-> 1532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GroupNotFoundError(path)\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzarr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/api.py:570\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    559\u001b[0m     decode_cf,\n\u001b[1;32m    560\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    569\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 570\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    577\u001b[0m     backend_ds,\n\u001b[1;32m    578\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    589\u001b[0m )\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:965\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    946\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     zarr_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    963\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    964\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 965\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/backends/zarr.py:449\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    433\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to open Zarr store with consolidated metadata, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut successfully read with non-consolidated metadata. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    447\u001b[0m             )\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m zarr\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mGroupNotFoundError:\n\u001b[0;32m--> 449\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_consolidated(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'gs://leap-scratch/timh37/CMIP6/datasets_eu_1p5/sfcWind/CESM2/CMIP.NCAR.CESM2.historical.r11i1p1f1.day.sfcWind.gn.gs:'"
     ]
    }
   ],
   "source": [
    "xr.open_dataset('gs://'+fn,engine='zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0320f070-cc9c-4a5a-9ecd-942e06a9d802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.open_dataset('gs://leap-scratch/jbusecke/CMIPcex/timeseries_eu_1p5/pr/CMCC-ESM2/CMIP.CMCC.CMCC-ESM2.historical.r1i1p1f1.day.pr.gn.gs://cmip6/CMIP6/CMIP/CMCC/CMCC-ESM2/historical/r1i1p1f1/day/pr/gn/v20210114/.20210114.hist_ssp585',\n",
    "               engine='zarr',chunks={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c60a7-b2b9-423f-bb2a-5e8263f9c71b",
   "metadata": {},
   "source": [
    "Open datasets into dictionary of datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57ca67d-4dd6-4eb7-bd54-d3f2d63df818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc06d2f8053f4cca97518c48cb891382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make output folder\n",
    "var1 = var1_dir.split('/')[-1]\n",
    "var2 = var2_dir.split('/')[-1]\n",
    "\n",
    "out_path = os.path.join(out_dir,var1_dir.split('/')[-2],var1+'_'+var2,\n",
    "                        str(window_len)+'yr_'+str(threshold).replace('0.','p')+'_lag'+str(max_lag)+'d_declus'+str(declus_window_len)+'d_ref'+str(ref_year))\n",
    "\n",
    "#open datasets\n",
    "var1_models = [k.split('/')[-1] for k in fs.ls(var1_dir) if k.startswith('.')==False]\n",
    "var2_models = [k.split('/')[-1] for k in fs.ls(var2_dir) if k.startswith('.')==False]\n",
    "models = [k for k in var1_models if k in var2_models]\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "for s,source_id in tqdm(enumerate(models)):\n",
    "    for f,file in enumerate(fs.ls(os.path.join(var1_dir,source_id))):\n",
    "        \n",
    "        try:\n",
    "            var1_var2_ds = xr.open_mfdataset(('gs://'+file,'gs://'+file.replace(var1,var2)),engine='zarr',use_cftime=True)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        k = file.split('/')[-1]\n",
    "        ddict[k] = var1_var2_ds.sel(time=slice(str(int(output_yrs[0]-window_len/2)), str(int(output_yrs[-1]+window_len/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e78fb4-9992-4ae1-ad1f-19a4dab57942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e10baf79c9744ae9eae01befd81e94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,ds in tqdm(ddict.items()):\n",
    "    if 'member_id' not in ds.dims:\n",
    "        ds = ds.expand_dims('member_id')\n",
    "    ds = ds.load()\n",
    "    ds = ds.transpose('time',...)\n",
    "   \n",
    "    source_id = k.split('.')[0]\n",
    "    output_fn = os.path.join(out_path,source_id,k.replace(var1,'num_extremes'))+'.nc'\n",
    "    \n",
    "    if (~overwrite_existing) & (os.path.exists(output_fn)):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            os.makedirs(os.path.join(out_path,source_id))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    attrs = ds.attrs\n",
    "    ds = ds.isel(member_id=0) \n",
    "    \n",
    "    #remove leap days\n",
    "    if len(np.unique(ds.time.resample(time='1Y').count()))>1: #remove leap days so that each computation window has the same length\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            ds = ds.sel(time=~((ds.time.dt.month == 2) & (ds.time.dt.day == 29))) #^probably (hopefully) only has a small effect on the results\n",
    "\n",
    "    days_in_year = int(ds.time.resample(time='1Y').count()[0])\n",
    "    \n",
    "    #construct time period indices\n",
    "    if window_len%2 !=0: #odd\n",
    "        window_start_idx = days_in_year*(output_yrs-int(output_yrs[0]-window_len/2)-int(np.floor(window_len/2)))\n",
    "        first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "    else: #even\n",
    "        window_start_idx = days_in_year*(output_yrs-int(output_yrs[0]-window_len/2)-int(window_len/2)+1)\n",
    "        first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "\n",
    "    if np.max(first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:])>=len(ds.time): #if window exceeds simulation length\n",
    "        if (int(output_yrs[-1]+window_len/2)==2100) & (ddict[k].time.dt.year[-1]==2099): #if end year is 2099 instead of 2100, shift windows to 1 year earlier\n",
    "            window_start_idx = window_start_idx - days_in_year #shift windows back by 1 year\n",
    "        else:\n",
    "            print('Requested time periods exceed simulation length for '+k)\n",
    "        continue #skip\n",
    "\n",
    "    window_idx = xr.DataArray( #indices of time periods\n",
    "        data=first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:],\n",
    "        dims=[\"time_in_window_idx\",\"window\"],\n",
    "        coords=dict(time_in_window_idx=first_window_idx,window=output_yrs),)\n",
    "    \n",
    "    #select data in user-defined time windows\n",
    "    try:\n",
    "        ds_wdws = ds.isel(time=window_idx) \n",
    "    except:\n",
    "        print('could not select requested time windows from '+k)\n",
    "        continue\n",
    "        \n",
    "    #find extremes using POT with historical threshold values\n",
    "    var1_extremes = po_t_of_refyear(ds_wdws[var1],threshold,ref_year,dim='time_in_window_idx')\n",
    "    var2_extremes = po_t_of_refyear(ds_wdws[var2],threshold,ref_year,dim='time_in_window_idx')\n",
    "\n",
    "    var1_extremes_declustered = var1_extremes.where(var1_extremes==var1_extremes.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "    var2_extremes_declustered = var2_extremes.where(var2_extremes==var2_extremes.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "\n",
    "    #boolean array of when days experience joint extremes within 'max_lag' lag from eachother\n",
    "    joint_extremes = np.isfinite((rolling_max(var2_extremes_declustered,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_declustered)) \n",
    "    \n",
    "    #store number of extremes per month in dataset\n",
    "    num_extremes = count_num_extremes_pmonth(joint_extremes).to_dataset(name='num_joint_extremes') #joint extremes\n",
    "    num_extremes['num_'+var1+'_extremes'] = count_num_extremes_pmonth(np.isfinite(var1_extremes_declustered)) #univariate extremes var1\n",
    "    num_extremes['num_'+var2+'_extremes'] = count_num_extremes_pmonth(np.isfinite(var2_extremes_declustered)) #univariate extremes var2\n",
    "\n",
    "    \n",
    "    #decompose future changes in number of joint extremes (only if not declustering and 0-day lag)\n",
    "    \n",
    "    #initialize output\n",
    "    num_extremes['num_joint_extremes_'+var1+'_driven']          = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var1\n",
    "    num_extremes['num_joint_extremes_'+var2+'_driven']          = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var2\n",
    "    num_extremes['num_joint_extremes_'+var1+'_'+var2+'_driven'] = np.nan*num_extremes['num_joint_extremes'].copy(deep=True) #dN due to univariate changes var1 and var2\n",
    "    \n",
    "    num_extremes['num_'+var1+'_extremes_refWindow_futT']        = np.nan*num_extremes['num_'+var1+'_extremes'].copy(deep=True) #number of peaks in future windows exceeding the reference threshold values\n",
    "    num_extremes['num_'+var2+'_extremes_refWindow_futT']        = np.nan*num_extremes['num_'+var2+'_extremes'].copy(deep=True) #number of peaks in future windows exceeding the reference threshold values\n",
    "    \n",
    "    if (max_lag == 0) & (declus_window_len==1):\n",
    "    \n",
    "        #1) sort (in magnitude) the values in the reference period of each variable\n",
    "        if ('latitude' in ds.dims) & ('longitude' in ds.dims):\n",
    "            sorted_var1_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var1],axis=0),dims=ds_wdws.sel(window=ref_year)[var1].dims,\n",
    "                                               coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,latitude=ds_wdws.latitude,longitude=ds_wdws.longitude))\n",
    "            sorted_var2_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var2],axis=0),dims=ds_wdws.sel(window=ref_year)[var2].dims,\n",
    "                                          coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,latitude=ds_wdws.latitude,longitude=ds_wdws.longitude))\n",
    "        elif ('tg' in ds.dims):\n",
    "            sorted_var1_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var1],axis=0),dims=ds_wdws.sel(window=ref_year)[var1].dims,\n",
    "                                               coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,tg=ds_wdws.tg))\n",
    "            sorted_var2_ref = xr.DataArray(data=np.sort(ds_wdws.sel(window=ref_year)[var2],axis=0),dims=ds_wdws.sel(window=ref_year)[var2].dims,\n",
    "                                          coords=dict(time_in_window_idx=ds_wdws.time_in_window_idx,tg=ds_wdws.tg))\n",
    "        \n",
    "        #2) find the number of extremes that exceed the reference period threshold value in other windows    \n",
    "        for w,win in enumerate(ds_wdws.window): #loop over each window\n",
    "            num_var1_extremes_in_wdw = np.isfinite(var1_extremes_declustered).sum(dim='time_in_window_idx').sel(window=win)#.load()\n",
    "            num_var2_extremes_in_wdw = np.isfinite(var2_extremes_declustered).sum(dim='time_in_window_idx').sel(window=win)#.load()\n",
    "        \n",
    "            #use that number to define the equivalent threshold in the historical period (var_{U_{var}}^{hist} in the paper)\n",
    "            var1_eqv_thresholds = sorted_var1_ref.isel(time_in_window_idx=-1*(num_var1_extremes_in_wdw))\n",
    "            var2_eqv_thresholds = sorted_var2_ref.isel(time_in_window_idx=-1*(num_var2_extremes_in_wdw))\n",
    "\n",
    "            #3) determine the extremes above those threshold values in the reference window\n",
    "            var1_extremes_fut_threshold = ds_wdws[var1].sel(window=ref_year).where(ds_wdws[var1].sel(window=ref_year)>=var1_eqv_thresholds)\n",
    "            var2_extremes_fut_threshold = ds_wdws[var2].sel(window=ref_year).where(ds_wdws[var2].sel(window=ref_year)>=var2_eqv_thresholds)\n",
    "\n",
    "            #4) determine the joint extremes using these extremes:\n",
    "            joint_extremes_var1_driven = np.isfinite((rolling_max(var2_extremes_declustered.sel(window=ref_year),max_lag*2+1,dim='time_in_window_idx')*var1_extremes_fut_threshold))\n",
    "            joint_extremes_var2_driven = np.isfinite((rolling_max(var2_extremes_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_declustered.sel(window=ref_year)))\n",
    "            joint_extremes_var1_var2_driven = np.isfinite((rolling_max(var2_extremes_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_extremes_fut_threshold))\n",
    "\n",
    "            #5) count per month & write to output dataset\n",
    "            num_extremes['num_joint_extremes_'+var1+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var1_driven)\n",
    "            num_extremes['num_joint_extremes_'+var2+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var2_driven)\n",
    "            num_extremes['num_joint_extremes_'+var1+'_'+var2+'_driven'].loc[dict(window=win)] = count_num_extremes_pmonth(joint_extremes_var1_var2_driven)\n",
    "            \n",
    "            num_extremes['num_'+var1+'_extremes_refWindow_futT'].loc[dict(window=win)] = count_num_extremes_pmonth(np.isfinite(var1_extremes_fut_threshold))\n",
    "            num_extremes['num_'+var2+'_extremes_refWindow_futT'].loc[dict(window=win)] = count_num_extremes_pmonth(np.isfinite(var2_extremes_fut_threshold))\n",
    "            \n",
    "    #store the results\n",
    "    num_extremes = num_extremes.expand_dims('member_id') #add back member_id as dimension\n",
    "    num_extremes.attrs = attrs #add original attributes\n",
    "    \n",
    "    #add information about the joint extremes analysis\n",
    "    num_extremes.attrs['window_length'] = str(window_len)\n",
    "    num_extremes.attrs['declustering_length'] = str(declus_window_len)\n",
    "    num_extremes.attrs['allowed_lag'] = str(max_lag)\n",
    "    num_extremes.attrs['ref_window'] = str(ref_year)\n",
    "    num_extremes.attrs['source_id'] = source_id\n",
    "    num_extremes.to_netcdf(output_fn,mode='w')\n",
    "    num_extremes.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cefce-2355-4c31-b95b-7310725cab32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be9c911-ab1a-4611-af33-6c0c3f429761",
   "metadata": {},
   "source": [
    "var1 = var1_dir.split('/')[-1]\n",
    "var2 = var2_dir.split('/')[-1]\n",
    "\n",
    "models_var1 = [k.split('/')[-1] for k in fs.ls(var1_dir)]\n",
    "models_var2 = [k.split('/')[-1] for k in fs.ls(var2_dir)]\n",
    "source_ids = sorted(list(set(models_var1) & set(models_var2))) #intersection of models\n",
    "\n",
    "for source_id in [k for k in source_ids if ~k.startswith('.')]: #loop over models\n",
    "  \n",
    "    var1_model_path = os.path.join(var1_dir,source_id)\n",
    "    var2_model_path = os.path.join(var2_dir,source_id)\n",
    "    \n",
    "    #sfcWind_exps = [s.split('_')[-1][0:-3] for s in os.listdir(sfcWind_path) if s.startswith('.')==False]\n",
    "    #pr_exps = [s.split('_')[-1][0:-3] for s in os.listdir(pr_path) if s.startswith('.')==False]\n",
    "    \n",
    "    #get experiment_id's\n",
    "    var1_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var1_model_path) if s.startswith('.')==False] \n",
    "    var2_exps = [s.split('/')[-1].split('_')[-1][0:-5] for s in fs.ls(var2_model_path) if s.startswith('.')==False]\n",
    "    experiment_ids = list(set(var1_exps) & set(var2_exps))\n",
    "\n",
    "    for experiment_id in experiment_ids: #loop over experiments\n",
    "        #load data:\n",
    "        fn = fnmatch.filter(fs.ls(var1_model_path),'*'+experiment_id+'*')[0]\n",
    "        fn = fn.split('/')[-1]\n",
    "        print('Processing file: '+fn)\n",
    "        if input_is_gridded==False:\n",
    "            var1_var2_data = xr.open_mfdataset((os.path.join('gs://',var1_model_path,fn),os.path.join('gs://',var2_model_path,fn)),engine='zarr',compat='override',chunks={'member_id':1,'time':100000})\n",
    "        else:\n",
    "            #sfcWind_pr = xr.open_mfdataset((os.path.join(sfcWind_path,fn),os.path.join(pr_path,fn)),chunks={'member_id':1,'time':100000,'longitude':3})#.sel(longitude=np.arange(-25,11))\n",
    "            var1_var2_data = xr.open_mfdataset((os.path.join('gs://',var1_model_path,fn),os.path.join('gs://',var2_model_path,fn)),engine='zarr',chunks={'member_id':1,'time':100000,'longitude':5})\n",
    " \n",
    "        #generate output paths\n",
    "        #model_path = os.path.join('/home/jovyan/CMIP6cf/output/dependence/sfcWind_pr_europe/40yr_p98_lag0d_declus1d_ref2000',sfcWind_pr.source_id)\n",
    "        output_path = '/home/jovyan/CMIP6cex/output/num_extremes/'+var1+'_g2_'+var2+'_'+var1_dir.split('_')[-1]+'/'+str(window_len)+'yr_'+str(threshold).replace('0.','p')+'_lag'+str(max_lag)+'d_declus'+str(declus_window_len)+'d_ref'+str(ref_year)\n",
    "        output_model_path = os.path.join(output_path,var1_var2_data.source_id)\n",
    "        output_fn = os.path.join(output_model_path,fn.replace('.zarr','.nc'))\n",
    "\n",
    "        #construct time window indices\n",
    "        if len(np.unique(var1_var2_data.time.resample(time='1Y').count()))>1: #remove leap days so that each computation window has the same length\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "                var1_var2_data = var1_var2_data.sel(time=~((var1_var2_data.time.dt.month == 2) & (var1_var2_data.time.dt.day == 29))) #^probably (hopefully) only has a small effect on the results\n",
    "        \n",
    "        days_in_year = int(var1_var2_data.time.resample(time='1Y').count()[0])\n",
    "        \n",
    "        if window_len%2 !=0: #odd\n",
    "            window_start_idx = days_in_year*(output_yrs-1850-int(np.floor(window_len/2)))\n",
    "            first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "        else: #even\n",
    "            window_start_idx = days_in_year*(output_yrs-1850-int(window_len/2)+1)\n",
    "            first_window_idx = np.arange(0*days_in_year,window_len*days_in_year)\n",
    "        \n",
    "        if np.max(first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:])>=len(var1_var2_data.time): #if window exceeds simulation length\n",
    "            continue #skip\n",
    "            #raise Exception('Windows exceed simulation length.')\n",
    "            \n",
    "        window_idx = xr.DataArray( #indices of windows\n",
    "            data=first_window_idx[:,np.newaxis]+window_start_idx[np.newaxis,:],\n",
    "            dims=[\"time_in_window_idx\",\"window\"],\n",
    "            coords=dict(\n",
    "                time_in_window_idx=first_window_idx,\n",
    "                window=output_yrs\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "        if not os.path.exists(output_model_path):\n",
    "            os.mkdir(output_model_path)\n",
    "            \n",
    "        for m,member in tqdm(enumerate(var1_var2_data.member_id)): #loop over members of each model to compute the dependence\n",
    "        \n",
    "            var1_var2_data_mem = var1_var2_data.sel(member_id=member)\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                var1_var2_data_wdws = var1_var2_data_mem.isel(time=window_idx) #select data in user-defined time windows\n",
    "            \n",
    "            data_is_complete = np.isfinite(var1_var2_data_wdws[var1]).all(dim='time_in_window_idx') * np.isfinite(var1_var2_data_wdws[var2]).all(dim='time_in_window_idx') #check data-completeness in each window\n",
    "\n",
    "            #derive peaks\n",
    "            var1_peaks = po_t_of_refyear(var1_var2_data_wdws[var1],threshold,ref_year,dim='time_in_window_idx')\n",
    "            var2_peaks = po_t_of_refyear(var1_var2_data_wdws[var2],threshold,ref_year,dim='time_in_window_idx')\n",
    "            \n",
    "            var1_peaks_declustered = var1_peaks.where(var1_peaks==var1_peaks.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "            var2_peaks_declustered = var2_peaks.where(var2_peaks==var2_peaks.rolling({'time_in_window_idx':declus_window_len},center=True,min_periods=1).max(skipna=True))\n",
    "            \n",
    "            #determine joint extremes within 'max_lag' lag from eachother\n",
    "            joint_extremes = np.isfinite((rolling_max(var2_peaks_declustered,max_lag*2+1,dim='time_in_window_idx')*var1_peaks_declustered)) #previously: 'co_occurring'\n",
    "            \n",
    "            #generate output dataset for current member\n",
    "            num_extremes_mem = sum_num_extremes_pmonth(joint_extremes).to_dataset(name='num_joint_extremes')\n",
    "            num_extremes_mem['num_'+var1+'_extremes'] = sum_num_extremes_pmonth(np.isfinite(var1_peaks_declustered))\n",
    "            num_extremes_mem['num_'+var2+'_extremes'] = sum_num_extremes_pmonth(np.isfinite(var2_peaks_declustered))\n",
    "            \n",
    "            ####DECOMPOSITION OF CHANGES (probably not correct if declustering!!):\n",
    "            #1) sort (in magnitude) values in reference period to determine the equivalent threshold percentiles in other windows\n",
    "            if input_is_gridded:\n",
    "                sorted_var1_ref = xr.DataArray(data=np.sort(var1_var2_data_wdws.sel(window=ref_year)[var1],axis=0),dims=['time_in_window_idx','latitude','longitude'],\n",
    "                                                   coords=dict(time_in_window_idx=var1_var2_data_wdws.time_in_window_idx,latitude=var1_var2_data_wdws.latitude,longitude=var1_var2_data_wdws.longitude)).chunk({'longitude':5})\n",
    "                sorted_var2_ref = xr.DataArray(data=np.sort(var1_var2_data_wdws.sel(window=ref_year)[var2],axis=0),dims=['time_in_window_idx','latitude','longitude'],\n",
    "                                              coords=dict(time_in_window_idx=var1_var2_data_wdws.time_in_window_idx,latitude=var1_var2_data_wdws.latitude,longitude=var1_var2_data_wdws.longitude)).chunk({'longitude':5})\n",
    "            else:\n",
    "                sorted_var1_ref = xr.DataArray(data=np.sort(var1_var2_data_wdws.sel(window=ref_year)[var1],axis=0),dims=['time_in_window_idx','tg'],\n",
    "                                                   coords=dict(time_in_window_idx=var1_var2_data_wdws.time_in_window_idx,tg=var1_var2_data_wdws.tg))\n",
    "                sorted_var2_ref = xr.DataArray(data=np.sort(var1_var2_data_wdws.sel(window=ref_year)[var2],axis=0),dims=['time_in_window_idx','tg'],\n",
    "                                              coords=dict(time_in_window_idx=var1_var2_data_wdws.time_in_window_idx,tg=var1_var2_data_wdws.tg))\n",
    "            #initialize output arrays\n",
    "            num_extremes_mem['num_joint_extremes_'+var1+'_driven'] = num_extremes_mem['num_joint_extremes'].copy(deep=True)\n",
    "            num_extremes_mem['num_joint_extremes_'+var2+'_driven'] = num_extremes_mem['num_joint_extremes'].copy(deep=True)\n",
    "            num_extremes_mem['num_joint_extremes_'+var1+'_'+var2+'_driven'] = num_extremes_mem['num_joint_extremes'].copy(deep=True)\n",
    "            num_extremes_mem['num_'+var1+'_extremes_refWindow_futT'] = num_extremes_mem['num_'+var1+'_extremes'].copy(deep=True)\n",
    "            num_extremes_mem['num_'+var2+'_extremes_refWindow_futT'] = num_extremes_mem['num_'+var2+'_extremes'].copy(deep=True)\n",
    "            \n",
    "            for w,win in enumerate(var1_var2_data_wdws.window): #loop over each window to do the decomposition\n",
    "                #2) find the threshold values in the reference period corresponding to the percentile of events exceeding the reference threshold values in the future (var_{U_{var}}^{hist} in the paper)\n",
    "                var1_eqv_thresholds = sorted_var1_ref.isel(time_in_window_idx=-1*(np.isfinite(var1_peaks_declustered).sum(dim='time_in_window_idx').sel(window=win).load()))\n",
    "                var2_eqv_thresholds = sorted_var2_ref.isel(time_in_window_idx=-1*(np.isfinite(var2_peaks_declustered).sum(dim='time_in_window_idx').sel(window=win).load()))\n",
    "                \n",
    "                #3) determine the peaks above those threshold values in the reference window\n",
    "                var1_peaks_fut_threshold = var1_var2_data_wdws[var1].sel(window=ref_year).where(var1_var2_data_wdws[var1].sel(window=ref_year)>=var1_eqv_thresholds) #determine the peaks in the reference period above those values\n",
    "                var2_peaks_fut_threshold = var1_var2_data_wdws[var2].sel(window=ref_year).where(var1_var2_data_wdws[var2].sel(window=ref_year)>=var2_eqv_thresholds)\n",
    "\n",
    "                #4) determine the joint extremes for different components:\n",
    "                # a) var2 peaks above standard threshold in reference period, var1 above future threshold percentile in reference period\n",
    "                joint_extremes_var1_driven = np.isfinite((rolling_max(var2_peaks_declustered.sel(window=ref_year),max_lag*2+1,dim='time_in_window_idx')*var1_peaks_fut_threshold))\n",
    "\n",
    "                # b) var1 peaks above standard threshold in reference period, var2 above future threshold percentile in reference period\n",
    "                joint_extremes_var2_driven = np.isfinite((rolling_max(var2_peaks_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_peaks_declustered.sel(window=ref_year)))\n",
    "\n",
    "                # c) var1 and var 2 above future threshold percentile in reference period\n",
    "                joint_extremes_var1_var2_driven = np.isfinite((rolling_max(var2_peaks_fut_threshold,max_lag*2+1,dim='time_in_window_idx')*var1_peaks_fut_threshold))\n",
    "           \n",
    "                #count per month\n",
    "                num_extremes_mem['num_joint_extremes_'+var1+'_driven'].loc[dict(window=win)] = sum_num_extremes_pmonth(joint_extremes_var1_driven)\n",
    "                num_extremes_mem['num_joint_extremes_'+var2+'_driven'].loc[dict(window=win)] = sum_num_extremes_pmonth(joint_extremes_var2_driven)\n",
    "                num_extremes_mem['num_joint_extremes_'+var1+'_'+var2+'_driven'].loc[dict(window=win)] = sum_num_extremes_pmonth(joint_extremes_var1_var2_driven)\n",
    "                num_extremes_mem['num_'+var1+'_extremes_refWindow_futT'].loc[dict(window=win)] = sum_num_extremes_pmonth(np.isfinite(var1_peaks_fut_threshold))\n",
    "                num_extremes_mem['num_'+var2+'_extremes_refWindow_futT'].loc[dict(window=win)] = sum_num_extremes_pmonth(np.isfinite(var2_peaks_fut_threshold))\n",
    "            \n",
    "            #store metadata\n",
    "            num_extremes_mem['complete_window'] = data_is_complete #store where windows miss data\n",
    "                        \n",
    "            num_extremes_mem = num_extremes_mem.expand_dims(dim={\"member_id\": 1}) #add coordinates & dimensions\n",
    "\n",
    "            num_extremes_mem.attrs = var1_var2_data.attrs #keep original attributes and add information on the extremes analysis\n",
    "            num_extremes_mem.attrs['window_length'] = str(window_len)\n",
    "            num_extremes_mem.attrs['declustering'] = 'Rolling window of '+str(declus_window_len)+' days'\n",
    "            num_extremes_mem.attrs['allowed_lag'] = str(max_lag)\n",
    "            num_extremes_mem.attrs['ref_window'] = str(ref_year)\n",
    "            \n",
    "            num_extremes_mem.to_netcdf(output_fn.replace('.nc','_'+num_extremes_mem.member_id.values[0]+'.nc'),mode='w')\n",
    "            num_extremes_mem.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
