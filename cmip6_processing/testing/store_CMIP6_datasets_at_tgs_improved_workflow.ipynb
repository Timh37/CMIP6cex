{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5105/3747057756.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "from distributed import worker_client, as_completed\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "from typing import Dict\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f71554-c3ab-4127-ba54-a1138f7ce204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lonlat_idx_nearest_to_tgs(tg_ds,ds):\n",
    "    '''tg_ds = xr.DataSet containing 'lon' and 'lat' coordinates of tide gauges\n",
    "    ds    = xr.DataSet containing CMIP6 data to subset\n",
    "    '''\n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #compute distances between TG coordinates and grid cell centers\n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-tg_ds.lat) )**2 +\n",
    "        np.cos((np.pi/180)*tg_ds.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-tg_ds.lon))**2) )\n",
    "    \n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name]) #find indices of nearest grid cells\n",
    "    return idx_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8731ac78-8165-4157-affd-4c48215cb4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create regridders per source_id (datasets of source_id should be on the same grid after passing through reduce_cat_to_max_num_realizations\n",
    "def create_regridder_dict(dataset_dict: Dict[str, xr.Dataset], target_grid_ds: xr.Dataset) -> Dict[str, xe.Regridder]:\n",
    "    regridders = {}\n",
    "    source_ids = np.unique([ds.attrs['source_id'] for ds in dataset_dict.values()])\n",
    "    for si in tqdm(source_ids):\n",
    "        matching_keys = [k for k in dataset_dict.keys() if si in k]\n",
    "        # take the first one (we don't really care here which one we use)\n",
    "        ds = dataset_dict[matching_keys[0]]\n",
    "        regridder = xe.Regridder(ds,target_grid_ds,'bilinear',ignore_degenerate=True,periodic=True) #create regridder for this source_id\n",
    "        regridders[si] = regridder\n",
    "    return regridders\n",
    "\n",
    "def create_regridder_dict_per_tg(dataset_dict,tg_coords):\n",
    "    regridders = {}\n",
    "    \n",
    "    source_ids = np.unique([ds.attrs['source_id'] for ds in dataset_dict.values()])\n",
    "    for si in tqdm(source_ids):\n",
    "        source_regridders = {}\n",
    "        matching_keys = [k for k in dataset_dict.keys() if si in k]\n",
    "        # take the first one (we don't really care here which one we use)\n",
    "        ds = dataset_dict[matching_keys[0]]\n",
    "        \n",
    "        for tg in range(len(tg_coords.tg)):\n",
    "            single_tg = tg_coords.isel(tg=[tg])\n",
    "            regridder = xe.Regridder(ds,single_tg, method='nearest_s2d',periodic=True,ignore_degenerate=True)\n",
    "            source_regridders[str(single_tg.tg.values)] = regridder\n",
    "        regridders[si] = source_regridders\n",
    "        \n",
    "    return regridders\n",
    "\n",
    "\n",
    "def create_nearest_idx_dict(dataset_dict,tg_coords):\n",
    "    indices = {}\n",
    "    \n",
    "    source_ids = np.unique([ds.attrs['source_id'] for ds in dataset_dict.values()])\n",
    "    for si in tqdm(source_ids):\n",
    "        matching_keys = [k for k in dataset_dict.keys() if si in k]\n",
    "        # take the first one (we don't really care here which one we use)\n",
    "        ds = dataset_dict[matching_keys[0]]\n",
    "        indices[si] = get_lonlat_idx_nearest_to_tgs(tg_coords,ds)\n",
    "        \n",
    "    return indices\n",
    "    \n",
    "def make_filepath(output_path, ds):\n",
    "    key = ds.attrs[\"ddict_key\"]\n",
    "    variable = ds.attrs['variable_id']\n",
    "    return os.path.join(output_path,variable,ds.source_id,key.split('.gs')[0])\n",
    "\n",
    "def pr_flux_to_m(ds):\n",
    "    if 'pr' in ds.variables: #if dataset contains precipitation\n",
    "        if ds.pr.units == 'kg m-2 s-1': #if precipitation units are flux 'kg m-2 s-1', convert to daily accumulated 'm'\n",
    "            with xr.set_options(keep_attrs=True):\n",
    "                ds['pr'] = 24*3600*ds['pr']/1000 #multiply by number of seconds in a day to get to kg m-2, and divide by density (kg/m3) to get to m    \n",
    "                ds.pr.attrs['units'] = 'm'#convert pr units to m if\n",
    "        elif ds.pr.units == 'm':\n",
    "            ds = ds\n",
    "        else:\n",
    "            raise('Variable pr has unrecognized unit.')\n",
    "    return ds \n",
    "\n",
    "def drop_duplicate_timesteps_from_ds(ds):\n",
    "    #select only unique timesteps\n",
    "    unique_time, idx = np.unique(ds.time,return_index=True)\n",
    "\n",
    "    if len(ds.time) != len(unique_time):\n",
    "        ds = ds.isel(time=idx)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "# output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'\n",
    "output_path = 'gs://leap-persistent/timh37/CMIP6/datasets_eu_gesla2_tgs/'\n",
    "overwrite_existing = True #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "tg_coords = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/gssr_mlr_coefs_1p5_9deg_gesla2.nc')\n",
    "\n",
    "query_vars = ['sfcWind','pr'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalog\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "#assign priority for keeping duplicate datasets in these catalogs\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogs\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalog per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogs provide duplicate datasets, keep the newest version, and if the versions are identical, keep the highest priority catalog\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21df06e-0a6b-47f9-b48d-b032aed6d2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2373' class='' max='2373' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2373/2373 04:24&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#open datasets in dictionary\n",
    "ssp_cats_merged.esmcat.aggregation_control.groupby_attrs = []\n",
    "ddict_all = ssp_cats_merged.to_dataset_dict(zarr_kwargs={'use_cftime':True},aggregate=True) # single stores (Perhaps we dont need some of them, but at this point we do not really care)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeea07f4-e5c5-47bb-904e-16542bd103db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605794c0e6f04298906963be58ede162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create xesmf regridders for each unique CMIP6 model\n",
    "regridder_dict = create_nearest_idx_dict(ddict_all, tg_coords)\n",
    "#regridder_dict = create_regridder_dict_per_tg(ddict_all,tg_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f7dd37-45d9-4805-942d-cd10bced18fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-d449744d-72b5-11ee-93f1-c2434f1e9048</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_gateway.GatewayCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"/services/dask-gateway/clusters/prod.e7faf44369df4dda994b6fa6e9ba7f59/status\" target=\"_blank\">/services/dask-gateway/clusters/prod.e7faf44369df4dda994b6fa6e9ba7f59/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"/services/dask-gateway/clusters/prod.e7faf44369df4dda994b6fa6e9ba7f59/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>GatewayCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Name: </b>prod.e7faf44369df4dda994b6fa6e9ba7f59\n",
       "    <li><b>Dashboard: </b><a href='/services/dask-gateway/clusters/prod.e7faf44369df4dda994b6fa6e9ba7f59/status' target='_blank'>/services/dask-gateway/clusters/prod.e7faf44369df4dda994b6fa6e9ba7f59/status</a>\n",
       "  </ul>\n",
       "</div>\n",
       "\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tls://10.1.230.14:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start new dask cluster\n",
    "gateway = Gateway()\n",
    "\n",
    "# close existing clusters (be careful if you have multiple clusters/servers open!)\n",
    "open_clusters = gateway.list_clusters()\n",
    "print(list(open_clusters))\n",
    "if len(open_clusters)>0:\n",
    "    for c in open_clusters:\n",
    "        cluster = gateway.connect(c.name)\n",
    "        cluster.shutdown()  \n",
    "\n",
    "\n",
    "options = gateway.cluster_options()\n",
    "options.worker_memory = 18\n",
    "# options.worker_cores = 12\n",
    "\n",
    "# Create a cluster with those options\n",
    "cluster = gateway.new_cluster(options)\n",
    "client = cluster.get_client()\n",
    "cluster.adapt(20, 100)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbb1af-d12b-464b-88aa-bb46283b4c7d",
   "metadata": {},
   "source": [
    "Regridding step (using xesmf):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c12cb22-d8e4-4091-aeca-709903835e39",
   "metadata": {
    "tags": []
   },
   "source": [
    "regridded_datasets = []\n",
    "for key,ds in tqdm(ddict_all.items()):\n",
    "    ds.attrs[\"ddict_key\"] = key #add current key information to attributes\n",
    "    #output_fn = make_filepath(output_path, ds)\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "    regridders = regridder_dict[ds.attrs['source_id']] #select regridder for this source_id\n",
    "      \n",
    "    datasets=[]\n",
    "    for tg in range(len(tg_coords.tg)):\n",
    "        single_tg = tg_coords.isel(tg=[tg])\n",
    "        regridder = regridders[str(single_tg.tg.values)]\n",
    "        datasets.append(regridder(ds,keep_attrs=True).squeeze())\n",
    "      \n",
    "    ds_at_tgs = xr.concat(datasets, dim='tg')\n",
    "    ds_at_tgs['tg']=tg_coords.tg.values\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #regridded_ds = regridder(ds, keep_attrs=True) #do the regridding\n",
    "    \n",
    "    \n",
    "    #idx = get_lonlat_idx_nearest_to_tgs(tg_coords,ds)\n",
    "    \n",
    "    #lon_name = list(k for k in ds.dims if 'lon' in k)[0]\n",
    "    #lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #regridded_ds = ds.isel({lat_name:idx[lat_name],lon_name:idx[lon_name]})\n",
    "    \n",
    "    regridded_datasets.append(ds_at_tgs.unify_chunks().chunk({'time':40000,'tg':1000})) #append to list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8bd256-7988-4a19-9afc-381f5c041ca9",
   "metadata": {},
   "source": [
    "Regridding step (using fancy indexing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305646bf-6a0d-4fc3-a943-43ae56a8f5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba804212d1fa4c54aca5724fa5e217a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regridded_datasets = []\n",
    "for key,ds in tqdm(ddict_all.items()):\n",
    "    ds.attrs[\"ddict_key\"] = key #add current key information to attributes\n",
    "    output_fn = make_filepath(output_path, ds)\n",
    "    \n",
    "    ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "\n",
    "    #regridders = regridder_dict[ds.attrs['source_id']] #select regridder for this source_id\n",
    "    idx = regridder_dict[ds.attrs['source_id']]\n",
    "   \n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0]\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    regridded_ds = ds.isel({lat_name:idx[lat_name],lon_name:idx[lon_name]})\n",
    "    \n",
    "    regridded_datasets.append(regridded_ds.unify_chunks().chunk({'time':40000,'tg':1000})) #append to list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9d3cd-2366-4957-973d-ffa7bb0c02aa",
   "metadata": {},
   "source": [
    "Save regridder datasets to zarr in parallel in batches using dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f713e1f-4765-4e72-aae9-1a6c980cdc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8396a2acbf43f78369010e1f0396cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 36.53 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 33.03 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/ssl.py\", line 1342, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1007)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 192, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1454, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 1385, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.10/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2023-10-24 22:17:33,777 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/distributed/client.py:3141: UserWarning: Sending large graph of size 42.76 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# following https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask\n",
    "def write_wrapper(ds, overwrite=overwrite_existing, fs=None): #wrapper around to_zarr to submit to dask distributed cluster\n",
    "    target = make_filepath(output_path, ds)\n",
    "    with worker_client() as client:\n",
    "        try:\n",
    "            if overwrite or not fs.exists(target): # only write if store doesnt exist or overwrite is true\n",
    "                ds = ds.drop_dims(['bnds','nbnd','height'],errors=\"ignore\") #drop some auxiliary coordinates\n",
    "                ds = pr_flux_to_m(ds) #convert pr units to m if pr in ds\n",
    "                ds = drop_duplicate_timesteps_from_ds(ds)          \n",
    "                \n",
    "                ds = ds.expand_dims('member_id')\n",
    "                ds.to_zarr(store=target, mode='w') #store\n",
    "                return target, 'written freshly'\n",
    "            else:\n",
    "                return target, 'already written, skipped'\n",
    "        except Exception as e:\n",
    "            return target, f\"Failed with: {e}\"\n",
    "\n",
    "# There is some more advanced way of doing this with the `as_completed` iterator, to achieve a 'steady' supply of submissions to the client. # (see answers in https://stackoverflow.com/questions/66769922/concurrently-write-xarray-datasets-to-zarr-how-to-efficiently-scale-with-dask), \n",
    "# but for our intents and purposes, we can just submit medium sized batches here:\n",
    "# This seems to scale ok (there is still downtime between the batch submissions). For comparison, just using a big cluster and looping over the datasets achieved ~3x speed up (not bad), \n",
    "# but here we are looking at 10+x\n",
    "\n",
    "interval = 10 # this seems to work fine, except a few warnings about a large graph... You could play with this, but higher numbers seemed to \n",
    "# make the scheduler quite unstable...\n",
    "regridded_datasets_batches = [regridded_datasets[a:a+interval] for a in range(0,len(regridded_datasets), interval)]\n",
    "\n",
    "written_stores = []\n",
    "\n",
    "for ds_batch in tqdm(regridded_datasets_batches):\n",
    "    # futures = [client.submit(write_wrapper, ds) for ds in ds_batch]\n",
    "    futures = client.map(write_wrapper, ds_batch, overwrite=False, fs=fs)\n",
    "    for future, result in as_completed(futures, with_results=True):\n",
    "        written_stores.append(result)\n",
    "        future.release()\n",
    "    # do we need to deal with failed futures?\n",
    "    # explicitly delete futures to ease pressure on client (JB: I do not 100% understand how this works TBH).\n",
    "    del futures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
