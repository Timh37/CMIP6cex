{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8780/933853136.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "'''script to regrid CMIP6 datatsets to target grid and store them'''\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from xmip.postprocessing import combine_datasets, _match_datasets,_concat_sorted_time,concat_members\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat, drop_older_versions\n",
    "from cmip_ds_dict_operations import select_period, pr_flux_to_m, drop_duplicate_timesteps, drop_coords, drop_incomplete\n",
    "import xesmf as xe\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() #list stores, stripp zarr from filename, load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a10f51e-3913-4e72-ad64-61e67de65953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lonlat_idx_nearest_to_tgs(tg_ds,ds):\n",
    "    '''tg_ds = xr.DataSet containing 'lon' and 'lat' coordinates of tide gauges\n",
    "    ds    = xr.DataSet containing CMIP6 data to subset\n",
    "    '''\n",
    "    lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "    lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "    \n",
    "    #compute distances between TG coordinates and grid cell centers\n",
    "    distances = 2*np.arcsin( np.sqrt(\n",
    "        np.sin( (np.pi/180) * 0.5*(ds[lat_name]-tg_ds.lat) )**2 +\n",
    "        np.cos((np.pi/180)*tg_ds.lat)*np.cos((np.pi/180)*ds[lat_name])*np.sin((np.pi/180)*0.5*(ds[lon_name]-tg_ds.lon))**2) )\n",
    "    \n",
    "    idx_nearest = distances.argmin(dim=[lon_name,lat_name]) #find indices of nearest grid cells\n",
    "    return idx_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295422c2-6743-4393-93e8-6dd2d478660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#configure settings\n",
    "output_path = 'gs://leap-persistent/timh37/CMIP6/timeseries_eu_gesla2_tgs/test'\n",
    "overwrite_existing = True #whether or not to process files for which output already exists (to-do: implement)\n",
    "\n",
    "tg_coords = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/gssr_mlr_coefs_1p5_9deg_gesla2.nc')\n",
    "\n",
    "query_vars = ['sfcWind','pr'] #variables to process\n",
    "required_vars = ['sfcWind','pr','psl'] #variables that includes models should provide\n",
    "\n",
    "ssps = ['ssp245','ssp585']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0d6af3-eab5-4cc8-95cc-f5cf417a8f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#query simulations & manipulate data catalogue:\n",
    "col = google_cmip_col() #google cloud catalogue\n",
    "qc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\") #temporary pangeo-leap-forge catalogue\n",
    "noqc_col = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-noqc-test.json\")\n",
    "\n",
    "col_df = col.df\n",
    "qc_df = qc_col.df\n",
    "nonqc_df = noqc_col.df\n",
    "\n",
    "col_df['prio'] = 2\n",
    "qc_df['prio'] = 3\n",
    "nonqc_df['prio'] = 1\n",
    "\n",
    "col.esmcat._df = pd.concat([col_df,qc_df,nonqc_df],ignore_index=True) #merge these catalogues\n",
    "ssp_cats = defaultdict(dict)\n",
    "\n",
    "#search catalogue per ssp (need to do this for each SSP separately as availability may differ between them)\n",
    "for s,ssp in enumerate(ssps):\n",
    "    ssp_cat = col.search( #find instances providing all required query_vars for both historical & ssp experiments\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "    ssp_cats[ssp] = ssp_cat\n",
    "    \n",
    "ssp_cats_merged = ssp_cats[ssp] #merge catalogues for all ssps, and drop duplicate historical simulations\n",
    "ssp_cats_merged.esmcat._df = pd.concat([v.df for k,v in ssp_cats.items()],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "ssp_cats_merged = drop_older_versions(ssp_cats_merged) #if google cloud and leap-pangeo catalogues provide duplicate datasets, keep the newest version, and if the versions are identical, keep the leap-pangeo dataset\n",
    "ssp_cats_merged = reduce_cat_to_max_num_realizations(ssp_cats_merged) #per model, select grid and 'ipf' combination providing most realizations (needs to be applied to both SSPs together to ensure the same variants are used under both scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b138937-7159-47c0-b838-b6114e39a5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf2b4cb17b04652948e1e89627ca1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version.prio'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1064' class='' max='1064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1064/1064 02:22&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r1i1p1f1.pr\n",
      "Dropping duplicate timesteps for:FGOALS-g3.gn.day.r1i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r3i1p1f1.pr\n",
      "Dropping duplicate timesteps for:FGOALS-g3.gn.day.r3i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r2i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r3i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r4i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r11i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r1i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r4i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:CESM2.gn.day.r11i1p1f1.pr\n",
      "Dropping duplicate timesteps for:CESM2-WACCM.gn.day.r2i1p1f1.sfcWind\n",
      "Dropping duplicate timesteps for:EC-Earth3-Veg.gr.day.r5i1p1f1.pr\n",
      "Dropping duplicate timesteps for:EC-Earth3-Veg.gr.day.r5i1p1f1.sfcWind\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9fdabcb24d484f96e748ea83a169ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s,ssp in tqdm(enumerate(['ssp245'])): #for each ssp:  \n",
    "    #select historical and ssp data in merged catalogue for this particular ssp\n",
    "    cat_to_open = ssp_cats_merged.search(\n",
    "    experiment_id=['historical',ssp],\n",
    "    table_id='day',\n",
    "    variable_id=required_vars,\n",
    "    require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "    cat_to_open = drop_vars_from_cat(cat_to_open,[k for k in required_vars if k not in query_vars]) #out of required variables only process query variables\n",
    "    #open datasets into dictionary\n",
    "    cat_to_open.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "    #to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "        #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "    kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "    ddict = cat_to_open.to_dataset_dict(**kwargs) #open datasets into dictionary\n",
    "\n",
    "    #preprocess datasets in dictionary\n",
    "    ddict = pr_flux_to_m(ddict) #convert pr flux to accumulated pr\n",
    "    ddict = drop_duplicate_timesteps(ddict) #remove duplicate timesteps if datasets have them\n",
    "    #ddict = select_period(ddict,1850,2100) #preselect time periods, do this at later stage in the chain?\n",
    "    ddict = drop_coords(ddict,['bnds','nbnd','height']) #remove some unused auxiliary coordinates\n",
    "    \n",
    "    #concatenate historical and ssp datasets in time\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        hist_ssp = combine_datasets(ddict,_concat_sorted_time,match_attrs =['source_id', 'grid_label','table_id','variant_label','variable_id'],combine_func_kwargs={'join':'inner','coords':'minimal'})    \n",
    "\n",
    "    hist_ssp_ = defaultdict(dict) #probably a better way to do this, but there are approx. 1 files for which the time units are inconsistent between historical and ssp\n",
    "    for k,v in hist_ssp.items():\n",
    "        if v.time[-1].values.dtype != v.time[0].values.dtype:\n",
    "            print('dropping ' + k +' due to inconsistent timestamps in historical and ssp runs')\n",
    "            continue\n",
    "        else:\n",
    "            hist_ssp_[k] = v\n",
    "            \n",
    "    hist_ssp_ = drop_duplicate_timesteps(hist_ssp_) #remove overlap between historical and ssp experiments which sometimes exists\n",
    "    #hist_ssp_complete = drop_incomplete(hist_ssp_) #remove historical+ssp timeseries which are not montonically increasing or have large timegaps (based on Julius Buseckes rudimentary testing in CMIP6-LEAP-feadstock)\n",
    "    hist_ssp_complete= hist_ssp_\n",
    "    \n",
    "    #regrid these datasets to the target tide gauges\n",
    "    ddict_at_tgs = defaultdict(dict)\n",
    "    for key,ds in tqdm(hist_ssp_complete.items()):\n",
    "        variable = key.split('.')[-1]\n",
    "        #check if dataset was already stored\n",
    "        output_fn = os.path.join(output_path,variable,ds.source_id,key+'.hist_'+ssp) #output filename\n",
    "        try:\n",
    "            if (~overwrite_existing) & (output_fn.replace('gs://','') in fs.ls(os.path.join(output_path,variable,ds.source_id))):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        ds.attrs[\"time_concat_key\"] = key #add current key information to attributes\n",
    "        ds = ds.isel(dcpp_init_year=0,drop=True) #remove this coordinate\n",
    "        \n",
    "        if ('missing_value' in ds[variable].encoding) & ('_FillValue' in ds[variable].encoding):\n",
    "            if ds[variable].encoding['missing_value'] != ds[variable].encoding['_FillValue']:\n",
    "                del ds[variable].encoding['missing_value']\n",
    "                \n",
    "        idx_nearest = get_lonlat_idx_nearest_to_tgs(tg_coords,ds)\n",
    "        lon_name = list(k for k in ds.dims if 'lon' in k)[0] #find lon/lat coordinate names\n",
    "        lat_name = list(k for k in ds.dims if 'lat' in k)[0]\n",
    "\n",
    "        ds_at_tgs = ds.isel({lat_name:idx_nearest[lat_name],lon_name:idx_nearest[lon_name]}) #this leads the kernel to crash, but simply just indexing the first 100 latitudes is fine?\n",
    "        ds_at_tgs = ds_at_tgs.chunk({'time':len(ds_at_tgs.time),'tg':len(ds_at_tgs.tg)})\n",
    "        \n",
    "        ds_at_tgs['tg'] = ds_at_tgs.tg.astype('str') #something wrong with encoding object types in zarr, this is the work-around\n",
    "        \n",
    "        ddict_at_tgs[key] = ds_at_tgs\n",
    "        ds.to_zarr(output_fn,mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
