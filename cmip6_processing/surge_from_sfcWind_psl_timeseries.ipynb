{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5460ab-4b17-40cd-9e2c-775cb92f4e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2325/2781798931.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fnmatch\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765d24e6-db8f-49c4-95f4-49e6d68d6a68",
   "metadata": {},
   "source": [
    "Configure the script and load the MLR model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edb6074-9387-42a2-9560-3fe9d3b1334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 9 #n by n degree grid around each TG\n",
    "cmip6_resolution = 1.5 #resolution of common grid that CMIP6 data should be interpolated to (must agree with grid at which MLR is trained at)\n",
    "\n",
    "overwrite_existing = False #whether to overwrite results residing in output directory\n",
    "num_grid_cells = int(grid_size/cmip6_resolution) #compute number of grid cells\n",
    "\n",
    "start_year=1970\n",
    "end_year=2100\n",
    "\n",
    "mlr_coefs = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/gssr_mlr_coefs_1p5_9deg_gesla2.nc') #load MLR coefficients at TGs\n",
    "era5_pcs = xr.open_dataset('/home/jovyan/CMIP6cex/cmip6_processing/era5_pca_components_1p5_9deg_gesla2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Loop over timeseries of `psl` & `sfcWind` at common 1.5 by 1.5 degree grid and open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42603aced488403d99113ec643cc5680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var1 = 'psl'\n",
    "var2 = 'sfcWind'\n",
    "\n",
    "#CMIP6 \n",
    "var1_dir = 'leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'+var1 #output of store_CMIP6_regridded_timeseries.ipynb\n",
    "var2_dir = 'leap-persistent/timh37/CMIP6/timeseries_eu_1p5/'+var2 #output of store_CMIP6_regridded_timeseries.ipynb\n",
    "\n",
    "output_dir = 'leap-persistent/timh37/CMIP6/timeseries_eu_gesla2_tgs/'\n",
    "\n",
    "var1_models = [k.split('/')[-1] for k in fs.ls(var1_dir) if k.startswith('.')==False] #find available models\n",
    "var2_models = [k.split('/')[-1] for k in fs.ls(var2_dir) if k.startswith('.')==False]\n",
    "\n",
    "models = [k for k in var1_models if k in var2_models]\n",
    "ddict = defaultdict(dict)\n",
    "\n",
    "#open regridded timeseries for var1 and 2 for each available model\n",
    "for s,source_id in tqdm(enumerate(models)):\n",
    "    for f,file in enumerate(fs.ls(os.path.join(var1_dir,source_id))): #for each variant/SSP\n",
    "        \n",
    "        try:\n",
    "            var1_var2_ds = xr.open_mfdataset(('gs://'+file,'gs://'+file.replace(var1,var2)),engine='zarr',use_cftime=True)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        k = file.split('/')[-1]\n",
    "        ddict[k] = var1_var2_ds.sel(time=slice(str(start_year), str(end_year))) #put datasets into dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e35df4-da0a-4380-bbb5-82f957cbe422",
   "metadata": {},
   "source": [
    "Generate predictors and compute surges for each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5120150-7b95-438b-bffb-9b940d19c8a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29e7e87e48c4182bbef517fd79ab918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate lon/lat indices around each TG and put them in dataarrays used for fancy indexing\n",
    "ds0 = ddict[list(ddict)[0]]\n",
    "\n",
    "lat_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution))) #initialize\n",
    "lon_ranges = np.zeros((len(mlr_coefs.tg),int(grid_size/cmip6_resolution)))\n",
    "\n",
    "for t,tg in enumerate(mlr_coefs.tg.values): #grids around TGs\n",
    "    lat_ranges[t,:] = ds0.latitude[((ds0.latitude>=(mlr_coefs.sel(tg=tg).lat-grid_size/2)) & (ds0.latitude<=(mlr_coefs.sel(tg=tg).lat+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "    lon_ranges[t,:] = ds0.longitude[((ds0.longitude>=(mlr_coefs.sel(tg=tg).lon-grid_size/2)) & (ds0.longitude<=(mlr_coefs.sel(tg=tg).lon+grid_size/2)))][0:int(grid_size/cmip6_resolution)]\n",
    "\n",
    "lons_da = xr.DataArray(lon_ranges,dims=['tg','lon_around_tg'],coords={'tg':mlr_coefs.tg,'lon_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "lats_da = xr.DataArray(lat_ranges,dims=['tg','lat_around_tg'],coords={'tg':mlr_coefs.tg,'lat_around_tg':np.arange(0,int(grid_size/cmip6_resolution))})\n",
    "\n",
    "for k,ds in tqdm(ddict.items()): #for each psl & sfcWind combination for a given model, variant and/or SSP\n",
    "    output_fn = os.path.join(output_dir,'surge',ds.source_id,k.replace(var1,'surge')) #make output filename\n",
    "    try:\n",
    "        if (~overwrite_existing) & (output_fn.replace('gs://','') in fs.ls(os.path.join(output_dir,'surge',ds.source_id))):\n",
    "            continue\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    attrs = ds.attrs #temporarily store input dataset attributes\n",
    "    ds['sfcWind_sqd'] = ds['sfcWind']**2 #add wind squared to MLR predictors\n",
    "    ds['sfcWind_cbd'] = ds['sfcWind']**3 #add wind cubed to MLR predictors\n",
    "\n",
    "    ds = (ds-ds.mean(dim='time'))/ds.std(dim='time',ddof=0) #normalize predictor variables\n",
    "   \n",
    "    ds = ds.isel(member_id=0).load() #load into memory\n",
    "    ds = ds.sel(latitude=lats_da,longitude=lons_da) #select data at lon & lat around TGs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * grid_size * grid_size))\n",
    "    ds['predictors'] = ds[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"predictor_var\") \n",
    "    ds['predictors'] = ds['predictors'].transpose(\"time\",\"predictor_var\",\"lon_around_tg\",...).stack(f=['predictor_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "\n",
    "    #compute surges from predictors:\n",
    "    surge_ds = xr.Dataset(data_vars=dict(surge=(['time','tg'], np.nan*np.zeros( (len(ds.time),len(mlr_coefs.tg))) )),\n",
    "                            coords=dict(time=ds.time,tg=mlr_coefs.tg)) #initialize output dataset per model\n",
    "    \n",
    "    for t,tg in enumerate(mlr_coefs.tg): #loop over tide gauges (necessary to loop because of EOF analysis)\n",
    "        predictors_at_tg = ds.sel(tg=tg) #get predictors for this TG\n",
    "        mlr_coefs_at_tg = mlr_coefs.mlr_coefs.sel(tg=tg) #get MLR coefficients for this TG\n",
    "\n",
    "        num_pcs = int(np.sum(np.isfinite(mlr_coefs_at_tg)))-1 #number of mlr coefs = number of PCs to derive, intercept doesn't count\n",
    "        idx_timesteps_w_data = np.argwhere((np.isfinite(predictors_at_tg.predictors).all(axis=1)).values).flatten() #omit timesteps with NaN if any\n",
    "\n",
    "        #get principal components from predictors\n",
    "        pca = PCA(num_pcs)\n",
    "        pca.fit(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data)) #remove missing values for PCA\n",
    "        pcs = pca.transform(predictors_at_tg.predictors.isel(time=idx_timesteps_w_data))\n",
    "\n",
    "        components = xr.DataArray(data=pca.components_,dims=['pc','f'],coords=dict(pc=np.arange(num_pcs),f=predictors_at_tg.f)) #PCA spatial patterns\n",
    "\n",
    "        #compute RMSEs with ERA5 principal component patterns, only considering the pressure part of the forcing (first num_grid_cells**2)\n",
    "        rmses = np.sqrt(((components.isel(f=np.arange(num_grid_cells**2))-era5_pcs.sel(tg=tg).isel(f=np.arange(num_grid_cells**2)).isel(pc=np.arange(num_pcs)).component)**2).mean(dim='f')) #original sign\n",
    "        rmses_flipped = np.sqrt(((components.isel(f=np.arange(num_grid_cells**2))--era5_pcs.isel(f=np.arange(num_grid_cells**2)).sel(tg=tg).isel(pc=np.arange(num_pcs)).component)**2).mean(dim='f')) #opposite sign\n",
    "\n",
    "        s = (rmses<rmses_flipped).astype('int') #flip pcs if rmse of flipped pc is lower (see manuscript for more explanation)\n",
    "        s[s==0]=-1\n",
    "        pcs = pcs * s.values\n",
    "\n",
    "        #multiply with ERA5 regression coefficients to compute surges\n",
    "        surge_ds['surge'][idx_timesteps_w_data,t] = np.sum(mlr_coefs_at_tg[np.isfinite(mlr_coefs_at_tg)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "    \n",
    "    #store the output\n",
    "    surge_ds.attrs        = attrs\n",
    "    surge_ds              = surge_ds.expand_dims('member_id')\n",
    "    surge_ds['tg']        = surge_ds.tg.astype('str') #something wrong with encoding object types in zarr, this is the work-around\n",
    "    surge_ds['member_id'] = surge_ds.member_id.astype('str')\n",
    "    \n",
    "    #storage\n",
    "    surge_ds.chunk({'time':len(surge_ds.time)}).to_zarr('gs://'+output_fn,mode='w')\n",
    "    surge_ds = {} #empty the output for the next iteration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
