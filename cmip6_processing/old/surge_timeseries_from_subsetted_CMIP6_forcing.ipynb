{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cc54f9-e4f0-42f1-b90d-8869b6866786",
   "metadata": {},
   "source": [
    "**Need a lot of memory to run this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204334ce-ab38-445f-aad0-ae91d07b400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1002/1654824287.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from xmip.postprocessing import combine_datasets,_concat_sorted_time\n",
    "from sklearn.decomposition import PCA\n",
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem() # equivalent to fsspec.fs('gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ccba6-255a-464b-9c64-3ff2803720ee",
   "metadata": {},
   "source": [
    "Loop over subsetted `psl` & `sfcWind` datasets and open them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19544d38-afd3-4cb3-8dad-cde0a1c2e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in_dir = '/home/jovyan/CMIP6cf/output/subsetted_forcing/'\n",
    "in_dir = 'leap-persistent/timh37/CMIP6/subsetted_data/forcing_gssr_tgs'\n",
    "models = [k.split('/')[-1] for k in fs.ls(in_dir)]\n",
    "\n",
    "ddict = defaultdict(dict)\n",
    "#for source_id in [s for s in os.listdir(in_dir) if s.startswith('.')==False]:\n",
    "for source_id in ['CESM2']:#[s for s in models if s.startswith('.')==False]:\n",
    "    \n",
    "    experiments = [s.split('/')[-1].split('_')[2] for s in fs.ls(os.path.join(in_dir,source_id))]\n",
    "    experiment_ids = [s for s in experiments if s.startswith('.')==False]\n",
    "    #experiment_ids = [s.split('_')[2] for s in os.listdir(os.path.join(in_dir,source_id)) if s.startswith('.')==False]\n",
    "    \n",
    "    for experiment_id in set(experiment_ids): #for each experiment_id, open the datasets, concatenating all realizations:\n",
    "        \n",
    "        #source_ds = xr.open_mfdataset(os.path.join(in_dir,source_id,'*'+experiment_id+'*.nc'),join='outer',combine='nested',\n",
    "        #                              compat='override',coords='minimal',concat_dim='member_id') #need to test this for large np. of realizations, like EC-Earth3\n",
    "        source_ds = xr.open_mfdataset(os.path.join('gs://',in_dir,source_id,'*'+experiment_id+'*.zarr'),engine='zarr',chunks={},join='outer',combine='nested',\n",
    "                                      compat='override',coords='minimal',concat_dim='member_id') #need to test this for large np. of realizations, like EC-Earth3\n",
    "        \n",
    "        ddict[source_ds.original_key.rsplit('.',1)[0]] = source_ds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa5f5a-beb7-4e81-b6c7-e913faeed978",
   "metadata": {},
   "source": [
    "Append SSP runs to historical runs for each SSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae6b18b5-86aa-4075-b10a-986e817754f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssps = set([k.split('.')[2] for k in ddict.keys() if 'ssp' in k])\n",
    "\n",
    "ddict_concat = defaultdict(dict)\n",
    "\n",
    "for ssp in ssps:\n",
    "    ddict_ssp = defaultdict(dict)\n",
    "    \n",
    "    for k in ddict.keys():\n",
    "        if ((ssp in k) or ('historical' in k)):\n",
    "            if k.replace('historical',ssp) in ddict.keys(): #only consider historical if there's also ssp\n",
    "                ddict_ssp[k] = ddict[k]\n",
    "            \n",
    "    #append SSP to historical, only for realizations for which both experiments are provided (join=inner)\n",
    "    hist_ssp = combine_datasets(ddict_ssp,\n",
    "                                _concat_sorted_time,\n",
    "                                match_attrs =['source_id', 'grid_label','table_id'],combine_func_kwargs={'join':'inner'})\n",
    "    \n",
    "    for key,ds in hist_ssp.items(): #put back together in dictionary\n",
    "        ddict_concat[key+'.'+ssp] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba084-7e91-4236-8038-0f102af6a5fb",
   "metadata": {},
   "source": [
    "Sanity-check timeseries length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e669831-6ae9-4d7c-acbf-337047c2e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ddict_concat.items():\n",
    "    num_days = (v.time[-1]-v.time[0]).dt.days\n",
    "    assert (len(v.time) > .9*num_days) & (len(v.time) < 1.1*num_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1e7bc-b42d-4831-88f6-f466c61472bc",
   "metadata": {},
   "source": [
    "Generate forcing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b52404-5a12-416d-b07c-93167e7926f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate forcing to compute surges with\n",
    "for k,v in ddict_concat.items():\n",
    "    attrs = v.attrs\n",
    "    \n",
    "    v['sfcWind_sqd'] = v['sfcWind']**2 #add wind squared\n",
    "    v['sfcWind_cbd'] = v['sfcWind']**3 #add wind cubed\n",
    "    \n",
    "    v = (v-v.mean(dim='time'))/v.std(dim='time',ddof=0) #normalize (ignores nan by default?)\n",
    "    v.attrs = attrs\n",
    "    \n",
    "    #concatenate & stack normalized forcing variables to data array with shape (time,(4 variables * num_degr * num_degr))\n",
    "    v['forcing'] = v[[\"psl\", \"sfcWind\", \"sfcWind_sqd\",\"sfcWind_cbd\"]].to_array(dim=\"forcing_var\") \n",
    "    v['forcing'] = v['forcing'].transpose(\"time\",\"forcing_var\",\"lon_around_tg\",...).stack(f=['forcing_var','lon_around_tg','lat_around_tg'],create_index=False)\n",
    "    ddict_concat[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58fb60-3813-4edc-92b5-32b93741a8e9",
   "metadata": {},
   "source": [
    "Derive the principal components and multiply with regression coefficients derived from ERA5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e2fbb9-aa37-4b5d-a27b-6e84b946ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrcoefs = xr.open_dataset('/home/jovyan/CMIP6cf/gssr_coefs_1degRes_forcing.nc') #contains coordinates of and MLR coefficients at TGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62d50cfe-172c-4687-88cf-04c9111d0fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641699e49a8848ebbd2abe16512d004d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deriving surges from forcing for: CESM2.gn.day.ssp585\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataArray' object has no attribute 'to_zarr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m surge_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m surge_ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39massign_coords(lon\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtg\u001b[39m\u001b[38;5;124m'\u001b[39m, mlrcoefs\u001b[38;5;241m.\u001b[39mlon\u001b[38;5;241m.\u001b[39mdata),lat\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtg\u001b[39m\u001b[38;5;124m'\u001b[39m, mlrcoefs\u001b[38;5;241m.\u001b[39mlat\u001b[38;5;241m.\u001b[39mdata))\u001b[38;5;241m.\u001b[39massign_attrs(ds\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#store:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#surge_ds['surge'].to_netcdf(output_fn,mode='w')\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43msurge_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msurge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_zarr\u001b[49m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs://\u001b[39m\u001b[38;5;124m'\u001b[39m,output_fn),mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m surge_ds\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/xarray/core/common.py:268\u001b[0m, in \u001b[0;36mAttrAccessMixin.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m source[name]\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataArray' object has no attribute 'to_zarr'"
     ]
    }
   ],
   "source": [
    "for k,ds in tqdm(ddict_concat.items()): #loop over datasets\n",
    "    print('Deriving surges from forcing for: '+k)\n",
    " \n",
    "    #generate path for storing output\n",
    "    #model_path = os.path.join('/home/jovyan/CMIP6cf/output/surge_timeseries/',ds.source_id)\n",
    "    #output_fn = os.path.join(model_path,k.replace('.','_')+'.nc')\n",
    "    \n",
    "    model_path = os.path.join('leap-persistent/timh37/CMIP6/timeseries/surge_tgs',ds.source_id)\n",
    "    output_fn = os.path.join(model_path,k.replace('.','_')+'.zarr')\n",
    "    \n",
    "    #if not os.path.exists(model_path):\n",
    "    #    os.mkdir(model_path)\n",
    "    \n",
    "    #derive surges from forcing\n",
    "    surge_ds = xr.Dataset(data_vars=dict(surge=(['member_id','time','tg'], np.nan*np.zeros( (len(ds.member_id),len(ds.time),len(ds.tg))) )),\n",
    "                            coords=dict(member_id=ds.member_id,time=ds.time,tg=ds.tg)) #initialize output\n",
    "    \n",
    "    for i_member,member in enumerate(ds.member_id):\n",
    "        forcing_mem = ds.forcing.sel(member_id=member).copy(deep=True).load() #load forcing data array into memory (for all tg for current dataset and member)\n",
    "        \n",
    "        \n",
    "        for i_tg,tg in enumerate(ds.tg):\n",
    "            #get model forcing at TG\n",
    "            forcing_tg = forcing_mem.sel(tg=tg) \n",
    "            \n",
    "            #get MLR coefficients at TG\n",
    "            tg_coefs = mlrcoefs.mlrcoefs.sel(tg=tg)\n",
    "            num_pcs = int(np.sum(np.isfinite(tg_coefs)))-1 #number of coefs = number of PCs to derive, intercept doesn't count\n",
    "            \n",
    "            i_timesteps_w_data = np.argwhere(np.isfinite(forcing_tg.data).all(axis=1)).flatten()\n",
    "            \n",
    "            #get principal components (using sklearn to keep deterministic signs consistent)\n",
    "            pca = PCA(num_pcs)\n",
    "            pca.fit(forcing_tg.isel(time=i_timesteps_w_data).data) #remove missing values for PCA\n",
    "            pcs = pca.transform(forcing_tg.isel(time=i_timesteps_w_data).data)\n",
    "            \n",
    "            #multiply with ERA5 regression coefficients to compute surges\n",
    "            surge_ds['surge'][i_member,i_timesteps_w_data,i_tg] = np.sum(tg_coefs[np.isfinite(tg_coefs)].values * np.column_stack((np.ones(pcs.shape[0]),pcs)),axis=1) \n",
    "\n",
    "    surge_ds['surge'] = surge_ds['surge'].assign_coords(lon=('tg', mlrcoefs.lon.data),lat=('tg', mlrcoefs.lat.data)).assign_attrs(ds.attrs)\n",
    "    \n",
    "    #store:\n",
    "    #surge_ds['surge'].to_netcdf(output_fn,mode='w')\n",
    "    surge_ds.to_zarr(os.path.join('gs://',output_fn),mode='w')\n",
    "    surge_ds.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b386765-f619-4313-8593-c78d66231a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
