{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0ff811-96d3-4c4b-ae9c-5c34ea682794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1214/897296857.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "from xmip.utils import google_cmip_col\n",
    "from cmip_catalogue_operations import reduce_cat_to_max_num_realizations, drop_vars_from_cat\n",
    "import typing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275aec62-ab17-43c9-bea6-ba3917c6ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_datasets() work around to merge variables into datasets that have not exactly matching coordinates but are supposed to have the same grid\n",
    "def align_lonlat(ds_list):\n",
    "    aligned_ds_list = []\n",
    "    for ds in ds_list: #list of ds can't seem to be passed to xr.align instead\n",
    "        a,b = xr.align(ds_list[0],ds,join='override',exclude=['time','member_id'])\n",
    "        aligned_ds_list.append(b)\n",
    "    return aligned_ds_list\n",
    "\n",
    "def merge_variables_aligning_lonlat(ds_list):\n",
    "    aligned_ds_list = align_lonlat(ds_list) #override same-dimension lon/lat prior to concatenating (ensures lon/lats are not padded)\n",
    "    return xr.merge(aligned_ds_list, join='outer',compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f04cf4-acad-4920-bfc0-b4a0d7f2295a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#borrowed from intake-esm so that I can apply this on the concatenation of different data catalogues\n",
    "def search_apply_require_all_on(\n",
    "    *,\n",
    "    df: pd.DataFrame,\n",
    "    query: dict[str, typing.Any],\n",
    "    require_all_on: typing.Union[str, list[typing.Any]],\n",
    "    columns_with_iterables: set = None,\n",
    ") -> pd.DataFrame:\n",
    "    _query = query.copy()\n",
    "    \n",
    "    for column in require_all_on:\n",
    "        _query.pop(column, None)\n",
    "    \n",
    "    keys = list(_query.keys())\n",
    "    grouped = df.groupby(require_all_on)\n",
    "    values = [tuple(v) for v in _query.values()]\n",
    "    condition = set(itertools.product(*values))\n",
    "    query_results = []\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        group_for_index = group\n",
    "        # Unpack iterables to get testable index.\n",
    "        for column in (columns_with_iterables or set()).intersection(keys):\n",
    "            group_for_index = unpack_iterable_column(group_for_index, column)\n",
    "\n",
    "        index = group_for_index.set_index(keys).index\n",
    "        if not isinstance(index, pd.MultiIndex):\n",
    "            index = {(element,) for element in index.to_list()}\n",
    "        else:\n",
    "            index = set(index.to_list())\n",
    "        if condition.issubset(index):  # with iterables we could have more then requested\n",
    "            query_results.append(group)\n",
    "\n",
    "    if query_results:\n",
    "        return pd.concat(query_results).reset_index(drop=True)\n",
    "\n",
    "    return pd.DataFrame(columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4d6c24-cedc-439a-b12c-30110aaaa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcol = google_cmip_col() #xmip wrapper\n",
    "ccol = intake.open_esm_datastore(\"https://storage.googleapis.com/leap-persistent-ro/data-library/catalogs/cmip6-test/leap-pangeo-cmip6-test.json\")\n",
    "\n",
    "variable = 'sfcWind' #variable to obtain data for\n",
    "query_vars = ['sfcWind','pr','psl'] #variables models simulations are required to provide\n",
    "experiments = ['historical','ssp245']\n",
    "\n",
    "cat1_ssp245 = gcol.search(experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars)\n",
    "cat2_ssp245 = ccol.search(experiment_id=['historical','ssp245'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars)\n",
    "\n",
    "cat_ssp245 = cat1_ssp245\n",
    "cat_ssp245.esmcat._df = pd.concat([cat1_ssp245.df,cat2_ssp245.df],ignore_index=True).drop_duplicates(ignore_index=True) #combine the dataframes of the two catalogues\n",
    "cat_ssp245.esmcat._df = search_apply_require_all_on(df=cat_ssp245.df,query=dict(experiment_id=['historical','ssp245'],\n",
    "                                                    table_id=['day'],variable_id=query_vars),require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "#treating SSPs separately as I don't know how else to apply the 'require all on' function to each ssp catalogue separately\n",
    "cat1_ssp585 = gcol.search(experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars)\n",
    "cat2_ssp585 = ccol.search(experiment_id=['historical','ssp585'],\n",
    "    table_id='day',\n",
    "    variable_id=query_vars)\n",
    "\n",
    "cat_ssp585 = cat1_ssp585\n",
    "cat_ssp585.esmcat._df = pd.concat([cat1_ssp585.df,cat2_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True) #combine the dataframes of the two catalogues\n",
    "cat_ssp585.esmcat._df = search_apply_require_all_on(df=cat_ssp585.df,query=dict(experiment_id=['historical','ssp585'],\n",
    "                                                    table_id=['day'],variable_id=query_vars),require_all_on=['source_id', 'member_id','grid_label'])\n",
    "\n",
    "cat = cat_ssp585\n",
    "cat.esmcat._df = pd.concat([cat_ssp245.df,cat_ssp585.df],ignore_index=True).drop_duplicates(ignore_index=True)\n",
    "cat = reduce_cat_to_max_num_realizations(cat) #per model, select grid and 'ipf' combination providing most realizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfe2bb-a6b2-46d9-9353-a2bd92e9f1f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "for source_id in cat.df.source_id.unique():\n",
    "    print(source_id)\n",
    "    for ssp in ['ssp245','ssp585']:\n",
    "        print(ssp)\n",
    "        print(len(cat.df[(cat.df.source_id==source_id) & (cat.df.experiment_id==ssp)].member_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75db5b8-8ff4-4639-a265-4668f3a56e2f",
   "metadata": {},
   "source": [
    "To deal with duplicates with different versions, keep the newest versions. Executed by ordering versions ascendingly, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63ad8b8-91cc-4386-8ba4-c1b1483045fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(cat.df)):\n",
    "    if isinstance(cat.df.loc[i,'version'],int)==False:\n",
    "        cat.df.loc[i,'version'] = int(cat.df.loc[i,'version'].replace('v',''))\n",
    "cat.esmcat._df = cat.df.sort_values(by='version', ascending=False).drop_duplicates(subset=['activity_id','institution_id','source_id','experiment_id','member_id','table_id','variable_id','grid_label']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60374e0f-e0d8-42ca-839c-8943a9b27f50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1214/3080329572.py:10: DeprecationWarning: cdf_kwargs and zarr_kwargs are deprecated and will be removed in a future version. Please use xarray_open_kwargs instead.\n",
      "  ddict = cat.to_dataset_dict(**kwargs) #open datasets into dictionary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='537' class='' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      99.44% [537/540 01:09&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test loading in data for a single variable\n",
    "query_vars.remove(variable)\n",
    "cat = drop_vars_from_cat(cat,query_vars) #only process data for 'variable'\n",
    "\n",
    "cat.esmcat.aggregation_control.groupby_attrs = [] #to circumvent aggregate=false bug\n",
    "\n",
    "#to avoid this issue: https://github.com/intake/intake-esm/issues/496\n",
    "    #doesn't actually aggregate if we set cmip6_cat.esmcat.aggregation_control.groupby_attrs = []\n",
    "kwargs = {'zarr_kwargs':{'consolidated':True,'use_cftime':True},'aggregate':True} #keyword arguments for generating dictionary of datasets from cmip6 catalogue\n",
    "ddict = cat.to_dataset_dict(**kwargs) #open datasets into dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d4612-db28-41be-bbaa-332c3b91668a",
   "metadata": {},
   "source": [
    "Loading in works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
